{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dcb5e52",
   "metadata": {},
   "source": [
    "# Visual Backbone\n",
    "This notebook explores `ViT` transformer-based `visual-backbone` architecture (it makes sense taking in account the documents sequential nature) to compare with [CNN-based backbone](Visual-Backbone-CNN.ipynb). \n",
    "\n",
    "For the sake of exploration we build all from scratch and run some empirical study on key elements to determine default implementation details.\n",
    "\n",
    "* [Dataset and Dataloader](#data)\n",
    "* [ViT transformer model](#blocks)\n",
    "    * [Blocks](#blocks)\n",
    "    * [Encoder](#encoder)\n",
    "    * [Decoder](#decoder)\n",
    "    * [UNet](#model)    \n",
    "* [Comparative training and evaluation](#run)\n",
    "    * [Define models](#1)\n",
    "    * [Define optimization](#2)\n",
    "    * [Define validation metrics](#3)\n",
    "    * [Run parallel training with different configuration](#4)\n",
    "    * [Evaluate results](#5)\n",
    "        * [Evaluate embeddings](#embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cf6f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "from IPython.display import SVG\n",
    "from pathlib import Path\n",
    "from einops import rearrange, reduce, repeat\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torchmetrics import F1Score, JaccardIndex\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81322936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load local notebook-utils\n",
    "from scripts import render\n",
    "from scripts.dataset import *\n",
    "from scripts.trainer import *\n",
    "from scripts.backbone import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b58ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch._dynamo.config.verbose = True\n",
    "torch.cuda.empty_cache()\n",
    "print('GPU' if DEVICE == 'cuda' else 'no GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4a4e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images with semantic segmentation masks available\n",
    "images = [str(x).split('/').pop() for x in Path(f'{ROOT}/data/masks').glob('*.png')]\n",
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e624ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIEW_SIZE = 128\n",
    "LATENT_DIM = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5235b099",
   "metadata": {},
   "source": [
    "<a name=\"data\"></a>\n",
    "\n",
    "## Dataset\n",
    "The same [dataset we used for CNN-based model](Visual-Backbone-CNN.ipynb#data) -- our decoder handles `value` extraction and denoising rather than reconstruction: we generate one-dimensional binary masks for the targets; for the inputs we generate a set of random view-ports (center, rotation, zoom) from a noisy version of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e72eb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use images with masks\n",
    "samples = images #np.random.choice(images, 320, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25efa2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get static batch\n",
    "batch = prep_batch(samples, RandomViewDataset, 8, VIEW_SIZE)\n",
    "show_inputs(batch)\n",
    "show_targets(batch)\n",
    "\n",
    "# sample input/target\n",
    "X, Y = batch[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34076a8a",
   "metadata": {},
   "source": [
    "<a name=\"blocks\"></a>\n",
    "\n",
    "## ViT transformer blocks\n",
    "Transformers deal with sequences of tokens. `ViT` rearranges an image into a sequence of flattened patches and adds a learnable position embedding to a patch embedding before feeding it into a transformer-encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47613cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG('assets/uvit-blocks.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace23417",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViewToSequence(nn.Module):\n",
    "    def __init__(self,\n",
    "                 view_size: int,\n",
    "                 patch_size: int,\n",
    "                 embed_size: int,\n",
    "                 semantic_dim: int = 0,\n",
    "                 channels: int = 1):\n",
    "        \n",
    "        super(ViewToSequence, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = nn.Conv2d(channels, embed_size, kernel_size=patch_size, stride=patch_size)\n",
    "        # conditional and other tokens\n",
    "        self.tokens = nn.Parameter(torch.randn(1, semantic_dim, embed_size)) if semantic_dim > 0 else None\n",
    "        self.positions = nn.Parameter(torch.randn((view_size // patch_size) ** 2 + semantic_dim, embed_size))\n",
    "                \n",
    "    def forward(self, x):\n",
    "        b = x.shape[0]\n",
    "        # patch-sequence: either linear or conv\n",
    "        x = self.projection(x)\n",
    "        x = rearrange(x, 'b e (h) (w) -> b (h w) e')\n",
    "        if not self.tokens is None:\n",
    "            tokens = repeat(self.tokens, '() n e -> b n e', b=b)\n",
    "            # prepend the tokens to the input\n",
    "            x = torch.cat([tokens, x], dim=1)\n",
    "        # add positional embedding\n",
    "        x += self.positions\n",
    "        return x\n",
    "\n",
    "seq = ViewToSequence(VIEW_SIZE, 4, LATENT_DIM)(X)\n",
    "seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f728abac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceToView(nn.Module):\n",
    "    def __init__(self,\n",
    "                 view_size: int,\n",
    "                 patch_size: int,\n",
    "                 embed_size: int,\n",
    "                 semantic_dim: int = 0,\n",
    "                 channels: int = 1):\n",
    "        \n",
    "        super(SequenceToView, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.semantic_dim = semantic_dim\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.LayerNorm(embed_size),\n",
    "            nn.Linear(embed_size, patch_size ** 2 * channels, bias=True))\n",
    "        # prevent artifacts\n",
    "        #self.conv = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)\n",
    "        x = x[:, self.semantic_dim:, :] # skip tokens\n",
    "        d, p = int(x.shape[1] ** 0.5), self.patch_size\n",
    "        return rearrange(x, 'b (h w) (p1 p2 c) -> b c (h p1) (w p2)', h=d, w=d, p1=p, p2=p)\n",
    "        #return self.conv(x)\n",
    "    \n",
    "SequenceToView(VIEW_SIZE, 4, LATENT_DIM, channels=1)(seq).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d088d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_size: int,\n",
    "                 num_heads: int = 4):\n",
    "        \n",
    "        super(Attention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        # queries, keys, values in one matrix\n",
    "        self.qkv = nn.Linear(embed_size, embed_size * 3, bias=False)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(embed_size, embed_size),\n",
    "            nn.ReLU()) # added to eliminate distractions\n",
    "        \n",
    "    def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:\n",
    "        # split keys, queries and values in num_heads\n",
    "        q, k, v = rearrange(self.qkv(x), 'b n (h d qkv) -> (qkv) b h n d', h=self.num_heads, qkv=3)\n",
    "        # sum up over the last axis\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', q, k) # batch, num_heads, query_len, key_len\n",
    "        if mask is not None:\n",
    "            energy.mask_fill(~mask, torch.finfo(torch.float32).min)\n",
    "            \n",
    "        scaling = self.embed_size ** 0.5\n",
    "        att = torch.softmax(energy, dim=-1) / scaling\n",
    "        # sum up over the third axis\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "    \n",
    "#seq = ViewToSequence(VIEW_SIZE, 4, LATENT_DIM)(X)\n",
    "#Attention(LATENT_DIM)(seq).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5c208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Sequential):\n",
    "    def __init__(self, emb_size: int, expansion: int = 4):\n",
    "        super(MLP, self).__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(expansion * emb_size, emb_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b013ad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_size: int,\n",
    "                 bridge: bool,\n",
    "                 expansion: int = 4):\n",
    "        \n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.LayerNorm(embed_size),\n",
    "            Attention(embed_size))\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.LayerNorm(embed_size),\n",
    "            MLP(embed_size, expansion=expansion))\n",
    "        \n",
    "        self.merge = nn.Linear(2 * embed_size, embed_size) if bridge else None\n",
    "            \n",
    "    def forward(self, x, pass_through=None):\n",
    "        if self.merge is not None:\n",
    "            x = self.merge(torch.cat((pass_through, x), dim=2))\n",
    "        x = x + self.attn(x)\n",
    "        x = x + self.mlp(x)\n",
    "        return x\n",
    "        \n",
    "#TransformerBlock(LATENT_DIM, True)(seq, seq).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79430f91",
   "metadata": {},
   "source": [
    "<a name=\"encoder\"></a>\n",
    "\n",
    "### Encoder\n",
    "`Encoder` converts the image into a vector (embedding). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a86500",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 view_size: int,\n",
    "                 patch_size: int,\n",
    "                 embed_size: int,\n",
    "                 depth: int,\n",
    "                 expansion: int = 4):\n",
    "        \n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        # patch embed\n",
    "        self.sequence = ViewToSequence(view_size, patch_size, embed_size)\n",
    "        # down-blocks\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(embed_size, False, expansion) for _ in range(depth)])\n",
    "        self.depth = depth\n",
    "        self.view_size = view_size\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_size = embed_size\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = self.sequence(x)\n",
    "        outputs = []\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            outputs.append(x)\n",
    "        return outputs\n",
    "    \n",
    "#for x in TransformerEncoder(VIEW_SIZE, 4, LATENT_DIM, 4)(X): print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab52fe0",
   "metadata": {},
   "source": [
    "To produce embeddings we apply `mean` reduction at the `bottleneck` output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93784015",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanReduce(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.mean(x, axis=1)\n",
    "    \n",
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self, backbone: nn.Module, reduce: nn.Module = nn.Identity()):\n",
    "        super().__init__()\n",
    "        self.encoder = backbone\n",
    "        # freeze weights\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.reduce = reduce\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # our unet-encoder returns list of outputs from all the levels --\n",
    "        # here we only need the bottleneck\n",
    "        x = self.encoder(x).pop()\n",
    "        return self.reduce(x).squeeze()\n",
    "    \n",
    "# frozen encoder (same as we used with CNN, but different reduce)\n",
    "#VisualEncoder(TransformerEncoder(VIEW_SIZE, 4, LATENT_DIM, 4), MeanReduce()).to(DEVICE)(X.to(DEVICE)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136dcdfa",
   "metadata": {},
   "source": [
    "<a name=\"decoder\"></a>\n",
    "\n",
    "### Decoder\n",
    "`Decoder` takes an embedding vector and reconstruct an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a35f755",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 view_size: int,\n",
    "                 patch_size: int,\n",
    "                 embed_size: int,\n",
    "                 depth: int,\n",
    "                 channels: int = 1,\n",
    "                 bridge: bool = True,\n",
    "                 expansion: int = 4):\n",
    "        \n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        # up-blocks\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(embed_size, bridge, expansion) for _ in range(depth)])\n",
    "        self.unpatch = SequenceToView(view_size, patch_size, embed_size, channels=channels)\n",
    "        \n",
    "    def forward(self, outputs):\n",
    "        assert len(outputs) == len(self.blocks) + 1\n",
    "        x = outputs.pop()\n",
    "        for block in self.blocks:\n",
    "            x = block(x, outputs.pop())\n",
    "        return self.unpatch(x)\n",
    "\n",
    "#encoded = TransformerEncoder(VIEW_SIZE, 4, LATENT_DIM, 4)(X)\n",
    "#TransformerDecoder(VIEW_SIZE, 4, LATENT_DIM, 2, channels=2)(encoded).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1182dc",
   "metadata": {},
   "source": [
    "<a name=\"model\"></a>\n",
    "\n",
    "### Encoder + Decoder\n",
    "The model takes in an encoder (maybe pretrained) and attaches a matching decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1977e8a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Sequential):\n",
    "    def __init__(self, encoder: TransformerEncoder, output_dim: int = 1, bridge: bool = True):\n",
    "        # construct matching decoder\n",
    "        decoder = TransformerDecoder(encoder.view_size, encoder.patch_size, encoder.embed_size,\n",
    "                                     encoder.depth - 1, channels=output_dim, bridge=bridge)\n",
    "        super(UNet, self).__init__(encoder, decoder, nn.Softmax(dim=1))\n",
    "    \n",
    "#encoder = TransformerEncoder(VIEW_SIZE, 4, LATENT_DIM, 4)\n",
    "#summary(UNet(encoder, output_dim=2, bridge=True).to(DEVICE), (1, VIEW_SIZE, VIEW_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4858f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compare no-bride vs. bridge\n",
    "arc = { '':False, 'B':True }\n",
    "tags = list(arc.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232e842a",
   "metadata": {},
   "source": [
    "<a name=\"run\"></a>\n",
    "\n",
    "## Comparative training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239c0b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RandomViewDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c07378",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = np.random.choice(samples, int(len(samples) * 0.95), replace=False)\n",
    "test_samples = list(set(samples).difference(set(train_samples)))\n",
    "len(train_samples), len(test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfe38a3",
   "metadata": {},
   "source": [
    "With our chosen scenario the actual train/test datasets sizes will depend on the `batch_size` -- we generate a batch from each sample page: `size`=`num_samples`x`batch_size`.\n",
    "\n",
    "<a name=\"1\"></a>\n",
    "\n",
    "#### 1. Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7bb9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 4\n",
    "depth = 4\n",
    "\n",
    "num_classes = 2\n",
    "encoders, models = [], []\n",
    "\n",
    "for tag in tags:\n",
    "    encoders.append(TransformerEncoder(VIEW_SIZE, patch_size, LATENT_DIM, depth))\n",
    "    models.append(UNet(encoders[-1], output_dim=num_classes, bridge=arc[tag]).to(DEVICE))\n",
    "    \n",
    "    # continue training with saved models\n",
    "    #models[-1].load_state_dict(torch.load(f'./models/visual-unet-ViT-{patch_size}-{depth}-{tag}.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6096bec",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "\n",
    "#### 2. Define optimization\n",
    "We use the same `DiceLoss` which handles class imbalance internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a020e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6\n",
    "criterions = [DiceLoss(num_classes).to(DEVICE) for _ in range(len(models))]\n",
    "optimizers = [AdamW(model.parameters(), lr=learning_rate) for model in models]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e376adf4",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "\n",
    "#### 3. Define evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d44a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [{'f1-score': F1Score(task='multiclass', num_classes=num_classes).to(DEVICE),\n",
    "            'jaccard': JaccardIndex(task='multiclass', num_classes=num_classes).to(DEVICE)}\n",
    "           for _ in range(len(models))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3824d102",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "\n",
    "#### 4. Run training with validation\n",
    "We train all the models side-by-side on the same data batches for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3a5a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_epochs = 1\n",
    "validation_steps = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06017fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer = MultiTrainer(dataset, models, VIEW_SIZE, criterions, optimizers, metrics, tags=tags)\n",
    "results = trainer.run(train_samples, test_samples, batch_size, num_epochs, validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964c2254",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "\n",
    "#### 5. Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fea424",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_compare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c1094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6727c1b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# let's see some examples side-by-side\n",
    "sample = np.random.choice(samples)\n",
    "loader = DataLoader(RandomViewDataset(sample, VIEW_SIZE, max_samples=8), batch_size=8)\n",
    "for model in models:\n",
    "    model.eval()\n",
    "with torch.no_grad():\n",
    "    for X, Y in loader:\n",
    "        P = [torch.argmax(model(X.to(DEVICE)), axis=1).cpu() for model in models]\n",
    "        for i in range(X.shape[0]): # batch\n",
    "            fig, ax = plt.subplots(1, len(models) + 1, figsize=(8, 8))\n",
    "            ax[0].imshow(X[i,:].squeeze().numpy(), 'gray')\n",
    "            for n in range(1, len(models) + 1): # model\n",
    "                ax[n].imshow(P[n - 1][i,:].squeeze().numpy(), 'gray')\n",
    "                if i == 0:\n",
    "                    ax[n].set_title(f'{tags[n - 1]} model', fontsize=10)\n",
    "            for n in range(len(models) + 1):\n",
    "                ax[n].axis('off')\n",
    "                if i == 0 and n == 0:\n",
    "                    ax[0].set_title('Input', fontsize=10)\n",
    "        plt.show()            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c48fe72",
   "metadata": {},
   "source": [
    "<a name=\"embeddings\"></a>\n",
    "\n",
    "To evaluate the embeddings produced by trained encoders we can use the basic types of pages identified in the [baselines exploration notebook](Doc-Classification-Baselines.ipynb#labels) -- our models should be able to tell them apart. Let's look how well these groups are separated in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c6da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['mixed','plain-text','form-table','non-doc']\n",
    "\n",
    "labeled = pd.read_csv('./data/labeled-sample.csv')\n",
    "labeled.groupby('label').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160fff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25% non-docs for contrast\n",
    "classes = ['mixed','plain-text','form-table','non-doc']\n",
    "dataset = TopViewDataset(VIEW_SIZE, labeled['source'], labeled['label'], contrast=0.25)\n",
    "profiles, scores = [], []\n",
    "results = trainer.results\n",
    "for name, encoder in zip(tags, encoders):\n",
    "    name = 'Base' if name == '' else name\n",
    "    # use model encoder to get embeddings\n",
    "    embeddings, labels = get_embeddings(dataset, encoder, reduce=MeanReduce())\n",
    "    P, pca_ratios, L, lda_ratios = get_profile(embeddings, labels)\n",
    "    scores.append(silhouette_score(P[:,:3], labels, metric='euclidean'))\n",
    "    score = silhouette_score(L, labels, metric='euclidean')\n",
    "    results[len(scores) - 1]['contrast-score'] = score\n",
    "    profiles.append(pca_ratios)\n",
    "    # classes aggregated\n",
    "    centers = np.array([np.median(L[np.where(np.array(labels) == k)], axis=0) for k in range(len(classes))])\n",
    "    cmap = colormaps['gist_rainbow']\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(7, 3.2))\n",
    "    for j in range(len(classes)):\n",
    "        s = np.where(np.array(labels) == j)\n",
    "        ax[0].scatter(P[s,0], P[s,1], s=3, color=cmap(j/3), alpha=0.3)\n",
    "        ax[1].scatter(L[s,0], L[s,1], s=3, color=cmap(j/3), alpha=0.3)\n",
    "    for j in range(len(classes)):\n",
    "        ax[1].scatter(centers[j,0], centers[j,1], color=cmap(j/3),\n",
    "                      s=75, marker='pos^'[j], edgecolor='black', label=classes[j])\n",
    "    for j, (t, s) in enumerate([('PCA', scores[-1]),('LDA', score)]):\n",
    "        ax[j].set_xticks([])\n",
    "        ax[j].set_yticks([])\n",
    "        ax[j].set_title(f'{t}  silhouette-score: {s:.4f}', fontsize=10)\n",
    "    ax[1].legend(title=f'{name} model', fontsize=8, bbox_to_anchor=(1, 1), frameon=False)\n",
    "    plt.show()\n",
    "    \n",
    "# compare all\n",
    "plot_profiles(tags, profiles, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1934a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents only\n",
    "classes = ['mixed','plain-text','form-table']\n",
    "dataset = TopViewDataset(VIEW_SIZE, labeled['source'], labeled['label'], contrast=0)\n",
    "profiles, scores = [], []\n",
    "for name, encoder in zip(tags, encoders):\n",
    "    name = 'Base' if name == '' else name\n",
    "    # use model encoder to get embeddings\n",
    "    embeddings, labels = get_embeddings(dataset, encoder, reduce=MeanReduce())\n",
    "    P, pca_ratios, L, lda_ratios = get_profile(embeddings, labels)\n",
    "    scores.append(silhouette_score(P[:,:3], labels, metric='euclidean'))\n",
    "    score = silhouette_score(L, labels, metric='euclidean')\n",
    "    results[len(scores) - 1]['cluster-score'] = score\n",
    "    profiles.append(pca_ratios)\n",
    "    # classes\n",
    "    centers = np.array([np.median(L[np.where(np.array(labels) == k)], axis=0) for k in range(len(classes))])    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(7, 3.2))\n",
    "    for j in range(len(classes)):\n",
    "        s = np.where(np.array(labels) == j)\n",
    "        ax[0].scatter(P[s,0], P[s,1], s=3, color=cmap(j/3), alpha=0.3)\n",
    "        ax[1].scatter(L[s,0], L[s,1], s=3, color=cmap(j/3), alpha=0.3)\n",
    "    for j in range(len(classes)):\n",
    "        ax[1].scatter(centers[j,0], centers[j,1], color=cmap(j/3),\n",
    "                      s=75, marker='pos^'[j], edgecolor='black', label=classes[j])\n",
    "    for j, (t, s) in enumerate([('PCA', scores[-1]),('LDA', score)]):\n",
    "        ax[j].set_xticks([])\n",
    "        ax[j].set_yticks([])\n",
    "        ax[j].set_title(f'{t}  silhouette-score: {s:.4f}', fontsize=10)\n",
    "    ax[1].legend(title=f'{name} model', fontsize=8, bbox_to_anchor=(1, 1), frameon=False)\n",
    "    plt.show()\n",
    "\n",
    "# compare all\n",
    "plot_profiles(tags, profiles, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa5d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame.from_dict(results)\n",
    "results['model'] = tags\n",
    "results.set_index('model').style.format('{:.4f}').background_gradient('Greens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e53cc8",
   "metadata": {},
   "source": [
    "    for tag, model, encoder in zip(tags, models, encoders):\n",
    "        torch.save(model.state_dict(), f'./models/visual-unet-ViT-{patch_size}-{depth}-{tag}.pt')\n",
    "        \n",
    "    results.to_csv(f'./models/visual-unet-ViT-{patch_size}-{depth}.csv')\n",
    "    trainer.save(f'./models/visual-unet-ViT-{patch_size}-{depth}')\n",
    "    \n",
    "    # save base-model trained encoder\n",
    "    torch.save(encoders[0].state_dict(), './models/visual-backbone-ViT.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2ba047",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
