{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dcb5e52",
   "metadata": {},
   "source": [
    "# Visual: semantic segmentation\n",
    "This notebook focused on layout understanding tasks. We use our pretrained visual-backbone with frozen weights and train the decoders with `bridge` (cross-connections) enabled. In this experiment the view is `aligned` (random orientation, small skew considered), and zoom level is sufficient to make decision (top-view, or, a quarter-page at least).\n",
    "\n",
    "* [Dataset and Dataloader](#data)\n",
    "* [Model architecture](#model)\n",
    "* [Training and Validation](#run)\n",
    "    * [Define optimization](#2)\n",
    "    * [Define validation metrics](#3)\n",
    "    * [Run training](#4)\n",
    "    * [Evaluate results](#5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cf6f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "from pathlib import Path\n",
    "from einops import rearrange\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torchmetrics import F1Score, JaccardIndex, ConfusionMatrix\n",
    "from torchsummary import summary\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81322936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load local notebook-utils\n",
    "from scripts.backbone import *\n",
    "from scripts.dataset import *\n",
    "from scripts.trainer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b58ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print('GPU' if DEVICE == 'cuda' else 'no GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4a4e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic segmentation masks\n",
    "masks = [str(x).split('/').pop() for x in Path('./data/masks').glob('*.png')\n",
    "         if not str(x).startswith('data/masks/que-')]\n",
    "len(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e624ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIEW_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66c0128",
   "metadata": {},
   "source": [
    "<a name=\"data\"></a>\n",
    "\n",
    "### Create dataset and dataloader\n",
    "The input is a noisy version of a page-alined view-port (0, 90, 180, 270)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e2191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use images with masks\n",
    "samples = masks #np.random.choice(masks, 640, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6afd0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    batch of page-aligned view-ports from the single document for the set of tasks:\n",
    "    1. semantic segmentation\n",
    "    2. value extraction and denoising\n",
    "    3. orientation detection\n",
    "    \"\"\"\n",
    "    def __init__(self, source: str, view_size: int, max_samples: int, max_skew: float: = 3.):\n",
    "        self.view_size = view_size\n",
    "        self.max_samples = max_samples\n",
    "        # load source image\n",
    "        orig = np.array(ImageOps.grayscale(Image.open(f'{ROOT}/data/images/{source}')))\n",
    "        view = make_noisy_sample(orig)\n",
    "        # load segmentation mask\n",
    "        mask = np.array(Image.open(f'{ROOT}/data/masks/{source}'))\n",
    "        # define renderers for all\n",
    "        self.view = render.AgentView((view).astype(np.uint8), view_size, bias=np.random.randint(100))\n",
    "        self.segmentation = render.AgentView((np.eye(len(ORDER))[mask][:,:,1:] > 0) * 255, view_size)\n",
    "        self.value = render.AgentView(255 - orig, view_size)\n",
    "        # define image preprocesing\n",
    "        self.transform = Normalize\n",
    "    \n",
    "    def random_viewport(self):\n",
    "        \"\"\"\n",
    "        aligned view-port: only a small skew considered\n",
    "        \"\"\"\n",
    "        center = (np.array(self.view.space.center) * (0.1 + np.random.rand() * 1.8)).astype(int)\n",
    "        rotation = np.random.choice([0, 90, 180, 270])\n",
    "        zoom = -1 - np.random.rand() * 2.5\n",
    "        return center, rotation, zoom\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.max_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if np.random.rand() < 0.2: # random non-doc image for out-of-class example balanced repr.\n",
    "            X = self.transform(make_negative_sample(self.view_size).astype(np.float32)/255.)\n",
    "            Y = torch.Tensor(np.zeros((self.view_size, self.view_size))).long()\n",
    "            return X, (Y, Y, 0)\n",
    "        # generate random viewport\n",
    "        std = 0\n",
    "        while std < 10: # make sure there's something to see\n",
    "            center, rotation, zoom = self.random_viewport()\n",
    "            view = self.view.set_state(center, rotation, zoom)\n",
    "            std = np.std(view)        \n",
    "        # orientation task\n",
    "        Y3 = rotation//90 + 1\n",
    "        # add random skew\n",
    "        rotation += int(np.random.rand() * max_skew - max_skew)\n",
    "        # render views\n",
    "        X = self.transform(self.view.set_state(center, rotation, zoom).astype(np.float32)/255.)\n",
    "        # initialize segmentation masks channels\n",
    "        Y1 = np.zeros((self.view_size, self.view_size, len(ORDER)))\n",
    "        # render masks in the same view-port\n",
    "        view = self.segmentation.set_state(center, rotation, zoom)\n",
    "        # fix scattered after rotation value back to binary\n",
    "        view = (view/255. > 0.25).astype(int)\n",
    "        # set target as a class-indices matrix\n",
    "        Y1[:,:,1:] = view\n",
    "        # segmentation task target\n",
    "        Y1 = torch.Tensor(np.argmax(Y1, axis=(2))).long()\n",
    "        # value task target\n",
    "        view = self.value.set_state(center, rotation, zoom)\n",
    "        Y2 = torch.Tensor(view/255. >= 0.25).squeeze().long()\n",
    "        return X, (Y1, Y2, Y3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9bece7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample = np.random.choice(samples)\n",
    "# test loader\n",
    "n = 8\n",
    "loader = DataLoader(MultitaskDataset(sample, VIEW_SIZE, max_samples=n), batch_size=n, shuffle=False)\n",
    "orientation = ['N/A','0','90','180','270']\n",
    "# show first batch\n",
    "for X, (Y1, Y2, Y3) in loader:\n",
    "    print(f'source: {sample}\\nX: {X.shape}  Y1:{Y1.shape}  Y2:{Y2.shape}  Y3:{Y3.shape}')\n",
    "    for i in range(n):\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(8, 8))\n",
    "        ax[0].imshow(X[i,:].squeeze(), 'gray')\n",
    "        ax[0].axis('off')\n",
    "        # restore channels to avoid visual confusion\n",
    "        matrix = (np.eye(len(ORDER))[Y1[i,:]][:,:,1:] > 0) * 255\n",
    "        # til -> ilt change RGB order for better lines visibility\n",
    "        ax[1].imshow(matrix[:,:,[1,0,2]])\n",
    "        ax[1].axis('off')\n",
    "        ax[2].imshow(Y2[i,:], 'gray')\n",
    "        ax[2].axis('off')  \n",
    "        if i == 0:\n",
    "            ax[0].set_title(f'Input view', fontsize=10)\n",
    "            ax[1].set_title('Segmentation task', fontsize=10)\n",
    "            ax[2].set_title('Value task', fontsize=10)\n",
    "        else:\n",
    "            ax[0].set_title(f'orientation: {orientation[Y3[i]]}', fontsize=10)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34076a8a",
   "metadata": {},
   "source": [
    "<a name=\"model\"></a>\n",
    "\n",
    "## Model\n",
    "Based on our [comparative experiment](Visual-Backbone-CNN.ipynb) the default CNN-based architecture we chose `64/4/residual`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfa1832",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNELS = 64\n",
    "DEPTH = 4\n",
    "#backbone = CNNEncoder(out_channels=CHANNELS, depth=DEPTH, residual=True).to(DEVICE)\n",
    "#backbone.load_state_dict(torch.load(f'./models/visual-backbone-CNN-R-64-4.pt'))\n",
    "#summary(backbone, (1, VIEW_SIZE, VIEW_SIZE))\n",
    "\n",
    "PATCH_SIZE = 4\n",
    "#backbone = TransformerEncoder(VIEW_SIZE, PATCH_SIZE, LATENT_DIM, DEPTH)\n",
    "#backbone.load_state_dict(torch.load(f'./models/visual-backbone-ViT-4-4.pt'))\n",
    "#summary(encoder.to(DEVICE), (1, VIEW_SIZE, VIEW_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92cad25",
   "metadata": {},
   "source": [
    "<a name=\"model\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d311b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, latent_dim: int, num_classes: int):\n",
    "        super(ClassificationHead, self).__init__(\n",
    "            nn.LayerNorm(latent_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_dim, num_classes))\n",
    "\n",
    "def get_cnn_encoder(num_classes: int):\n",
    "    return nn.Sequential(\n",
    "        nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        nn.Flatten(start_dim=1),\n",
    "        ClassificationHead(LATENT_DIM, num_classes),\n",
    "        nn.Softmax(dim=1))\n",
    "\n",
    "def get_vit_encoder(num_classes: int):\n",
    "    return nn.Sequential(\n",
    "        MeanReduce(),\n",
    "        ClassificationHead(LATENT_DIM, num_classes),\n",
    "        nn.Softmax(dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66312476",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_encoder, get_decoder = get_cnn_encoder, get_cnn_decoder\n",
    "#get_encoder, get_decoder = get_vit_encoder, get_vit_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0a7cc0",
   "metadata": {},
   "source": [
    "In this new experiment we use the same encoder with multiple task-specific decoders: `segmentation`, `value`, and `orientation` detector head to train all as a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4f7514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_decoder(num_classes: int, bridge: bool = False):\n",
    "    return nn.Sequential(\n",
    "        CNNDecoder(LATENT_DIM, DEPTH - 1, True, bridge, True),\n",
    "        nn.Conv2d(CHANNELS, num_classes, 1, 1),\n",
    "        nn.Softmax(dim=1))\n",
    "\n",
    "def get_vit_decoder(num_classes: int, bridge: bool = False):\n",
    "    return nn.Sequential(\n",
    "        TransformerDecoder(VIEW_SIZE, PATCH_SIZE, LATENT_DIM, DEPTH - 1, channels=num_classes, bridge=bridge),\n",
    "        nn.Softmax(dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fa0ab1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class MultitaskUCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    train multiple models using the same visual encoder:\n",
    "    two decoders: segmentation and value\n",
    "    and one classification head\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder: CNNEncoder, frozen: bool = False):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        if frozen: # freeze weights\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False        \n",
    "        channels, depth = encoder.out_channels, encoder.depth\n",
    "        embedding_size = channels * (2 ** (depth - 1))\n",
    "        # teask-specific decoders\n",
    "        self.segmentation = CNNDecoder(embedding_size, depth - 1, True, True, True)\n",
    "        self.value = CNNDecoder(embedding_size, depth - 1, True, True, True)\n",
    "        self.alignment = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.LayerNorm(embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_size, 5), # classes: n/a, 0, 90, 180, 270\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        # tasks heads\n",
    "        self.segmentation_logits = nn.Sequential(nn.Conv2d(channels, 4, 1, 1), nn.Softmax(dim=1))\n",
    "        self.value_logits = nn.Sequential(nn.Conv2d(channels, 2, 1, 1), nn.Softmax(dim=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        e = self.encoder(x)\n",
    "        segmentation = self.segmentation_logits(self.segmentation(e[:]))\n",
    "        value = self.value_logits(self.value(e[:]))\n",
    "        # avg pool (reduce) from the bottleneck\n",
    "        alignment = self.alignment(e[-1])\n",
    "        return segmentation, value, alignment\n",
    "\n",
    "#MultitaskUCNN(encoder).to(DEVICE)(X.to(DEVICE))\n",
    "#summary(MultitaskUCNN(encoder).to(DEVICE), (1, VIEW_SIZE, VIEW_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232e842a",
   "metadata": {},
   "source": [
    "<a name=\"run\"></a>\n",
    "\n",
    "## Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20b8189",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MultitaskDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd38be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = np.random.choice(samples, int(len(samples) * 0.95), replace=False)\n",
    "test_samples = list(set(samples).difference(set(train_samples)))\n",
    "len(train_samples), len(test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfe38a3",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "\n",
    "#### 1. Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7bb9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.load_state_dict(torch.load(f'./models/visual-encoder-CNN-R-{CHANNELS}-{DEPTH}.pt'))\n",
    "model = MultitaskUCNN(encoder, frozen=False).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6096bec",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "\n",
    "#### 2. Define optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293b19aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEGMENTATION_WEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9674922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DENOISING_WEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c1471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = [ DiceLoss(4).to(DEVICE), DiceLoss(2).to(DEVICE), nn.CrossEntropyLoss().to(DEVICE) ]\n",
    "criterion = HydraLoss(criteria).to(DEVICE)\n",
    "# optimize both: model and loss parameters\n",
    "params = [p for p in model.parameters()] + [p for p in criterion.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a020e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6\n",
    "optimizer = AdamW(params, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e376adf4",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "\n",
    "#### 3. Define evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87733a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    'segmentation': {\n",
    "        'f1-score': F1Score(task='multiclass', num_classes=4).to(DEVICE) },\n",
    "    'value': {\n",
    "        'f1-score': F1Score(task='multiclass', num_classes=2).to(DEVICE) },\n",
    "    'orientation': {\n",
    "        'confmat': ConfusionMatrix(task='multiclass', num_classes=5).to(DEVICE),\n",
    "        'f1-score': F1Score(task='multiclass', num_classes=5).to(DEVICE) }}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3824d102",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "\n",
    "#### 4. Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3a5a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfc6eef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!rm -rf ./runs/visual-multi-cnn\n",
    "trainer = Trainer(model, dataset, VIEW_SIZE, criterion, optimizer, metrics, multi_y=True,\n",
    "                  tensorboard_dir='runs/visual-multi-cnn') # log progress to tensorboard\n",
    "results = trainer.run(train_samples, test_samples, batch_size, num_epochs=num_epochs, validation_steps=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcde749",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "\n",
    "#### 5. Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af88af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(trainer.loss_history, trainer.metrics_history, multi_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89eb4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confmat(np.sum(np.array(results['orientation']['confmat']), axis=0),\n",
    "             orientation, 'Orientation task confusion-matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57b8ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.run(train_samples, test_samples, batch_size, num_epochs=1, validation_steps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bc3cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(trainer.loss_history, trainer.metrics_history, multi_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efd4de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confmat(np.sum(np.array(results['orientation']['confmat']), axis=0),\n",
    "             orientation, 'Orientation task confusion-matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8448510",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.run(train_samples, test_samples, batch_size, num_epochs=1, validation_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2c5f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(trainer.loss_history, trainer.metrics_history, multi_y=True)\n",
    "plot_confmat(np.sum(np.array(results['orientation']['confmat']), axis=0),\n",
    "             orientation, 'Orientation task confusion-matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087e0c0d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# let's see some examples with new variation from test-samples\n",
    "loader = DataLoader(MultitaskDataset(np.random.choice(samples), VIEW_SIZE, max_samples=8),\n",
    "                    batch_size=8, shuffle=False)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X, (Y1, Y2, Y3) in loader:\n",
    "        preds = model(X.to(DEVICE))        \n",
    "        P1 = np.argmax(preds[0].cpu().numpy(), axis=1)\n",
    "        P2 = np.argmax(preds[1].cpu().numpy(), axis=1)\n",
    "        P3 = np.argmax(preds[2].cpu().numpy(), axis=1)\n",
    "        for i in range(X.shape[0]):\n",
    "            fig, ax = plt.subplots(1, 4, figsize=(11, 11))\n",
    "            # input view\n",
    "            ax[0].imshow(X[i,:].squeeze().numpy(), 'gray')\n",
    "            ax[0].axis('off')\n",
    "            \n",
    "            # segmentation target color-channels\n",
    "            matrix = (np.eye(len(ORDER))[Y1[i,:]][:,:,1:] > 0) * 255\n",
    "            ax[1].imshow(matrix[:,:,[1,0,2]])\n",
    "            ax[1].axis('off')\n",
    "            \n",
    "            # task output\n",
    "            matrix = (np.eye(len(ORDER))[P1[i,:]][:,:,1:] > 0) * 255\n",
    "            ax[2].imshow(matrix[:,:,[1,0,2]])\n",
    "            ax[2].axis('off')\n",
    "            \n",
    "            # kinetic awareness task\n",
    "            ax[3].imshow(P2[i,:], 'gray')\n",
    "            ax[3].axis('off')\n",
    "            \n",
    "            if i == 0:\n",
    "                ax[0].set_title(f'Input view {orientation[Y3[i]]}', fontsize=10)\n",
    "                ax[1].set_title('Segmentation target', fontsize=10)\n",
    "                ax[2].set_title('Segmentation output', fontsize=10)\n",
    "                ax[3].set_title(f'Value output {orientation[P3[i]]}', fontsize=10)\n",
    "            else:\n",
    "                ax[0].set_title(orientation[Y3[i]], fontsize=10)\n",
    "                ax[3].set_title(orientation[P3[i]], fontsize=10)\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f845845",
   "metadata": {},
   "source": [
    "#### 6. Save progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d7bece",
   "metadata": {},
   "source": [
    "    torch.save({'epoch': num_epochs,\n",
    "                'batch_size': batch_size,\n",
    "                'learning_rate': learning_rate,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict()},\n",
    "               f'./models/visual-multi-CNN- R-{CHANNELS}-{DEPTH}.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369b3071",
   "metadata": {},
   "source": [
    "    # save encoder model\n",
    "    torch.save(encoder.state_dict(), f'./models/visual-encoder-CNN-R-{CHANNELS}-{DEPTH}-S.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec743bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
