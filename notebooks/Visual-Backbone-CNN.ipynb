{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dcb5e52",
   "metadata": {},
   "source": [
    "# Visual Backbone\n",
    "This notebook explores `CNN`-based `visual-backbone` architectures. The [other consideration](Visual-Backbone-ViT.ipynb) is to use `ViT` transformer: documents are sequential in nature.\n",
    "\n",
    "With downstream segmentation and classification tasks in mind we going to use `UNet` which won't produce most useful `latent space` due to `skip-connections` (bridges) and lack of compression (in case of a shallow network): first we explore different architectures. The ways to to make the latent space more useful we explore in the [VAE training notebook](Visual-Backbone-VAE.ipynb).\n",
    "\n",
    "For the sake of exploration we build all from scratch and run some empirical study on key elements to determine default implementation details.\n",
    "\n",
    "* [Dataset and Dataloader](#data)\n",
    "* [UNet model](#unet)\n",
    "    * [Attention](#attn)\n",
    "    * [Encoder](#encoder)\n",
    "    * [Decoder](#decoder)\n",
    "    * [Encoder-Decoder](#model)    \n",
    "* [Comparative training and evaluation](#run)\n",
    "    * [Define models](#1)\n",
    "    * [Define optimization](#2)\n",
    "    * [Define validation metrics](#3)\n",
    "    * [Run parallel training](#4)\n",
    "    * [Evaluate results](#5)\n",
    "    * [Evaluate embeddings](#embeddings)\n",
    "    \n",
    "#### Observations summary\n",
    "* all models can support both non-doc detection as `anomaly detection` and as a `separate class`\n",
    "* all models have sufficient info in the latent space to support our [baseline classification task](Visual-Classification-Baseline.ipynb)\n",
    "* `residual` and `skip` connections speed up training significantly\n",
    "* `self-attention` and `bridge-attention` help with denoising\n",
    "* `self-attention` helps with outliers while residual connections make it worth\n",
    "* skip-connections did not cause latent space to loose important information (bypass the bottleneck): in our case [we've got better clusters](#results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cf6f1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:02:16.210679Z",
     "iopub.status.busy": "2023-09-23T08:02:16.209708Z",
     "iopub.status.idle": "2023-09-23T08:02:19.147067Z",
     "shell.execute_reply": "2023-09-23T08:02:19.146011Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "from IPython.display import SVG\n",
    "from pathlib import Path\n",
    "from einops import rearrange\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import SGD, AdamW\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torchmetrics import F1Score, JaccardIndex\n",
    "from torchsummary import summary\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81322936",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:02:19.149845Z",
     "iopub.status.busy": "2023-09-23T08:02:19.149518Z",
     "iopub.status.idle": "2023-09-23T08:02:19.321544Z",
     "shell.execute_reply": "2023-09-23T08:02:19.320507Z"
    }
   },
   "outputs": [],
   "source": [
    "# load local notebook-utils\n",
    "from scripts import render\n",
    "from scripts.dataset import *\n",
    "from scripts.trainer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b58ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:02:19.324661Z",
     "iopub.status.busy": "2023-09-23T08:02:19.324110Z",
     "iopub.status.idle": "2023-09-23T08:02:19.329742Z",
     "shell.execute_reply": "2023-09-23T08:02:19.328859Z"
    }
   },
   "outputs": [],
   "source": [
    "#torch._dynamo.config.verbose = True\n",
    "torch.cuda.empty_cache()\n",
    "print('GPU' if DEVICE == 'cuda' else 'no GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4a4e23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:02:19.332899Z",
     "iopub.status.busy": "2023-09-23T08:02:19.332547Z",
     "iopub.status.idle": "2023-09-23T08:02:19.376037Z",
     "shell.execute_reply": "2023-09-23T08:02:19.375177Z"
    }
   },
   "outputs": [],
   "source": [
    "# images with semantic segmentation masks available\n",
    "images = [str(x).split('/').pop() for x in Path(f'{ROOT}/data/masks').glob('*.png')]\n",
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e624ac6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:02:19.380010Z",
     "iopub.status.busy": "2023-09-23T08:02:19.379600Z",
     "iopub.status.idle": "2023-09-23T08:02:19.383782Z",
     "shell.execute_reply": "2023-09-23T08:02:19.382695Z"
    }
   },
   "outputs": [],
   "source": [
    "VIEW_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5235b099",
   "metadata": {},
   "source": [
    "<a name=\"data\"></a>\n",
    "\n",
    "## Dataset\n",
    "Document views are somewhat discrete -- for this simple we formulate decoder's task as classification (each pixel either signal or void) -- the decoder handles value extraction / denoising rather than reconstruction: for the inputs we generate a set of random viewports (center, rotation, zoom) from a single noisy version of a page; for the targets we generate one-dimensional binary masks.\n",
    "\n",
    "We need some out-of-the-class samples for the contrast:\n",
    "* if `contrast` being used: non-docs are treated as a `class`\n",
    "* with no `contrast` training: non-docs are treated as `anomaly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e72eb25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:02:19.387057Z",
     "iopub.status.busy": "2023-09-23T08:02:19.386520Z",
     "iopub.status.idle": "2023-09-23T08:02:19.390345Z",
     "shell.execute_reply": "2023-09-23T08:02:19.389517Z"
    }
   },
   "outputs": [],
   "source": [
    "# docs samples\n",
    "samples = images #np.random.choice(images, 160, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d27fc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fraction of non-docs shown\n",
    "#CONTRAST = 0.1\n",
    "CONTRAST = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f5a3be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:02:19.393225Z",
     "iopub.status.busy": "2023-09-23T08:02:19.392754Z",
     "iopub.status.idle": "2023-09-23T08:02:19.489664Z",
     "shell.execute_reply": "2023-09-23T08:02:19.488510Z"
    }
   },
   "outputs": [],
   "source": [
    "# random images (non-docs; and no text anywhere) for out-of-class examples\n",
    "negatives = [str(x) for x in Path(f'{ROOT}/data/unsplash').glob('*.jpg')]\n",
    "\n",
    "def make_negative_sample(view_size):\n",
    "    \"\"\"\n",
    "    generate random non-document view\n",
    "    \"\"\"\n",
    "    sample = 255 - np.array(ImageOps.grayscale(Image.open(np.random.choice(negatives))))\n",
    "    nav = render.AgentView(sample, view_size, bias=np.random.randint(100))\n",
    "    center = (np.array(sample.shape) * (1 - np.random.rand(2))).astype(int)\n",
    "    rotation = np.random.randint(0, 360)\n",
    "    zoom = np.random.rand() * 2 - 2\n",
    "    return nav.set_state(center, rotation, zoom)\n",
    "\n",
    "plt.imshow(make_negative_sample(VIEW_SIZE), 'gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb76aac",
   "metadata": {},
   "source": [
    "The challenge here: presence of straight lines and grids in some non-doc images may confuse our model.\n",
    "\n",
    "To synthesize data on the go our data loader has to load the image from the disc, create a noisy version of it, and than render some random viewport -- that is slow -- some trade-off options:\n",
    "* generate a whole dataset prior to the training (dataloader will be handling only loading)\n",
    "* generate a set of noisy images (slowest part) prior to the training (dataloader will be handling viewports)\n",
    "* dataloader will be handling all but for a single source: batch of random view-ports with one noisy version\n",
    "\n",
    "For initial R&D we go with the first option which is slowest but gives us most flexibility. For the final stage we go with the second option (faster and easy to benchmark).\n",
    "\n",
    "With our chosen scenarios the actual train/test datasets sizes will depend on the `batch_size` -- training/validation process runs through all the samples and generate a batch from each sample: `size`=`num_samples`âœ•`batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f359edf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:02:19.494428Z",
     "iopub.status.busy": "2023-09-23T08:02:19.494025Z",
     "iopub.status.idle": "2023-09-23T08:02:19.499414Z",
     "shell.execute_reply": "2023-09-23T08:02:19.498563Z"
    }
   },
   "outputs": [],
   "source": [
    "# common preprocessing\n",
    "class NormalizeView:\n",
    "    \"\"\"\n",
    "    map to [0,1] and put channels first\n",
    "    \"\"\"\n",
    "    def __call__(self, X):\n",
    "        low, high = np.min(X), np.max(X)\n",
    "        X = (X - low).astype(float)\n",
    "        if high > low:\n",
    "            X /= (high - low)\n",
    "        if len(X.shape) == 3:\n",
    "            h, w, c = X.shape\n",
    "            return torch.Tensor(X).view(c, h, w)\n",
    "        return torch.Tensor(X).unsqueeze(0)\n",
    "    \n",
    "\n",
    "NormalizeView()(np.random.randint(20, 220, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6afd0b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:02:19.502146Z",
     "iopub.status.busy": "2023-09-23T08:02:19.501844Z",
     "iopub.status.idle": "2023-09-23T08:02:19.513826Z",
     "shell.execute_reply": "2023-09-23T08:02:19.512816Z"
    }
   },
   "outputs": [],
   "source": [
    "class RandomViewDataset(Dataset):\n",
    "    \"\"\"\n",
    "    use a single document noisy variation to create a batch of random view-ports\n",
    "    make new data loader for each doc rather than reload resources for each view\n",
    "    this scenario makes training very sensitive to the bad samples with bigger batches\n",
    "    \"\"\"\n",
    "    def __init__(self, source: str, view_size: int, max_samples: int = 64, threshold: float = 0.25):\n",
    "        self.view_size = view_size\n",
    "        self.max_samples = max_samples\n",
    "        self.threshold = threshold\n",
    "        # load source image\n",
    "        orig = np.array(ImageOps.grayscale(Image.open(f'{ROOT}/data/images/{source}')))\n",
    "        view = make_noisy_sample(orig)\n",
    "        # define renderers for all\n",
    "        self.view = render.AgentView((view).astype(np.uint8), view_size, bias=np.random.randint(100))\n",
    "        self.target = render.AgentView(255. - orig, view_size)\n",
    "        # define image preprocesing\n",
    "        self.transform = NormalizeView()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.max_samples\n",
    "    \n",
    "    def random_viewport(self):\n",
    "        # pan: anywhere within the page-view bounding box\n",
    "        center = (np.array(self.view.space.center) * (0.25 + np.random.rand() * 1.5)).astype(int)\n",
    "        rotation = np.random.randint(0, 360)\n",
    "        zoom = np.random.rand() * 4 - 3.5\n",
    "        return center, rotation, zoom\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # once a while we need a negative sample\n",
    "        if np.random.rand() < CONTRAST:\n",
    "            X = self.transform(make_negative_sample(self.view_size))\n",
    "            Y = torch.Tensor(np.zeros((self.view_size, self.view_size))).long()\n",
    "            return X, Y\n",
    "        # generate random viewport\n",
    "        center, rotation, zoom = self.random_viewport()\n",
    "        std = 0\n",
    "        while std < 10: # make sure there's something to see\n",
    "            center, rotation, zoom = self.random_viewport()\n",
    "            view = self.view.render(center, rotation, zoom)\n",
    "            std = np.std(view)\n",
    "        # render corresponding views\n",
    "        X = self.transform(view)\n",
    "        # initialize masks channels\n",
    "        target = self.transform(self.target.render(center, rotation, zoom))\n",
    "        # sqrt here to make subtle lines pass the threshold\n",
    "        Y = (target >= self.threshold).squeeze().long()\n",
    "        return X, Y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ddbbcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:02:19.517565Z",
     "iopub.status.busy": "2023-09-23T08:02:19.517253Z",
     "iopub.status.idle": "2023-09-23T08:02:20.570988Z",
     "shell.execute_reply": "2023-09-23T08:02:20.570157Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample = np.random.choice(samples)\n",
    "# test loader\n",
    "n = 8\n",
    "loader = DataLoader(RandomViewDataset(sample, VIEW_SIZE, max_samples=n), batch_size=n, shuffle=False)\n",
    "# show batch\n",
    "for X, Y in loader:\n",
    "    print(f'source: {sample}\\nbatch:  X:{X.shape}  Y:{Y.shape}')\n",
    "    for i in range(n):\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(5, 5))\n",
    "        ax[0].imshow(X[i,:].squeeze(), 'gray')\n",
    "        ax[0].axis('off')\n",
    "        ax[1].imshow(Y[i,:].squeeze(), 'gray')\n",
    "        ax[1].axis('off')\n",
    "        if i == 0:\n",
    "            ax[0].set_title('Input view')\n",
    "            ax[1].set_title('Decoder task')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34076a8a",
   "metadata": {},
   "source": [
    "<a name=\"unet\"></a>\n",
    "\n",
    "\n",
    "## UNet encoder-decoder model\n",
    "We used a standard `ResNet`-encoder for [the baselines](Doc-Classification-Baselines.ipynb). For this experiment we build our own from scratch.\n",
    "\n",
    "We use `GroupNorm` due to a small batch-size. Using `GELU` vs. `ReLU` noticeably stabilized the training in our case. Using `1x1 Conv` as a residual adapter should be explored. In this experiment we focus on `residual` and `skip`-connections (bridges), and two different types of `attention`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47613cda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:02:20.574626Z",
     "iopub.status.busy": "2023-09-23T08:02:20.574455Z",
     "iopub.status.idle": "2023-09-23T08:02:20.590862Z",
     "shell.execute_reply": "2023-09-23T08:02:20.590144Z"
    }
   },
   "outputs": [],
   "source": [
    "SVG('assets/unet-blocks.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d9beab",
   "metadata": {},
   "source": [
    "<a name=\"attn\"></a>\n",
    "\n",
    "### Attention\n",
    "`Self-Attention` highlights the important parts of the feature-map.\n",
    "The output of the self-attention is converted to the shape of the input, normalized, and mapped between zero and one. The values are then multiplied by the output of the convolutional block in order to apply a weight to it before it progress to the next level. We use this module interchangeably with `GELU` activation at the end of the block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad619bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, num_heads: int = 1, head_channels: int = None):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        hidden_dim = in_channels * num_heads if head_channels is None else head_channels * num_heads\n",
    "        # query-key-value\n",
    "        self.qkv = nn.Conv2d(in_channels, hidden_dim * 3, 1)\n",
    "        self.output = nn.Sequential(nn.Conv2d(hidden_dim, out_channels, 1), nn.GroupNorm(1, out_channels))\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[-2:]\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = rearrange(qkv, 'b (qkv heads c) h w -> qkv b heads c (h w)', heads=self.num_heads, qkv=3)\n",
    "        k = k.softmax(dim=-1)\n",
    "        context = torch.einsum('bhdn, bhen -> bhde', k, v)\n",
    "        context = torch.einsum('bhde, bhdn -> bhen', context, q)\n",
    "        context = rearrange(context, 'b heads c (h w) -> b (heads c) h w', heads=self.num_heads, w=w, h=h)\n",
    "        return torch.tanh(self.activation(self.output(context)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4929b3",
   "metadata": {},
   "source": [
    "As mentioned above, the document views are somewhat discrete and almost \"flat\" -- just 3 or 4 fixed resolution levels -- it seems reasonable to apply attention to the bridges (bridge-attention module) which helps highlight the important regions of the feature map by resolution level: then we have an attention-gate in the decoder with enabled pass-through connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a01b212",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:02:22.782823Z",
     "iopub.status.busy": "2023-09-23T08:02:22.782473Z",
     "iopub.status.idle": "2023-09-23T08:02:22.789740Z",
     "shell.execute_reply": "2023-09-23T08:02:22.788740Z"
    }
   },
   "outputs": [],
   "source": [
    "class BridgeAttention(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super(BridgeAttention, self).__init__()\n",
    "        #self.wx = ConvNorm(channels, channels, 1, 0)\n",
    "        #self.wg = ConvNorm(channels, channels, 1, 0)\n",
    "        self.wx = nn.Conv2d(channels, channels, 1, padding=0)\n",
    "        self.wg = nn.Conv2d(channels, channels, 1, padding=0)\n",
    "        self.activation = nn.ReLU()\n",
    "        #self.attn = ConvNorm(channels, channels, 1, padding=0)\n",
    "        self.attn = nn.Conv2d(channels, channels, 1, padding=0)\n",
    "        \n",
    "    def forward(self, pass_through, gaiting_signal):\n",
    "        x = self.wx(pass_through)\n",
    "        g = self.wg(gaiting_signal)\n",
    "        x = torch.tanh(self.attn(self.activation(x + g)))\n",
    "        return pass_through * (x + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79430f91",
   "metadata": {},
   "source": [
    "<a name=\"encoder\"></a>\n",
    "\n",
    "### Encoder\n",
    "Our `unet-encoder` returns a list of outputs (feature-maps) from all resolution (depth) levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace23417",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:02:20.600911Z",
     "iopub.status.busy": "2023-09-23T08:02:20.600618Z",
     "iopub.status.idle": "2023-09-23T08:02:20.608333Z",
     "shell.execute_reply": "2023-09-23T08:02:20.607491Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConvNorm(nn.Sequential):\n",
    "    def __init__(self, in_channels: int, out_channels: int, padding: int = 1):\n",
    "        super(ConvNorm, self).__init__(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=padding),\n",
    "            nn.GroupNorm(1, out_channels))\n",
    "\n",
    "        \n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, residual: bool = True, attn: bool = True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            ConvNorm(in_channels, out_channels),\n",
    "            nn.GELU(),\n",
    "            ConvNorm(out_channels, out_channels))\n",
    "        self.residual = nn.Conv2d(in_channels, out_channels, 1, padding=0) if residual else None\n",
    "        self.attn = SelfAttention(in_channels, out_channels) if attn is not None else None\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn = self.attn(x) if self.attn is not None else None\n",
    "        output = self.block(x)\n",
    "        if self.residual:\n",
    "            output = output + self.residual(x)\n",
    "        # either attention or activation\n",
    "        return self.activation(output) if attn is None else output * attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c9de7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:02:20.612016Z",
     "iopub.status.busy": "2023-09-23T08:02:20.611725Z",
     "iopub.status.idle": "2023-09-23T08:02:20.617537Z",
     "shell.execute_reply": "2023-09-23T08:02:20.616573Z"
    }
   },
   "outputs": [],
   "source": [
    "class DownsampleBlock(nn.Module):        \n",
    "    def __init__(self, in_channels: int, out_channels: int, residual: bool = True, attn: bool = True):\n",
    "        super(DownsampleBlock, self).__init__()\n",
    "        self.block = ConvBlock(in_channels, out_channels, residual, attn)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass_through = self.block(x)\n",
    "        output = self.pool(pass_through)\n",
    "        return output, pass_through\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d088d5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:02:20.620431Z",
     "iopub.status.busy": "2023-09-23T08:02:20.620142Z",
     "iopub.status.idle": "2023-09-23T08:02:22.767767Z",
     "shell.execute_reply": "2023-09-23T08:02:22.766708Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, in_channels: int = 1, channels: int = 64, depth: int = 4,\n",
    "                       residual: bool = True, attn: bool = True):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.depth = depth\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for _ in range(depth):\n",
    "            self.blocks.append(DownsampleBlock(in_channels, channels, residual, attn))\n",
    "            in_channels, channels = channels, channels * 2\n",
    "        self.residual = residual\n",
    "        self.attn = attn\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for block in self.blocks:\n",
    "            x, pass_through = block(x)\n",
    "            outputs.append(pass_through)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "encoder = CNNEncoder(channels=64, depth=4, residual=True, attn=True)\n",
    "summary(encoder.to(DEVICE), (1, VIEW_SIZE, VIEW_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f52fce",
   "metadata": {},
   "source": [
    "`VisualEncoder` converts the image into a vector (embedding). With shallow network we've got no compression and a high-dimensional feature map at the bottle neck. To reduce dimensions we apply `AdaptiveAvgPool2d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da85af3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:02:22.772671Z",
     "iopub.status.busy": "2023-09-23T08:02:22.772307Z",
     "iopub.status.idle": "2023-09-23T08:02:22.779597Z",
     "shell.execute_reply": "2023-09-23T08:02:22.778585Z"
    }
   },
   "outputs": [],
   "source": [
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self, backbone: nn.Module, reduce: nn.Module = None, frozen: bool = True):\n",
    "        super(VisualEncoder, self).__init__()\n",
    "        self.encoder = backbone\n",
    "        if frozen: # freeze weights\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.reduce = reduce\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # our unet-encoder returns list of outputs from all the levels --\n",
    "        # here we only need the bottleneck\n",
    "        x = self.encoder(x).pop()\n",
    "        if self.reduce is None:\n",
    "            return torch.flatten(x, start_dim=1)\n",
    "        return torch.flatten(self.reduce(x), start_dim=1)\n",
    "\n",
    "#summary(VisualEncoder(encoder, nn.AdaptiveAvgPool2d((1, 1))).to(DEVICE), (1, VIEW_SIZE, VIEW_SIZE))\n",
    "VisualEncoder(encoder, nn.AdaptiveAvgPool2d((1, 1))).to(DEVICE).to(DEVICE)(X.to(DEVICE)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4608e882",
   "metadata": {},
   "outputs": [],
   "source": [
    "VisualEncoder(encoder, None).to(DEVICE).to(DEVICE)(X.to(DEVICE)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1431015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can make trainable adaptor\n",
    "reduce = nn.Sequential(nn.Linear(131072, 512), nn.LayerNorm(512))\n",
    "VisualEncoder(encoder, nn.AdaptiveAvgPool2d((1, 1))).to(DEVICE).to(DEVICE)(X.to(DEVICE)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136dcdfa",
   "metadata": {},
   "source": [
    "<a name=\"decoder\"></a>\n",
    "\n",
    "### Decoder\n",
    "Our `unet-decoder` takes the list of feature-maps (`unet-encoder` output) and outputs the set of segmentation masks for the input image. With `bridge=False` (no skip-connections) the decoder will only use the bottleneck level feature map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cc6411",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:02:22.794186Z",
     "iopub.status.busy": "2023-09-23T08:02:22.793933Z",
     "iopub.status.idle": "2023-09-23T08:02:22.804503Z",
     "shell.execute_reply": "2023-09-23T08:02:22.803398Z"
    }
   },
   "outputs": [],
   "source": [
    "class UpsampleBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, residual: bool, attn: bool = True,\n",
    "                 bridge: bool = True, bridge_attn: bool = True):\n",
    "        super(UpsampleBlock, self).__init__()\n",
    "        self.deconv = nn.ConvTranspose2d(in_channels, in_channels//2, kernel_size=2, stride=2)\n",
    "        self.bridge_attn = BridgeAttention(out_channels) if bridge_attn and attn else None\n",
    "        self.block = ConvBlock(in_channels if bridge else in_channels//2, out_channels, residual, attn)\n",
    "        self.bridge = bridge\n",
    "\n",
    "    def forward(self, x, pass_through=None):\n",
    "        x = self.deconv(x)\n",
    "        if self.bridge: # use skip-connection\n",
    "            if self.bridge_attn: # apply attention-gate to skip-connection\n",
    "                pass_through = self.bridge_attn(pass_through, x)\n",
    "            x = torch.cat((pass_through, x), dim=1)\n",
    "        return self.block(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d252cc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:02:22.807359Z",
     "iopub.status.busy": "2023-09-23T08:02:22.807032Z",
     "iopub.status.idle": "2023-09-23T08:02:22.814586Z",
     "shell.execute_reply": "2023-09-23T08:02:22.813559Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNNDecoder(nn.Module):\n",
    "    def __init__(self, in_channels: int, output_dim: int, depth: int, residual: bool, attn: bool,\n",
    "                       bridge: bool, bridge_attn: bool):\n",
    "        super(CNNDecoder, self).__init__()\n",
    "        self.blocks = nn.ModuleList()\n",
    "        out_channels = in_channels//2\n",
    "        for _ in range(depth):\n",
    "            self.blocks.append(UpsampleBlock(in_channels, out_channels, residual, attn, bridge, bridge_attn))\n",
    "            in_channels, out_channels = out_channels, out_channels//2\n",
    "        self.head = nn.Conv2d(in_channels, output_dim, 1, padding=0) # 1x1 convolution\n",
    "        \n",
    "    def forward(self, outputs):\n",
    "        assert len(outputs) == len(self.blocks) + 1\n",
    "        outputs = list(outputs)\n",
    "        x = outputs.pop()\n",
    "        for block in self.blocks:\n",
    "            x = block(x, outputs.pop())\n",
    "        return self.head(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1182dc",
   "metadata": {},
   "source": [
    "<a name=\"model\"></a>\n",
    "\n",
    "### Encoder + Decoder\n",
    "The model takes in an encoder (maybe pretrained) and attaches a matching decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7713e64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:02:22.819161Z",
     "iopub.status.busy": "2023-09-23T08:02:22.818834Z",
     "iopub.status.idle": "2023-09-23T08:02:22.827346Z",
     "shell.execute_reply": "2023-09-23T08:02:22.826137Z"
    }
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Sequential):\n",
    "    def __init__(self, encoder: CNNEncoder, output_dim: int = 1, bridge: bool = True, bridge_attn: bool = True):\n",
    "        # construct matching decoder\n",
    "        in_channels = encoder.channels * (2 ** (encoder.depth - 1))\n",
    "        depth = encoder.depth - 1\n",
    "        decoder = CNNDecoder(in_channels, output_dim, depth,\n",
    "                             encoder.residual, encoder.attn, bridge, bridge_attn)\n",
    "        super(UNet, self).__init__(\n",
    "            encoder,\n",
    "            decoder,\n",
    "            nn.Softmax(dim=1)) # norm channels\n",
    "\n",
    "UNet(encoder, output_dim=3, bridge=False, bridge_attn=True).to(DEVICE)(X.to(DEVICE)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78a0440",
   "metadata": {},
   "source": [
    "Let's compare different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b37a411",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:02:22.830468Z",
     "iopub.status.busy": "2023-09-23T08:02:22.830133Z",
     "iopub.status.idle": "2023-09-23T08:02:22.836048Z",
     "shell.execute_reply": "2023-09-23T08:02:22.835136Z"
    }
   },
   "outputs": [],
   "source": [
    "arc = { '':     {'residual':False, 'attn':False, 'bridge':False, 'bridge_attn':False },\n",
    "       \n",
    "        'R':    {'residual':True,  'attn':False, 'bridge':False, 'bridge_attn':False },\n",
    "        'A':    {'residual':False, 'attn':True,  'bridge':False, 'bridge_attn':False }, \n",
    "        'B':    {'residual':False, 'attn':False, 'bridge':True,  'bridge_attn':False },\n",
    "       \n",
    "        'RA':   {'residual':True,  'attn':True,  'bridge':False, 'bridge_attn':False }, \n",
    "        'RB':   {'residual':True,  'attn':False, 'bridge':True,  'bridge_attn':False },\n",
    "        'AB':   {'residual':False, 'attn':True,  'bridge':True,  'bridge_attn':False }, \n",
    "        'BA':   {'residual':False, 'attn':False, 'bridge':True,  'bridge_attn':True  },\n",
    "       \n",
    "        'RAB':  {'residual':True,  'attn':True,  'bridge':True,  'bridge_attn':False },\n",
    "        'RBA':  {'residual':True,  'attn':False, 'bridge':True,  'bridge_attn':True  },\n",
    "        'ABA':  {'residual':False, 'attn':False, 'bridge':True,  'bridge_attn':True  },\n",
    "       \n",
    "        'RABA': {'residual':True,  'attn':True,  'bridge':True,  'bridge_attn':True  }}\n",
    "\n",
    "tags = list(arc.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232e842a",
   "metadata": {},
   "source": [
    "<a name=\"run\"></a>\n",
    "## Comparative training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239c0b9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:02:22.838883Z",
     "iopub.status.busy": "2023-09-23T08:02:22.838532Z",
     "iopub.status.idle": "2023-09-23T08:02:22.842631Z",
     "shell.execute_reply": "2023-09-23T08:02:22.841734Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = RandomViewDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4819c942",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:02:22.845966Z",
     "iopub.status.busy": "2023-09-23T08:02:22.845211Z",
     "iopub.status.idle": "2023-09-23T08:02:22.862122Z",
     "shell.execute_reply": "2023-09-23T08:02:22.861403Z"
    }
   },
   "outputs": [],
   "source": [
    "train_samples = np.random.choice(samples, int(len(samples) * 0.95), replace=False)\n",
    "test_samples = list(set(samples).difference(set(train_samples)))\n",
    "len(train_samples), len(test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfe38a3",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "\n",
    "#### 1. Define models\n",
    "We build several models with different features to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7bb9f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:02:22.866363Z",
     "iopub.status.busy": "2023-09-23T08:02:22.865987Z",
     "iopub.status.idle": "2023-09-23T08:02:23.330401Z",
     "shell.execute_reply": "2023-09-23T08:02:23.329539Z"
    }
   },
   "outputs": [],
   "source": [
    "channels = 64\n",
    "depth = 4\n",
    "\n",
    "num_classes = 2\n",
    "encoders, models = [], []\n",
    "\n",
    "for tag in tags:\n",
    "    encoders.append(CNNEncoder(channels=channels, depth=depth,\n",
    "                               residual=arc[tag]['residual'], attn=arc[tag]['attn']))\n",
    "    models.append(UNet(encoders[-1], output_dim=num_classes,\n",
    "                       bridge=arc[tag]['bridge'], bridge_attn=arc[tag]['bridge_attn']).to(DEVICE))\n",
    "    \n",
    "    # continue training with saved models\n",
    "    #models[-1].load_state_dict(torch.load(f'./models/visual-unet-CNN-{channels}-{depth}-{tag}.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6096bec",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "\n",
    "#### 2. Define optimization\n",
    "`CrossEntropy` with weighted classes should work, however, `DiceLoss` would be more robust against the imbalanced targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccba54ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:02:23.335598Z",
     "iopub.status.busy": "2023-09-23T08:02:23.335335Z",
     "iopub.status.idle": "2023-09-23T08:02:23.343689Z",
     "shell.execute_reply": "2023-09-23T08:02:23.342759Z"
    }
   },
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        # apply softmax here instead to make this interchangeable with crossentropy\n",
    "        #X = nn.Softmax(dim=1)(X)\n",
    "        # unsqueeze classes\n",
    "        Y = nn.functional.one_hot(Y, self.num_classes)\n",
    "        # align axes\n",
    "        X = rearrange(X, 'b c h w -> b h w c')\n",
    "        # compute class weight\n",
    "        W = torch.zeros((self.num_classes,))\n",
    "        W = 1. / (torch.sum(Y, (0, 1, 2)) ** 2 + 1e-9)        \n",
    "        # compute weighted cross and union sums over b h w\n",
    "        cross = X * Y\n",
    "        cross = W * torch.sum(cross, (0, 1, 2))\n",
    "        cross = torch.sum(cross)\n",
    "        union = Y + X\n",
    "        union = W * torch.sum(union, (0, 1, 2))\n",
    "        union = torch.sum(union)\n",
    "        return 1. - 2. * (cross + 1e-9)/(union + 1e-9)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a020e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:02:23.346541Z",
     "iopub.status.busy": "2023-09-23T08:02:23.346203Z",
     "iopub.status.idle": "2023-09-23T08:02:23.354567Z",
     "shell.execute_reply": "2023-09-23T08:02:23.353552Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-6\n",
    "criteria = [DiceLoss(num_classes).to(DEVICE) for _ in range(len(models))]\n",
    "optimizers = [AdamW(model.parameters(), lr=learning_rate) for model in models]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e376adf4",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "\n",
    "#### 3. Define evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d44a0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:02:23.358929Z",
     "iopub.status.busy": "2023-09-23T08:02:23.358666Z",
     "iopub.status.idle": "2023-09-23T08:02:23.371086Z",
     "shell.execute_reply": "2023-09-23T08:02:23.370290Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics = [{ 'f1-score': F1Score(task='multiclass', num_classes=num_classes).to(DEVICE),\n",
    "             'jaccard': JaccardIndex(task='multiclass', num_classes=num_classes).to(DEVICE) }\n",
    "           for _ in range(len(models))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3824d102",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "\n",
    "#### 4. Run training with validation\n",
    "We train all the models side-by-side on the same data batches for comparison.\n",
    "\n",
    "Samples separated into a `#validation_steps` partitions, each model runs training on each partition followed by validation on all `test_samples`. Batches generated online, so, test is never exactly the same which minimizes selection bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3a5a99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:02:23.373474Z",
     "iopub.status.busy": "2023-09-23T08:02:23.373312Z",
     "iopub.status.idle": "2023-09-23T08:02:23.376596Z",
     "shell.execute_reply": "2023-09-23T08:02:23.375925Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_epochs = 1\n",
    "validation_steps = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06017fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:02:23.380712Z",
     "iopub.status.busy": "2023-09-23T08:02:23.380398Z",
     "iopub.status.idle": "2023-09-23T18:40:58.032126Z",
     "shell.execute_reply": "2023-09-23T18:40:58.031004Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer = MultiTrainer(dataset, models, VIEW_SIZE, criteria, optimizers, metrics, tags=tags)\n",
    "results = trainer.run(train_samples, test_samples, batch_size, num_epochs, validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964c2254",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "\n",
    "#### 5. Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed809ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T18:41:00.053223Z",
     "iopub.status.busy": "2023-09-23T18:41:00.052790Z",
     "iopub.status.idle": "2023-09-23T18:41:00.310972Z",
     "shell.execute_reply": "2023-09-23T18:41:00.309937Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.plot_compare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fce9203",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T18:40:58.036556Z",
     "iopub.status.busy": "2023-09-23T18:40:58.036321Z",
     "iopub.status.idle": "2023-09-23T18:41:00.048538Z",
     "shell.execute_reply": "2023-09-23T18:41:00.047375Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.plot_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6727c1b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T18:41:00.315436Z",
     "iopub.status.busy": "2023-09-23T18:41:00.315231Z",
     "iopub.status.idle": "2023-09-23T18:41:02.483157Z",
     "shell.execute_reply": "2023-09-23T18:41:02.482213Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# some examples side-by-side\n",
    "sample = np.random.choice(samples)\n",
    "loader = DataLoader(RandomViewDataset(sample, VIEW_SIZE, max_samples=8), batch_size=8)\n",
    "for model in models:\n",
    "    model.eval()\n",
    "with torch.no_grad():\n",
    "    for X, Y in loader:\n",
    "        P = [torch.argmax(model(X.to(DEVICE)), axis=1).cpu() for model in models]\n",
    "        for i in range(X.shape[0]): # batch\n",
    "            fig, ax = plt.subplots(1, len(models) + 1, figsize=(12, 12))\n",
    "            ax[0].imshow(X[i,:].squeeze().numpy(), 'gray')\n",
    "            for n in range(1, len(models) + 1): # model\n",
    "                ax[n].imshow(P[n - 1][i,:].squeeze().numpy(), 'gray')\n",
    "                if i == 0:\n",
    "                    ax[n].set_title(f'{\"model\" if tags[n - 1] == \"\" else tags[n - 1]}', fontsize=10)\n",
    "            for n in range(len(models) + 1):\n",
    "                ax[n].axis('off')\n",
    "                if i == 0 and n == 0:\n",
    "                    ax[0].set_title('Input', fontsize=10)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee645a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = [] # sort in the order of performance\n",
    "for r, t, i in sorted(zip(trainer.results, tags, range(len(tags))),\n",
    "                      key=lambda v:v[0]['f1-score'] * v[0]['jaccard']):\n",
    "    print(f'{t:>4} f1-score: {r[\"f1-score\"]:<6} jaccard-index: {r[\"jaccard\"]} #{i}')\n",
    "    order.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f9f4e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# look closer with the same pregenerated batch within performance groups\n",
    "loader = DataLoader(RandomViewDataset(np.random.choice(samples), VIEW_SIZE, max_samples=4), batch_size=4)\n",
    "batch = []\n",
    "for X, Y in loader:\n",
    "    batch.append((X, Y))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for group in [order[:4], order[4:-4], order[-4:]]:\n",
    "        for X, Y in batch:\n",
    "            P = [torch.argmax(models[k](X.to(DEVICE)), axis=1).cpu() for k in group]\n",
    "            for i in range(X.shape[0]): # batch\n",
    "                fig, ax = plt.subplots(1, 5, figsize=(10, 10))\n",
    "                ax[0].imshow(X[i,:].squeeze().numpy(), 'gray')\n",
    "                for n in range(1, 5): # model\n",
    "                    ax[n].imshow(P[n - 1][i,:].squeeze().numpy(), 'gray')\n",
    "                    if i == 0:\n",
    "                        ax[n].set_title(f'{\"model\" if tags[group[n - 1]] == \"\" else tags[group[n - 1]]}',\n",
    "                                        fontsize=10)\n",
    "                for n in range(5):\n",
    "                    ax[n].axis('off')\n",
    "                    if i == 0 and n == 0:\n",
    "                        ax[0].set_title('Input', fontsize=10)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c48fe72",
   "metadata": {},
   "source": [
    "<a name=\"embeddings\"></a>\n",
    "\n",
    "#### Latent space evaluation\n",
    "To evaluate the embeddings produced by trained encoders we can use the basic types of pages identified in the [baselines exploration notebook](Doc-Classification-Baselines.ipynb#labels) -- our models should be able to tell them apart. Let's look how well these groups are separated in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c6da3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T18:41:02.486791Z",
     "iopub.status.busy": "2023-09-23T18:41:02.486605Z",
     "iopub.status.idle": "2023-09-23T18:41:02.497815Z",
     "shell.execute_reply": "2023-09-23T18:41:02.496848Z"
    }
   },
   "outputs": [],
   "source": [
    "classes = ['mixed','plain-text','form-table','non-doc']\n",
    "\n",
    "labeled = pd.read_csv('./data/labeled-sample.csv')\n",
    "labeled.groupby('label').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18306aea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T18:41:02.500869Z",
     "iopub.status.busy": "2023-09-23T18:41:02.500570Z",
     "iopub.status.idle": "2023-09-23T18:41:02.509606Z",
     "shell.execute_reply": "2023-09-23T18:41:02.508757Z"
    }
   },
   "outputs": [],
   "source": [
    "class TopViewDataset(Dataset):\n",
    "    \"\"\"\n",
    "    render full-page view\n",
    "    \"\"\"\n",
    "    def __init__(self, view_size: int, samples: list, labels: list, contrast: float = 0.):\n",
    "        self.view_size = view_size\n",
    "        # add non-docs for contrast if needed\n",
    "        n, c = int(len(samples) * contrast), max(labels)\n",
    "        self.samples = list(samples) + ['?'] * n\n",
    "        self.labels = labels\n",
    "        self.non_doc_class = c + 1\n",
    "        self.transform = NormalizeView()\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        source = self.samples[idx]\n",
    "\n",
    "        if source == '?': # non-doc sample for contrast\n",
    "            X = self.transform(make_negative_sample(self.view_size))\n",
    "            Y = self.non_doc_class\n",
    "            return X, Y\n",
    "        \n",
    "        # load source image\n",
    "        view = np.array(ImageOps.grayscale(Image.open(f'{ROOT}/data/images/{source}.png')))\n",
    "        # renderer full-page view\n",
    "        view = render.AgentView((255. - view).astype(np.uint8), self.view_size).top()\n",
    "        X = self.transform(view)\n",
    "        Y = self.labels[idx]\n",
    "        return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d813ea6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T18:41:02.513377Z",
     "iopub.status.busy": "2023-09-23T18:41:02.513071Z",
     "iopub.status.idle": "2023-09-23T18:41:03.025147Z",
     "shell.execute_reply": "2023-09-23T18:41:03.024562Z"
    }
   },
   "outputs": [],
   "source": [
    "text = labeled.loc[labeled['label']==1]\n",
    "loader = DataLoader(TopViewDataset(VIEW_SIZE, text['source'].to_list(), [1] * len(text)),\n",
    "                    batch_size=4, shuffle=False)\n",
    "for X, Y in loader:\n",
    "    for i in range(4):\n",
    "        fig, ax = plt.subplots(figsize=(3, 3))\n",
    "        ax.imshow(X[i,:].squeeze(), 'gray')\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bde4137",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T18:41:03.028840Z",
     "iopub.status.busy": "2023-09-23T18:41:03.028670Z",
     "iopub.status.idle": "2023-09-23T18:41:03.033833Z",
     "shell.execute_reply": "2023-09-23T18:41:03.033219Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_embeddings(dataset: Dataset, backbone: nn.Module, reduce: nn.Module = None,\n",
    "                   target_index: int = None, batch_size: int = 16):\n",
    "    calc = VisualEncoder(backbone, reduce).to(DEVICE)\n",
    "    calc.eval()\n",
    "    embeddings, labels = None, []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in DataLoader(dataset, batch_size=32):\n",
    "            vectors = calc(inputs.to(DEVICE)).cpu().numpy().squeeze()\n",
    "            embeddings = vectors if embeddings is None else np.concatenate([embeddings, vectors], axis=0)\n",
    "            labels += list(targets.cpu().numpy()) if target_index is None \\\n",
    "                                else list(targets[target_index].cpu().numpy())\n",
    "    return embeddings, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3142eba1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T18:41:03.036586Z",
     "iopub.status.busy": "2023-09-23T18:41:03.036278Z",
     "iopub.status.idle": "2023-09-23T18:41:03.040773Z",
     "shell.execute_reply": "2023-09-23T18:41:03.040160Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_profile(embeddings, labels):\n",
    "    \"\"\"\n",
    "    Principal Components and Linear Discriminant\n",
    "    \"\"\"\n",
    "    pca, lda = PCA(), LDA()\n",
    "    scaler = StandardScaler().fit(embeddings)\n",
    "    P, L = pca.fit_transform(scaler.transform(embeddings)), lda.fit_transform(embeddings, labels)\n",
    "    # compute 2d tSNE\n",
    "    #T = TSNE(n_components=2, perplexity=90).fit_transform(embeddings)\n",
    "    return P, pca.explained_variance_ratio_, L, lda.explained_variance_ratio_ #, T\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd97fb60",
   "metadata": {},
   "source": [
    "Let's check two main components interaction with never seen non-docs added to the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ece865b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T18:52:22.827298Z",
     "iopub.status.busy": "2023-09-23T18:52:22.826987Z",
     "iopub.status.idle": "2023-09-23T18:52:22.997970Z",
     "shell.execute_reply": "2023-09-23T18:52:22.997143Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_profiles(tags, profile, score):\n",
    "    # sort by silhouette-score\n",
    "    order = sorted(zip(tags, score, range(len(tags))), key=lambda x:x[1], reverse=True)\n",
    "    # show explained variance ratio profiles\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    for t, s, i in order:\n",
    "        plt.plot(profile[i][:7], color=f'C{i}', marker='oosDD'[len(t)],\n",
    "                 label=f'score: {s:.4f}  model: {\"base\" if t==\"\" else t}' )\n",
    "    plt.title('PCA explained variance ratio profiles', fontsize=10)\n",
    "    plt.legend(title='Clusters silhouette-score', fontsize=8, bbox_to_anchor=(1, 1), frameon=False)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a136978",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T18:41:03.043522Z",
     "iopub.status.busy": "2023-09-23T18:41:03.043218Z",
     "iopub.status.idle": "2023-09-23T18:52:22.825036Z",
     "shell.execute_reply": "2023-09-23T18:52:22.824191Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 25% non-docs for contrast\n",
    "classes = ['mixed','plain-text','form-table','non-doc']\n",
    "dataset = TopViewDataset(VIEW_SIZE, labeled['source'], labeled['label'], contrast=0.25)\n",
    "profiles, scores = [], []\n",
    "results = trainer.results\n",
    "for name, encoder in zip(tags, encoders):\n",
    "    name = 'Base' if name == '' else name\n",
    "    # use model encoder to get embeddings\n",
    "    embeddings, labels = get_embeddings(dataset, encoder, reduce=nn.AdaptiveAvgPool2d((1, 1)))\n",
    "    P, pca_ratios, L, lda_ratios = get_profile(embeddings, labels)\n",
    "    scores.append(silhouette_score(P[:,:3], labels, metric='euclidean'))\n",
    "    score = silhouette_score(L, labels, metric='euclidean')\n",
    "    results[len(scores) - 1]['contrast-score'] = score\n",
    "    profiles.append(pca_ratios)\n",
    "    # classes aggregated\n",
    "    centers = np.array([np.median(L[np.where(np.array(labels) == k)], axis=0) for k in range(len(classes))])\n",
    "    cmap = colormaps['gist_rainbow']\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(7, 3.2))\n",
    "    for j in range(len(classes)):\n",
    "        s = np.where(np.array(labels) == j)\n",
    "        ax[0].scatter(P[s,0], P[s,1], s=3, color=cmap(j/3), alpha=0.3)\n",
    "        ax[1].scatter(L[s,0], L[s,1], s=3, color=cmap(j/3), alpha=0.3)\n",
    "    for j in range(len(classes)):\n",
    "        ax[1].scatter(centers[j,0], centers[j,1], color=cmap(j/3),\n",
    "                      s=75, marker='pos^'[j], edgecolor='black', label=classes[j])\n",
    "    for j, (t, s) in enumerate([('PCA', scores[-1]),('LDA', score)]):\n",
    "        ax[j].set_xticks([])\n",
    "        ax[j].set_yticks([])\n",
    "        ax[j].set_title(f'{t}  silhouette-score: {s:.4f}', fontsize=10)\n",
    "    ax[1].legend(title=f'{name} model', fontsize=8, bbox_to_anchor=(1, 1), frameon=False)\n",
    "    plt.show()\n",
    "    \n",
    "# compare all\n",
    "plot_profiles(tags, profiles, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab847f0",
   "metadata": {},
   "source": [
    "<a name=\"results\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29acc706",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# documents only\n",
    "classes = ['mixed','plain-text','form-table']\n",
    "dataset = TopViewDataset(VIEW_SIZE, labeled['source'], labeled['label'], contrast=0)\n",
    "profiles, scores = [], []\n",
    "for name, encoder in zip(tags, encoders):\n",
    "    name = 'Base' if name == '' else name\n",
    "    # use model encoder to get embeddings\n",
    "    embeddings, labels = get_embeddings(dataset, encoder, reduce=nn.AdaptiveAvgPool2d((1, 1)))\n",
    "    P, pca_ratios, L, lda_ratios = get_profile(embeddings, labels)\n",
    "    scores.append(silhouette_score(P[:,:3], labels, metric='euclidean'))\n",
    "    score = silhouette_score(L, labels, metric='euclidean')\n",
    "    results[len(scores) - 1]['cluster-score'] = score\n",
    "    profiles.append(pca_ratios)\n",
    "    # classes\n",
    "    centers = np.array([np.median(L[np.where(np.array(labels) == k)], axis=0) for k in range(len(classes))])    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(7, 3.2))\n",
    "    for j in range(len(classes)):\n",
    "        s = np.where(np.array(labels) == j)\n",
    "        ax[0].scatter(P[s,0], P[s,1], s=3, color=cmap(j/3), alpha=0.3)\n",
    "        ax[1].scatter(L[s,0], L[s,1], s=3, color=cmap(j/3), alpha=0.3)\n",
    "    for j in range(len(classes)):\n",
    "        ax[1].scatter(centers[j,0], centers[j,1], color=cmap(j/3),\n",
    "                      s=75, marker='pos^'[j], edgecolor='black', label=classes[j])\n",
    "    for j, (t, s) in enumerate([('PCA', scores[-1]),('LDA', score)]):\n",
    "        ax[j].set_xticks([])\n",
    "        ax[j].set_yticks([])\n",
    "        ax[j].set_title(f'{t}  silhouette-score: {s:.4f}', fontsize=10)\n",
    "    ax[1].legend(title=f'{name} model', fontsize=8, bbox_to_anchor=(1, 1), frameon=False)\n",
    "    plt.show()\n",
    "\n",
    "# compare all\n",
    "plot_profiles(tags, profiles, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8d6337",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame.from_dict(results)\n",
    "results['model'] = tags\n",
    "results.set_index('model').style.format('{:.4f}').background_gradient('Greens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3176d2dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T18:52:23.000172Z",
     "iopub.status.busy": "2023-09-23T18:52:22.999953Z",
     "iopub.status.idle": "2023-09-23T18:52:23.425347Z",
     "shell.execute_reply": "2023-09-23T18:52:23.424403Z"
    }
   },
   "source": [
    "    for tag, model, encoder in zip(tags, models, encoders):\n",
    "        torch.save(model.state_dict(), f'./models/visual-unet-CNN-{channels}-{depth}-{tag}.pt')\n",
    "        if tag == 'RA': # save best trained encoder\n",
    "            torch.save(encoder.state_dict(), f'./models/visual-backbone-CNN.pt')\n",
    "        \n",
    "    results.to_csv(f'./models/visual-unet-CNN-{channels}-{depth}.csv')\n",
    "    trainer.save(f'./models/visual-unet-CNN-{channels}-{depth}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ad80f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
