{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ade47d0",
   "metadata": {},
   "source": [
    "# Doc-Indexing Pipeline\n",
    "\n",
    "\n",
    "* Data\n",
    "* Preprocessing and normalization\n",
    "* Phrase/Pattern search with `ElasticSearch`\n",
    "\n",
    "    * [Experiment with `ELSER`]()\n",
    "    * [Experiment with `shape-queries`]()\n",
    "    * Insert format\n",
    "    * Text-blocks bulk-insert\n",
    "    * Input-blocks bulk-insert\n",
    "    * Queries\n",
    "    \n",
    "* [Semantic search](#qdrant) with `Qdrant`\n",
    "    * Doc-titles bulk insert\n",
    "    * Sentences bulk-insert\n",
    "    * Queries\n",
    "\n",
    "* Indexing images\n",
    "    * As layout-blocks and text-content if any\n",
    "    * For `logos` recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97d8532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytesseract as ts\n",
    "\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps\n",
    "from unidecode import unidecode\n",
    "from torch.cuda import is_available\n",
    "from elasticsearch import Elasticsearch\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ab809d",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a991f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc-level lookup table\n",
    "docs = pd.read_csv('./data/forms.csv.gz')\n",
    "docs = docs.loc[docs['lang'].isin(['en','fr','sp'])].fillna('')\n",
    "docs['taxonomy'] = docs.apply(lambda r:f\"{r['type']}{r['sub']}\".strip().upper(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c53b829",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOX = ['left','top','right','bottom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0c1e3b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0279, 0.0196, 0.0028)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SCALE = 1000 # for integer ranges version\n",
    "\n",
    "pages = pd.read_csv('./data/pages.csv.gz')\n",
    "mean = pages.mean(numeric_only=True)\n",
    "DW, DH, DS = np.round(mean.loc[['word-width','word-height','space']], 4)\n",
    "#DW, DH, DS = np.round(mean.loc[['word-width','word-height','space']] * SCALE).astype(int)\n",
    "DW, DH, DS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02845349",
   "metadata": {},
   "source": [
    "We are going to index document pages textual content, layout and form-input blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf022ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(data):\n",
    "    \"\"\"\n",
    "    drop `code` (xml etc.)\n",
    "    \"\"\"\n",
    "    data.loc[:,'text'] = data['text'].fillna('').astype(str).str.strip()\n",
    "    return data.loc[~data['text'].str.startswith('<')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a8add8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_bbox(data):\n",
    "    \"\"\"\n",
    "    remove noise in bbox coordinates:\n",
    "    for data extracted with 200 dpi -- we only need 4 decimals\n",
    "    \"\"\"\n",
    "    #data = data[data[BOX].applymap(lambda x: isinstance(x, (int, float)))]\n",
    "    #data = data.loc[(data['right'] > data['left'])&(data['bottom'] > data['top'])]\n",
    "    data.loc[:,BOX] = np.round(data[BOX], 4)\n",
    "    #data.loc[:,columns] = np.round(data.loc[:,box] * SCALE).astype(int)\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebd02e7",
   "metadata": {},
   "source": [
    "### Text normalization and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e74ddbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text: str):\n",
    "    \"\"\"\n",
    "    normalize whitespace and unicode, keep casing and punctuation\n",
    "    remove `......` and `. . . . .` from anywhere\n",
    "    strip some non-word characters â–¶ from the start/end\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', unidecode(text).replace('*', '')).strip()\n",
    "    text = re.sub(r'\\.{2,}', '', text)\n",
    "    text = re.sub(r'\\s[\\s.]{3,}\\s', ' ', text)\n",
    "    \n",
    "    # remove hyphen between digit and letter (for taxonomy pattern matching)\n",
    "    text = re.sub(r'(?<=[A-Z])-(?=\\d)', '', re.sub(r'(?<=\\d)-(?=[A-Z])', '', text))\n",
    "    \n",
    "    return unidecode(re.sub('^[^a-zA-Z0-9\\(\\$]*|[^a-zA-Z0-9\\)]*$', '', text)).strip(' .')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0309463a",
   "metadata": {},
   "source": [
    "    # sample outcome\n",
    "    for d in docs.sample().to_dict('records'):    \n",
    "        files = pages[pages['doc']==d['file']]['source'].to_list()\n",
    "        for source in files:\n",
    "            data = data_filter(pd.read_csv(f'data/info/{source}.csv.gz'))\n",
    "            print(d['taxonomy'])\n",
    "            print('--------------------------------------------------------------------------')\n",
    "            for d in data.to_dict('records'):\n",
    "                print(str(d['text']).strip())\n",
    "                print('--------------------------------------------------------------------------')\n",
    "                print(normalize(str(d['text'])))\n",
    "                print('==========================================================================')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c17a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee7803ab",
   "metadata": {},
   "source": [
    "<a name=\"elastic\"></a>\n",
    "<img src=\"assets/elasticsearch.png\"\n",
    "     style=\"display:inline;float:left;vertical-aligh:middle;margin-right:15px\"/>\n",
    "\n",
    "## Phrase/Pattern search with `ElasticSearch`\n",
    "\n",
    "In this project we need the way to search for exact and fuzzy match in a spacial layout context. We index both `word` and `block` levels along with the relative coordinates on the page. The `word` blocks allow us to run a fuzzy match for the alpha-numeric patterns (taxonomy markers); the `block` may help with search for input labels/instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0df0dd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "eclient = Elasticsearch(\n",
    "    hosts=[os.environ['ELASTIC_URI']],\n",
    "    basic_auth=('elastic', os.environ['ELASTIC_PASSWORD']),\n",
    "    verify_certs=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "165fc319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eclient.indices.delete(index='doc-pages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9485407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX = 'doc-pages'\n",
    "\n",
    "SETTINGS = {\n",
    "    \"analysis\": {\n",
    "        \"analyzer\": {\n",
    "            \"custom_analyzer\": {\n",
    "                \"type\": \"custom\",\n",
    "                \"tokenizer\": \"whitespace\",\n",
    "                \"filter\": [\"lowercase\"]\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "MAPPINGS = {\n",
    "    \"properties\": {\n",
    "        # ELSER ml setup\n",
    "        #\"ml.tokens\": {\n",
    "        #    \"type\": \"rank_features\" \n",
    "        #},\n",
    "        \"content\": {\n",
    "            \"type\": \"text\",\n",
    "            \"analyzer\": \"custom_analyzer\"\n",
    "        },\n",
    "        \"block_type\": {\n",
    "            \"type\": \"keyword\"\n",
    "        },\n",
    "        \"font_size\": {\n",
    "            \"type\": \"byte\"\n",
    "        },\n",
    "        \"display\": {\n",
    "            \"type\": \"keyword\"\n",
    "        },\n",
    "        \"page_id\": {\n",
    "            \"type\": \"keyword\"\n",
    "        },\n",
    "        \"doc_id\": {\n",
    "            \"type\": \"keyword\"\n",
    "        },\n",
    "        \"taxonomy_id\": {\n",
    "            \"type\": \"keyword\"\n",
    "        },\n",
    "        \"orig\": {\n",
    "            \"type\": \"keyword\"\n",
    "        },\n",
    "        \"lang\": {\n",
    "            \"type\": \"keyword\"\n",
    "        },\n",
    "        # for range-queries experiment: short vs. float?\n",
    "        \"left\": {\n",
    "            \"type\": \"float\"\n",
    "        },\n",
    "        \"top\": {\n",
    "            \"type\": \"float\"\n",
    "        },\n",
    "        \"right\": {\n",
    "            \"type\": \"float\"\n",
    "        },\n",
    "        \"bottom\": {\n",
    "            \"type\": \"float\"\n",
    "        },\n",
    "        # for shape-queries experiment\n",
    "        #\"box\": {\n",
    "        #    \"type\": \"shape\"\n",
    "        #},\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26f61736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if not eclient.indices.exists(index=INDEX):\n",
    "    eclient.indices.create(index=INDEX, settings=SETTINGS, mappings=MAPPINGS)\n",
    "print(eclient.indices.exists(index=INDEX))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69d0f28",
   "metadata": {},
   "source": [
    "#### [Experiment with `ELSER`](https://www.elastic.co/blog/may-2023-launch-information-retrieval-elasticsearch-ai-model)\n",
    "\n",
    "Create the default ingestion pipeline to enable [ELSER](https://www.elastic.co/guide/en/machine-learning/8.8/ml-nlp-elser.html#download-deploy-elser) (f have access):\n",
    "    \n",
    "    curl -XPUT $ELASTIC_URI/_ingest/pipeline/form_blanks_pipeline \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"description\": \"Indexting text-bloks extracted from pdf-form-blanks for ELSER\",\n",
    "        \"processors\": [\n",
    "            {\n",
    "                \"inference\": {\n",
    "                    \"model_id\": \".elser_model_1\",\n",
    "                    \"target_field\": \"ml\",\n",
    "                    \"field_map\": {\n",
    "                        \"text\": \"text_field\"\n",
    "                    },\n",
    "                    \"inference_config\": {\n",
    "                        \"text_expansion\": { \n",
    "                            \"results_field\": \"tokens\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }'\n",
    "\n",
    "    curl -XPUT $ELASTIC_URI/documents \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"settings\": {\n",
    "            \"index.default_pipeline\": \"form_blanks_pipeline\"\n",
    "        }\n",
    "    }'\n",
    "\n",
    "\n",
    "Inference:    \n",
    "    \n",
    "    curl -GET $ELASTIC_URI/documents/_search\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "       \"query\": {\n",
    "            \"text_expansion\": {\n",
    "                 \"ml.tokens\": {\n",
    "                     \"model_id\":\".elser_model_1\",\n",
    "                     \"model_text\":\" ... \"\n",
    "                 }\n",
    "            }\n",
    "        }\n",
    "    }'\n",
    "    \n",
    "#### Experiment with [`shape-queries`](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-shape-query.html)\n",
    "The `shape` type vs. `geo_shape` type allows arbitrary values for coordinates (instead of `lat,lon` ranges). We have a relative scale (min width and height units) and mostly interested in `distance` and `within` relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "815701b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BBOX (10.00, 90.00, 90.00, 10.00)'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bbox_shape(d: dict):\n",
    "    \"\"\"\n",
    "    Convert bounding box (%scale) into a polygon to use geo-spacial queries\n",
    "    WKT specification expects the following order: minX, maxX, maxY, minY\n",
    "    \"\"\"\n",
    "    l, t, r, b = np.array([d['left'], d['top'], d['right'], d['bottom']]).astype(float) * 100\n",
    "    return f\"BBOX ({l:.2f}, {r:.2f}, {b:.2f}, {t:.2f})\"\n",
    "\n",
    "\n",
    "bbox_shape({'left':0.1, 'top':0.1,'right':0.9,'bottom':0.9})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778aa08a",
   "metadata": {},
   "source": [
    "#### Insert format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c84b095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_info(data, doc) -> dict:    \n",
    "    display = { 0:'h', 1:'v' } # horizontal and vertical\n",
    "    return {\n",
    "        'content': normalize_text(str(data['text'])),\n",
    "        'block_type': data['block-type'],\n",
    "        \n",
    "        # useful for identifying page title\n",
    "        'font_size': int(data['font-size']) if data['font-size'] > 0 else None,\n",
    "        \n",
    "        # sin and cos define text-line orientataion:\n",
    "        # could be different from the main content -- set flag -- h[orizontal] v[ertical] d[iagonal]\n",
    "        'display': '' if np.isnan(data['sin']) else display.get(data['sin'], 'd'),\n",
    "        \n",
    "        # layout\n",
    "        #'box': bbox_shape(data),        \n",
    "        'left': data['left'],\n",
    "        'top': data['top'],\n",
    "        'right': data['right'],\n",
    "        'bottom': data['bottom'],\n",
    "        \n",
    "        # page props\n",
    "        'page_id': data['page'],\n",
    "        'doc_id': doc['file'],\n",
    "        'taxonomy_id': doc['taxonomy'],\n",
    "        'taxonomy_ext': doc['ext'],\n",
    "        'lang': doc['lang'],\n",
    "        'orig': doc['orig'],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c62fa778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty pages: 0  no-whitespace: 230\n"
     ]
    }
   ],
   "source": [
    "# get stats on preprocessing\n",
    "count, seq, char, skip, mashed = [],[],[],[],[]\n",
    "for doc in docs.to_dict('records'):    \n",
    "    files = pages[pages['file']==doc['file']]['source'].to_list()\n",
    "    for source in files:\n",
    "        data = filter_text(pd.read_csv(f'data/info/{source}.csv.gz'))\n",
    "        count.append(len(data))\n",
    "        if len(data) == 0:\n",
    "            skip.append(source)\n",
    "            continue\n",
    "        for d in data.to_dict('records'):\n",
    "            record = parse_info(d, doc)\n",
    "            text = record['content']\n",
    "            seq.append(len(text))\n",
    "            # check for too long words unless urls\n",
    "            longest_word = max([x for x in text.split() if x.find('/') == -1], key=len, default='')\n",
    "            char.append(len(longest_word))\n",
    "            if char[-1] > 50:\n",
    "                #print(source, '===', longest_word)\n",
    "                mashed.append(source)\n",
    "        \n",
    "print(f'empty pages: {len(skip)}  no-whitespace: {len(set(mashed))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc5ec95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mashed = [x for x in set(mashed) if x.startswith('que-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb2387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(10, 3))\n",
    "X = sorted(Counter(count).items(), key=lambda x:x[0])\n",
    "ax[0].scatter([x[0] for x in X],[x[1] + 1 for x in X], s=3)\n",
    "ax[0].axvline(x=1000, linestyle=':', color='C3')\n",
    "ax[0].set_yscale('log')\n",
    "ax[0].set_title('estimate batch-size')\n",
    "X = sorted(Counter(seq).items(), key=lambda x:x[0])\n",
    "ax[1].scatter([x[0] for x in X],[x[1] + 1 for x in X], s=3)\n",
    "ax[1].set_yscale('log')\n",
    "ax[1].set_title('text length dist')\n",
    "X = sorted(Counter(char).items(), key=lambda x:x[0])\n",
    "ax[2].scatter([x[0] for x in X],[x[1] + 1 for x in X], s=3)\n",
    "ax[2].axvline(x=50, linestyle=':', color='C3')\n",
    "ax[2].set_yscale('log')\n",
    "ax[2].set_title('word length dist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48828da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_ndjson(data, doc, parse) -> list:\n",
    "    \"\"\"\n",
    "    Generator for text-blocks data ingest\n",
    "    \"\"\"\n",
    "    for d in data.to_dict('records'):\n",
    "        record = parse(d, doc)\n",
    "        if record['content'] == '': # skip empty\n",
    "            continue\n",
    "        taxonomy_ext = re.sub(r'\\W+', '', doc['ext']).upper()\n",
    "        ID = f\"{doc['file']}-{d['page']}-{d['block']}\".upper()\n",
    "        yield json.dumps({'index':{'_index':INDEX,'_id':ID}})\n",
    "        yield json.dumps({ x:record[x] for x in MAPPINGS['properties'] if not x.startswith('ml.') })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fff4a2",
   "metadata": {},
   "source": [
    "#### Text-blocks bulk-insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "582ba889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution time: 65.70min  errors in 0/0.00% pages\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "errors, doc_count, page_count = 0, 0, 0\n",
    "for doc in docs.to_dict('records'):    \n",
    "    files = pages[pages['file']==doc['file']]['source'].to_list()\n",
    "    for source in files:\n",
    "        if source in skip + mashed:\n",
    "            continue\n",
    "            \n",
    "        data = filter_text(pd.read_csv(f'data/info/{source}.csv.gz'))\n",
    "        # we will process images separately\n",
    "        data = data.loc[data['block-type']!='image']\n",
    "        if len(data) == 0:\n",
    "            continue\n",
    "        \n",
    "        data = normalize_bbox(data)        \n",
    "        # include trailing space to link tailing word\n",
    "        data.loc[data['block-type']=='word','right'] += DS\n",
    "        data.loc[data['block-type']=='word','left'] -= DS\n",
    "        \n",
    "        data.index.name = 'block'\n",
    "        data = data.reset_index()        \n",
    "        data['block'] = data.apply(lambda r:f\"{r['block-type'][0].upper()}{r['block']}\", axis=1)\n",
    "        inserts = data_to_ndjson(data, doc, parse_info)\n",
    "        result = eclient.bulk(index=INDEX, operations=inserts)\n",
    "        if result['errors']:\n",
    "            pass #print(result) # few blocks have invalid bounding box\n",
    "        errors += result['errors']\n",
    "        page_count += 1\n",
    "    doc_count += 1\n",
    "    print(f'done: {doc_count/len(docs):.2%}', end='\\r')\n",
    "        \n",
    "print(f'execution time: {(time() - start)/60:.2f}min  errors in {errors}/{errors/page_count:.2%} pages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab0b4dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eclient.search(index=INDEX, query={'match_all': {}})['hits']['hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "307b6be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_content(doc, page, size=1000):\n",
    "    query = {'bool': {'must': [{'match': {'doc_id': doc }}, {'match': {'page_id': page }}]}}\n",
    "    sort = [{'top': {'order': 'asc'}}, {'left': {'order': 'asc'}}]\n",
    "    hits = eclient.search(index=INDEX, query=query, sort=sort, size=size)\n",
    "    return [hit['_source'] for hit in hits['hits']['hits']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16376daa",
   "metadata": {},
   "source": [
    "#### Input-blocks bulk-insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42134ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pages with inputs\n",
    "forms = [str(x) for x in Path(f'./data/inputs').glob('*.csv.gz')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88310c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_inputs(doc, inputs):\n",
    "    \"\"\"\n",
    "    remove hoidden pdf-utils, keep only user-filled\n",
    "    \"\"\"\n",
    "    # filter out pdf-utils\n",
    "    inputs = inputs.loc[inputs['field_type_string']!='Button']\n",
    "    hidden = inputs.loc[(inputs['field_type_string']=='Text')&(inputs['right'] - inputs['left'] < DH)]\n",
    "    inputs = inputs.loc[~inputs.index.isin(hidden.index)]\n",
    "    if len(inputs) == 0:\n",
    "        return inputs\n",
    "    # get content from page (already indexed)\n",
    "    data = get_page_content(doc['file'], inputs.iloc[0]['page'], size=10000)\n",
    "    if len(data) == 0:\n",
    "        return inputs\n",
    "    \n",
    "    # auxiliary fields -- value comes in when other fields got filled:\n",
    "    # build low-res word-presence map to detect overlap easily\n",
    "    data = pd.DataFrame.from_dict(data)[['content','block_type'] + BOX]\n",
    "    M = np.round(data[data['block_type']=='word'].loc[:,BOX] * 100).astype(int)\n",
    "    if len(M) == 0:\n",
    "        return inputs\n",
    "    W, H = M[['right','bottom']].max().astype(int)\n",
    "    matrix = np.zeros((H, W))\n",
    "    for d in M.to_dict('records'):\n",
    "        matrix[int(d['top']) + 1:int(d['bottom']), int(d['left']) + 1:int(d['right'])] = 1\n",
    "    nested = []\n",
    "    test = np.round(inputs.loc[:,BOX] * 100).astype(int)\n",
    "    for i in inputs.index: # check if input space is already occupied\n",
    "        l, t, r, b = test.loc[i,:].values\n",
    "        if np.any(matrix[int(t) + 1:int(b), int(l) + 1:int(r)]):\n",
    "            nested.append(i)\n",
    "    # filter-out nested\n",
    "    return inputs.loc[~inputs.index.isin(nested)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19fb6ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_inputs(data, doc) -> dict:\n",
    "    return {\n",
    "        'content': f\"{data['field_type_string']} NAME: {data['field_name']} LABEL: {data['field_label']}\",\n",
    "        'block_type': 'input',\n",
    "        \n",
    "        # hints for visual\n",
    "        'font_size': int(data['text_fontsize']) if data['text_fontsize'] > 0 else None,\n",
    "        # display=0 seems to be the one we need, others may be conditional or hidden utils\n",
    "        'display': '' if np.isnan(data['field_display']) else str(data['field_display']),\n",
    "        \n",
    "        # layout\n",
    "        #'box': bbox_shape(data),\n",
    "        'left': data['left'],\n",
    "        'top': data['top'],\n",
    "        'right': data['right'],\n",
    "        'bottom': data['bottom'],\n",
    "        \n",
    "        # page props\n",
    "        'page_id': data['page'],\n",
    "        'doc_id': doc['file'],\n",
    "        'taxonomy_id': doc['taxonomy'],\n",
    "        'taxonomy_ext': doc['ext'],\n",
    "        'lang': doc['lang'],\n",
    "        'orig': doc['orig'],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d407d0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution time: 20.93min  errors in 0/0.00% pages\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "errors, count = 0, 0\n",
    "for file in forms:\n",
    "    doc = '-'.join(file.split('/').pop().split('-')[:-1])\n",
    "    doc = docs.loc[docs['file']==doc]\n",
    "    if len(doc) == 0:\n",
    "        continue\n",
    "    doc = doc.to_dict('records')[0]\n",
    "    if not doc['lang'] in ['en','fr','sp']:\n",
    "        continue\n",
    "        \n",
    "    inputs = pd.read_csv(file)\n",
    "    inputs = inputs.loc[(~inputs['field_type_string'].isna())&(~inputs['page'].isna())]\n",
    "    inputs.loc[:,['field_name','field_label']] = inputs[['field_name','field_label']].fillna('')    \n",
    "    # remove pdf-doc-tree: form1[0].Page1[0].Name_subform[0].TaxYear_group[0].TaxYearDate[0] -> TaxYearDate\n",
    "    inputs['field_name'] = inputs['field_name'].apply(lambda x:x.split('].').pop().split('[')[0]).to_list()\n",
    "    \n",
    "    inputs = normalize_bbox(inputs)\n",
    "    inputs = filter_inputs(doc, inputs)\n",
    "    if len(inputs) == 0:\n",
    "        continue\n",
    "\n",
    "    inputs.index = inputs.index.map(lambda x:f'I{x}') # mark as input-block\n",
    "    inputs.index.name = 'block'\n",
    "    inputs = inputs.reset_index()\n",
    "    inserts = data_to_ndjson(inputs, doc, parse_inputs)\n",
    "    result = eclient.bulk(index=INDEX, operations=inserts)\n",
    "    if result['errors']:\n",
    "        pass #print(result)\n",
    "    errors += result['errors']\n",
    "    count += 1\n",
    "    print(f'done: {count/len(forms):.2%}', end='\\r')\n",
    "        \n",
    "print(f'execution time: {(time() - start)/60:.2f}min  errors in {errors}/{errors/count:.2%} pages')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634598c6",
   "metadata": {},
   "source": [
    "### Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bfccfdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxonomy pattern fuzzy search success: 72.50%  avg. execution time: 0.0143sec\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "results = []\n",
    "for d in docs.to_dict('records'):\n",
    "    pattern = d['file'][5:] if d['file'][:5] == 'irs-f' else d['file'][4:]\n",
    "    query = {'bool': {'must': [{'match': {'taxonomy_id': d['taxonomy']}}],\n",
    "             'filter': [{'fuzzy': {'content': {'value': d['taxonomy'], 'fuzziness': 2 }}}] }}\n",
    "    result = eclient.search(index=INDEX, query=query, size=10)\n",
    "    results.append(len(result['hits']['hits']))\n",
    "\n",
    "print(f'Taxonomy pattern fuzzy search success: {sum(np.array(results) > 0)/len(docs):.2%}  '\n",
    "      f'avg. execution time: {(time() - start)/len(docs):.4f}sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d02ddcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxonomy pattern exact match success: 76.31%  avg. execution time: 0.005sec\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "results = []\n",
    "for d in docs.to_dict('records'):\n",
    "    pattern = d['file'][5:] if d['file'][:5] == 'irs-f' else d['file'][4:]\n",
    "    query = {'bool': {'must': [{'match': {'taxonomy_id': d['taxonomy']}}],\n",
    "             'filter': [{'match_phrase': {'content': {'query': d['taxonomy'], 'slop': 10 }}}] }}\n",
    "    result = eclient.search(index=INDEX, query=query, size=10)\n",
    "    results.append(len(result['hits']['hits']))\n",
    "\n",
    "\n",
    "print(f'Taxonomy pattern exact match success: {sum(np.array(results) > 0)/len(docs):.2%}  '\n",
    "      f'avg. execution time: {(time() - start)/len(docs):.3f}sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b62f48fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxonomy pattern in the header area exact match success: 47.41%  avg. execution time: 0.0050sec\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "results = []\n",
    "for d in docs.to_dict('records'):\n",
    "    pattern = d['file'][5:] if d['file'][:5] == 'irs-f' else d['file'][4:]\n",
    "    query = {'bool': {'must': [{'match': {'taxonomy_id': d['taxonomy']}}],\n",
    "             'filter': [\n",
    "                 {'match_phrase': {'content': {'query': d['taxonomy'], 'slop': 10 }}},\n",
    "                 {'range': {'bottom': {'lt': 0.2 }}},\n",
    "             ] }}\n",
    "    result = eclient.search(index=INDEX, query=query, size=10)\n",
    "    results.append(len(result['hits']['hits']))\n",
    "\n",
    "print(f'Taxonomy pattern in the header area exact match success: {sum(np.array(results) > 0)/len(docs):.2%}  '\n",
    "      f'avg. execution time: {(time() - start)/len(docs):.4f}sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aaf6fb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxonomy pattern in the footer area exact match success: 67.58%  avg. execution time: 0.0049sec\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "results = []\n",
    "for d in docs.to_dict('records'):\n",
    "    pattern = d['file'][5:] if d['file'][:5] == 'irs-f' else d['file'][4:]\n",
    "    query = {'bool': {'must': [{'match': {'taxonomy_id': d['taxonomy']}}],\n",
    "             'filter': [\n",
    "                 {'match_phrase': {'content': {'query': d['taxonomy'], 'slop': 10 }}},\n",
    "                 {'range': {'top': {'gt': 0.8 }}},\n",
    "             ] }}\n",
    "    result = eclient.search(index=INDEX, query=query, size=10)\n",
    "    results.append(len(result['hits']['hits']))\n",
    "\n",
    "print(f'Taxonomy pattern in the footer area exact match success: {sum(np.array(results) > 0)/len(docs):.2%}  '\n",
    "      f'avg. execution time: {(time() - start)/len(docs):.4f}sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78b301f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs(taxonomy):\n",
    "    \"\"\"\n",
    "    retrieve all docs of the type\n",
    "    \"\"\"\n",
    "    query = {'bool': {'must': [{'match': {'taxonomy_id': taxonomy}}, {'match': {'block_type': 'input'}}]}}\n",
    "    aggs = {'docs': {'terms': {'field': 'doc_id'}}}\n",
    "    return eclient.search(index=INDEX, query=query, aggs=aggs)['aggregations']['docs']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9471857d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'key': 'irs-fw2c', 'doc_count': 512},\n",
       " {'key': 'irs-fw2', 'doc_count': 272},\n",
       " {'key': 'irs-fw2_21', 'doc_count': 230},\n",
       " {'key': 'irs-fw2g', 'doc_count': 159},\n",
       " {'key': 'irs-fw2as_21', 'doc_count': 158},\n",
       " {'key': 'irs-fw2gu_21', 'doc_count': 158},\n",
       " {'key': 'irs-fw2vi_21', 'doc_count': 158},\n",
       " {'key': 'irs-fw2as', 'doc_count': 126},\n",
       " {'key': 'irs-fw2gu', 'doc_count': 126},\n",
       " {'key': 'irs-fw2vi', 'doc_count': 126}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_docs('W2')['buckets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "581c507a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_inputs(doc, input_type=None, size=100):\n",
    "    \"\"\"\n",
    "    find all the inputs (of the type if specified) in the document\n",
    "    \"\"\"\n",
    "    must = [{'match': {'doc_id': doc}}, {'match': {'block_type': 'input'}}]\n",
    "    if input_type is not None:\n",
    "        must.append({'match_phrase': {'content': {'query': input_type, 'slop': 5 }}})\n",
    "    query = {'bool': {'must': must }}\n",
    "    sort = [{'page_id': {'order': 'asc'}}, {'top': {'order': 'asc'}}, {'left': {'order': 'asc'}}]\n",
    "    return eclient.search(index=INDEX, query=query, sort=sort, size=size)['hits']['hits']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6456740a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 1   Inpit-ID: IRS-FW2-1-I0    CheckBox NAME: c1_1 LABEL: \n",
      "Page: 1   Inpit-ID: IRS-FW2-1-I28   CheckBox NAME: c1_2 LABEL: \n",
      "Page: 1   Inpit-ID: IRS-FW2-1-I29   CheckBox NAME: c1_3 LABEL: \n",
      "Page: 1   Inpit-ID: IRS-FW2-1-I30   CheckBox NAME: c1_4 LABEL: \n",
      "Page: 2   Inpit-ID: IRS-FW2-2-I27   CheckBox NAME: c2_2 LABEL: \n",
      "Page: 2   Inpit-ID: IRS-FW2-2-I28   CheckBox NAME: c2_3 LABEL: \n",
      "Page: 2   Inpit-ID: IRS-FW2-2-I29   CheckBox NAME: c2_4 LABEL: \n",
      "Page: 3   Inpit-ID: IRS-FW2-3-I27   CheckBox NAME: c2_2 LABEL: \n",
      "Page: 3   Inpit-ID: IRS-FW2-3-I28   CheckBox NAME: c2_3 LABEL: \n",
      "Page: 3   Inpit-ID: IRS-FW2-3-I29   CheckBox NAME: c2_4 LABEL: \n",
      "Page: 5   Inpit-ID: IRS-FW2-5-I27   CheckBox NAME: c2_2 LABEL: \n",
      "Page: 5   Inpit-ID: IRS-FW2-5-I28   CheckBox NAME: c2_3 LABEL: \n",
      "Page: 5   Inpit-ID: IRS-FW2-5-I29   CheckBox NAME: c2_4 LABEL: \n",
      "Page: 7   Inpit-ID: IRS-FW2-7-I27   CheckBox NAME: c2_2 LABEL: \n",
      "Page: 7   Inpit-ID: IRS-FW2-7-I28   CheckBox NAME: c2_3 LABEL: \n",
      "Page: 7   Inpit-ID: IRS-FW2-7-I29   CheckBox NAME: c2_4 LABEL: \n",
      "Page: 9   Inpit-ID: IRS-FW2-9-I0    CheckBox NAME: c2_1 LABEL: \n",
      "Page: 9   Inpit-ID: IRS-FW2-9-I28   CheckBox NAME: c2_2 LABEL: \n",
      "Page: 9   Inpit-ID: IRS-FW2-9-I29   CheckBox NAME: c2_3 LABEL: \n",
      "Page: 9   Inpit-ID: IRS-FW2-9-I30   CheckBox NAME: c2_4 LABEL: \n"
     ]
    }
   ],
   "source": [
    "inputs = find_inputs('irs-fw2', input_type='CheckBox', size=1000)\n",
    "for hit in inputs:\n",
    "    print(f\"Page: {hit['_source']['page_id']:<3} Inpit-ID: {hit['_id']:<15} {hit['_source']['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "00f068bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_parent_block(input_data, size=100):\n",
    "    \"\"\"\n",
    "    get a text-block which contains this input\n",
    "    \"\"\"\n",
    "    query = {\n",
    "        'bool': {\n",
    "            'must': [{'match': {'doc_id': input_data['_source']['doc_id'] }},\n",
    "                     {'match': {'page_id': input_data['_source']['page_id'] }},\n",
    "                     {'match': {'block_type': 'block'}}],\n",
    "            #'filter': [{'shape': {'box': {'shape': hit['_source']['box'], 'relation': 'contains'}}}]            \n",
    "            'filter': [{'range': {'top': {'lte': input_data['_source']['top']}}},\n",
    "                       {'range': {'bottom': {'gte': input_data['_source']['bottom']}}},\n",
    "                       {'range': {'left': {'lte': input_data['_source']['left']}}},\n",
    "                       {'range': {'right': {'gte': input_data['_source']['right']}}}]\n",
    "        }}\n",
    "    sort = [{'top': {'order': 'asc'}}, {'left': {'order': 'asc'}}]\n",
    "    return eclient.search(index=INDEX, query=query, sort=sort, size=size)['hits']['hits']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2842edc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top: 0.0686 Left: 0.2165  ID: IRS-FW2-1-I0\n",
      "----------------------------------------------------\n",
      "====================================================\n",
      "Top: 0.3534 Left: 0.5696  ID: IRS-FW2-1-I28\n",
      "----------------------------------------------------\n",
      "Top: 0.2960 Left: 0.5477  Text: 11 Nonqualified plans 12a See instructions for box 12 C o ed 12b C o ed 12c C o ed 12d C o d e\n",
      "====================================================\n",
      "Top: 0.3534 Left: 0.6284  ID: IRS-FW2-1-I29\n",
      "----------------------------------------------------\n",
      "Top: 0.2960 Left: 0.5477  Text: 11 Nonqualified plans 12a See instructions for box 12 C o ed 12b C o ed 12c C o ed 12d C o d e\n",
      "====================================================\n",
      "Top: 0.3534 Left: 0.6873  ID: IRS-FW2-1-I30\n",
      "----------------------------------------------------\n",
      "Top: 0.2960 Left: 0.5477  Text: 11 Nonqualified plans 12a See instructions for box 12 C o ed 12b C o ed 12c C o ed 12d C o d e\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "for hit in inputs[:4]:\n",
    "    print(f\"Top: {hit['_source']['top']:.4f} Left: {hit['_source']['left']:.4f}  ID: {hit['_id']}\")\n",
    "    print('----------------------------------------------------')\n",
    "    for x in find_parent_block(hit):\n",
    "        print(f\"Top: {x['_source']['top']:.4f} Left: {x['_source']['left']:.4f}  Text: {x['_source']['content']}\")\n",
    "    print('====================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "df303f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top: 0.0686 Left: 0.2165  ID: IRS-FW2-1-I0\n",
      "----------------------------------------------------\n",
      "====================================================\n",
      "Top: 0.3534 Left: 0.5696  ID: IRS-FW2-1-I28\n",
      "----------------------------------------------------\n",
      "Top: 0.3347 Left: 0.5668  Text: Statutory\n",
      "Top: 0.3353 Left: 0.5449  Text: 13\n",
      "Top: 0.3428 Left: 0.5668  Text: employee\n",
      "Top: 0.3745 Left: 0.5449  Text: 14\n",
      "Top: 0.3747 Left: 0.5640  Text: Other\n",
      "====================================================\n",
      "Top: 0.3534 Left: 0.6284  ID: IRS-FW2-1-I29\n",
      "----------------------------------------------------\n",
      "Top: 0.3347 Left: 0.6256  Text: Retirement\n",
      "Top: 0.3428 Left: 0.6256  Text: plan\n",
      "====================================================\n",
      "Top: 0.3534 Left: 0.6873  ID: IRS-FW2-1-I30\n",
      "----------------------------------------------------\n",
      "Top: 0.3347 Left: 0.6845  Text: Third-party\n",
      "Top: 0.3428 Left: 0.6845  Text: sick\n",
      "Top: 0.3428 Left: 0.7013  Text: pay\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "def find_neighbor_words(input_data, size=3):\n",
    "    \"\"\"\n",
    "    get text-blocks which are at the same level of above:\n",
    "    inputs usually on the same line or right under\n",
    "    \"\"\"\n",
    "    query = {\n",
    "        'bool': {\n",
    "            'must': [{'match': {'doc_id': input_data['_source']['doc_id'] }},\n",
    "                     {'match': {'page_id': input_data['_source']['page_id'] }},\n",
    "                     {'match': {'block_type': 'word'}}],\n",
    "            # define a bigger bounding box: to fit-in as +line up/down and +word left/right\n",
    "            'filter': [{'range': {'top': {'gte': input_data['_source']['top'] - DH}}},\n",
    "                       {'range': {'bottom': {'lte': input_data['_source']['bottom'] + DH}}},\n",
    "                       {'range': {'left': {'gte': input_data['_source']['left'] - DW}}},\n",
    "                       {'range': {'right': {'lte': input_data['_source']['right'] + DW}}}]\n",
    "        }}\n",
    "    sort = [{'top': {'order': 'asc'}}, {'left': {'order': 'asc'}}]\n",
    "    return eclient.search(index=INDEX, query=query, sort=sort, size=size)['hits']['hits']\n",
    "\n",
    "\n",
    "for hit in inputs[:4]:\n",
    "    print(f\"Top: {hit['_source']['top']:.4f} Left: {hit['_source']['left']:.4f}  ID: {hit['_id']}\")\n",
    "    print('----------------------------------------------------')\n",
    "    for x in find_neighbor_words(hit, size=10):\n",
    "        print(f\"Top: {x['_source']['top']:.4f} Left: {x['_source']['left']:.4f}  Text: {x['_source']['content']}\")\n",
    "    print('====================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bb6be4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bffc87ab",
   "metadata": {},
   "source": [
    "<a name=\"qdrant\"></a>\n",
    "<img src=\"assets/qdrant.png\"\n",
    "     style=\"display:inline;float:left;vertical-aligh:middle;margin-right:15px\"/>\n",
    "     \n",
    "## Semantic search with `Qdrant`\n",
    "We split the textual content into sentences wherever possible (titles, instructional pages) and vectorize them.\n",
    "The sentence length and the sequence length thresholds derived from the stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "54cc7107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(data):\n",
    "    return data['text'].apply(normalize_text).apply(lambda x:x.split('. ')).explode()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc00bbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text, length = [],[]\n",
    "for doc in docs.sample(100).to_dict('records'):    \n",
    "    files = pages[pages['file']==doc['file']]['source'].to_list()\n",
    "    for source in files:\n",
    "        data = filter_text(pd.read_csv(f'data/info/{source}.csv.gz'))\n",
    "        if len(data) == 0:\n",
    "            continue\n",
    "        sentences = split_sentences(data)\n",
    "        sentences = sentences[sentences.str.len() > 30]\n",
    "        text += sentences.to_list()\n",
    "        length += sentences.str.len().to_list()\n",
    "\n",
    "plt.hist(length, bins=20)\n",
    "plt.title('sequense length dist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775270a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in np.random.choice(text, 10): print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdc1b66",
   "metadata": {},
   "source": [
    "#### Doc-titles bulk insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2dea2bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER_MODEL = 'distiluse-base-multilingual-cased-v1'\n",
    "device = 'cuda' if is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bd7747f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_encoder = SentenceTransformer(ENCODER_MODEL, device=device)\n",
    "semantic_encoder.max_seq_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9e01004c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4436, 512)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# document description only\n",
    "embeddings = semantic_encoder.encode(docs['desc'].to_list())\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10db2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xyc = pd.DataFrame(TSNE(n_components=2).fit_transform(embeddings), columns=['x','y'])\n",
    "xyc['cluster'] = docs['orig']/2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.scatter(xyc['x'], xyc['y'], c=xyc['cluster'], cmap='brg', alpha=0.5, s=5)\n",
    "ax.set_title('Documents embeddings by origin')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d32acbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qclient = QdrantClient(host=os.environ['QDRANT_HOST'], port=6333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "94cb2823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qclient.delete_collection(collection_name=INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c4fc1f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qclient.create_collection(\n",
    "    collection_name=INDEX, \n",
    "    vectors_config=VectorParams(size=embeddings.shape[1], distance=Distance.COSINE),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b2ce4955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'orig': 0,\n",
       "  'lang': 'en',\n",
       "  'taxonomy': '990',\n",
       "  'ext': 'EZ (Sch G)',\n",
       "  'text': 'Supplemental Information Regarding Fundraising or Gaming Activities'},\n",
       " {'orig': 0,\n",
       "  'lang': 'en',\n",
       "  'taxonomy': '990',\n",
       "  'ext': 'EZ (Sch L)',\n",
       "  'text': 'Transactions with Interested Persons'},\n",
       " {'orig': 0,\n",
       "  'lang': 'en',\n",
       "  'taxonomy': '990',\n",
       "  'ext': 'EZ (Sch N)',\n",
       "  'text': 'Liquidation, Termination, Dissolution, or Significant Disposition of Assets'},\n",
       " {'orig': 0,\n",
       "  'lang': 'en',\n",
       "  'taxonomy': '990',\n",
       "  'ext': 'PF',\n",
       "  'text': 'Return of Private Foundation or Section 4947(a)(1) Trust Treated as Private Foundation'},\n",
       " {'orig': 0,\n",
       "  'lang': 'en',\n",
       "  'taxonomy': '990',\n",
       "  'ext': '(Sch D)',\n",
       "  'text': 'Supplemental Financial Statements'}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload = docs.loc[:,['orig','lang','taxonomy','ext','desc']]\n",
    "payload.columns = ['orig','lang','taxonomy','ext','text']\n",
    "payload = payload.to_dict('records')\n",
    "payload[100:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2364aca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "qclient.upload_collection(\n",
    "    collection_name=INDEX,\n",
    "    vectors=embeddings,\n",
    "    payload=payload,\n",
    "    ids=None, # assign automatically\n",
    "    batch_size=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f47972",
   "metadata": {},
   "source": [
    "#### Sentences bulk-insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "472a934e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution time: 2.03min\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "counter = 0\n",
    "for d in docs.to_dict('records'):    \n",
    "    files = pages[pages['file']==d['file']]['source'].to_list()\n",
    "    text = []\n",
    "    for source in files:\n",
    "        if source in skip + mashed:\n",
    "            continue\n",
    "        data = filter_text(pd.read_csv(f'data/info/{source}.csv.gz'))\n",
    "        if len(data) > 0:\n",
    "            continue\n",
    "            \n",
    "        # split text into sentences\n",
    "        sentences = split_sentences(data)\n",
    "        sentences = sentences[sentences.str.len() > 30]\n",
    "        text += sentences.to_list()\n",
    "    \n",
    "    # compute vectors\n",
    "    embeddings = semantic_encoder.encode(text)\n",
    "    payload = pd.DataFrame(np.array([d['orig'], d['lang'], d['taxonomy'], d['ext']] * len(text)).reshape((-1,4)),\n",
    "                           columns=['orig','lang','taxonomy','ext'])\n",
    "    payload['text'] = text\n",
    "    payload = payload.to_dict('records')\n",
    "    \n",
    "    # db insert\n",
    "    qclient.upload_collection(\n",
    "        collection_name=INDEX,\n",
    "        vectors=embeddings,\n",
    "        payload=payload,\n",
    "        ids=None,\n",
    "        batch_size=512,\n",
    "    )\n",
    "    counter += 1\n",
    "    print(f'done: {counter/len(docs):.0%}', end='\\r')\n",
    "        \n",
    "print(f'execution time: {(time() - start)/60:.2f}min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21af1848",
   "metadata": {},
   "source": [
    "### Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "de9e1b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSearch:\n",
    "    def __init__(self, device):\n",
    "        self.collection = INDEX\n",
    "        # initialize encoder model\n",
    "        self.model = SentenceTransformer(ENCODER_MODEL, device=device)\n",
    "        # initialize Qdrant client\n",
    "        self.client = QdrantClient(host=os.environ['QDRANT_HOST'], port=6333)\n",
    "\n",
    "    def find(self, text: str, num: int = 5):\n",
    "        # convert text query into vector\n",
    "        vector = self.model.encode(text).tolist()\n",
    "        # search for closest vectors in the collection\n",
    "        results = self.client.search(\n",
    "            collection_name=self.collection,\n",
    "            query_vector=vector,\n",
    "            query_filter=None,\n",
    "            top=num,\n",
    "        )\n",
    "        # return payload of closest matches\n",
    "        return sorted([hit.payload for hit in results], key=lambda x:x['orig'])\n",
    "\n",
    "search = SemanticSearch(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "932b89f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ext': '',\n",
       "  'lang': 'en',\n",
       "  'orig': 0,\n",
       "  'taxonomy': '8615',\n",
       "  'text': 'Tax for Certain Children Who Have Unearned Income'},\n",
       " {'ext': '',\n",
       "  'lang': 'en',\n",
       "  'orig': 0,\n",
       "  'taxonomy': '15110',\n",
       "  'text': 'Additional Child Tax Credit Worksheet'},\n",
       " {'ext': 'EN',\n",
       "  'lang': 'fr',\n",
       "  'orig': 2,\n",
       "  'taxonomy': 'IM30',\n",
       "  'text': \"Entente entre exploitants associÃ©s relative Ã  l'impÃ´t minier\"},\n",
       " {'ext': 'V',\n",
       "  'lang': 'en',\n",
       "  'orig': 2,\n",
       "  'taxonomy': 'IN103',\n",
       "  'text': 'Refundable Tax Credit for Childcare Expenses'},\n",
       " {'ext': '',\n",
       "  'lang': 'fr',\n",
       "  'orig': 2,\n",
       "  'taxonomy': 'IN103',\n",
       "  'text': \"Le crÃ©dit d'impÃ´t remboursable pour frais de garde d'enfants\"},\n",
       " {'ext': '29.E',\n",
       "  'lang': 'fr',\n",
       "  'orig': 2,\n",
       "  'taxonomy': 'CO1175',\n",
       "  'text': 'Entente relative Ã  la taxe sur les services publics'},\n",
       " {'ext': '8.63',\n",
       "  'lang': 'fr',\n",
       "  'orig': 2,\n",
       "  'taxonomy': 'TP1029',\n",
       "  'text': \"CrÃ©dit d'impÃ´t pour frais d'adoption\"},\n",
       " {'ext': '13',\n",
       "  'lang': 'fr',\n",
       "  'orig': 2,\n",
       "  'taxonomy': 'LM93',\n",
       "  'text': 'Contestation en matiÃ¨re fiscale'},\n",
       " {'ext': '3.4',\n",
       "  'lang': 'fr',\n",
       "  'orig': 2,\n",
       "  'taxonomy': 'TP766',\n",
       "  'text': 'ImpÃ´t sur le revenu fractionnÃ©'},\n",
       " {'ext': 'V',\n",
       "  'lang': 'en',\n",
       "  'orig': 2,\n",
       "  'taxonomy': 'IN260',\n",
       "  'text': 'Tax on Lodging'}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.find('taxes related to children')\n",
    "#search.find('taxable retirement')\n",
    "#search.find('medical credits')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dd23fc",
   "metadata": {},
   "source": [
    "<a name=\"images\"></a>\n",
    "<h3>Indexing images</h3>\n",
    "\n",
    "For indexing images we use both:\n",
    "* `ElasticSearch`: enables `layout` search thus we want the image-blocks indexed; we also want the text some images contain\n",
    "* `Qdrant`: enables `similarity` search -- we want to recognize `logos` for the starter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fb77d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_text(data):\n",
    "    \"\"\"\n",
    "    some images contain text we try to extract\n",
    "    \"\"\"\n",
    "    images = data.loc[data['block-type']=='image']\n",
    "    if len(images) == 0:\n",
    "        return data\n",
    "    \n",
    "    source, page = data.iloc[0][['source','page']]\n",
    "    image = np.array(ImageOps.grayscale(Image.open(f'./data/images/{source}-{page}.png')))\n",
    "    scale = min(image.shape)\n",
    "    text = []\n",
    "    for t, l, b, r in images[['top','left','bottom','right']].values:\n",
    "        if b - t > 0.5 or r - l > 0.5:\n",
    "            text.append('IMAGE: ')\n",
    "            continue\n",
    "        t, l, b, r = int(t * scale), int(l * scale), int(b * scale), int(r * scale)\n",
    "        try:\n",
    "            clip = image[max(t - 5, 0):min(b + 5, image.shape[0]), max(l - 5, 0):min(r + 5, image.shape[1])]\n",
    "            t = ts.image_to_string(clip).strip()\n",
    "            t = ' '.join(re.split(r'\\W+', t)).strip()\n",
    "            text.append(f'IMAGE: {t}')\n",
    "        except:\n",
    "            text.append('IMAGE: ')\n",
    "    data.loc[data['block-type']=='image','text'] = text\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b65645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
