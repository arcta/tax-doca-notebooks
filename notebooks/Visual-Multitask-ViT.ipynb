{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dcb5e52",
   "metadata": {},
   "source": [
    "# Visual: supervised multi-task pretraining\n",
    "This notebook explores the `CNN`-based `visual-encoder` pretraining for downstream tasks of segmentation and classification. The training runs with the `bridge` (cross-connections) disabled to force most information captured at the embedding level (bottleneck).\n",
    "\n",
    "* [Dataset and Dataloader](#data)\n",
    "* [Backbone model](#model)\n",
    "* [Training and Validation](#run)\n",
    "    * [Define models](#1)\n",
    "    * [Define optimization](#2)\n",
    "    * [Define validation metrics](#3)\n",
    "    * [Run training](#4)\n",
    "    * [Evaluate results](#5)\n",
    "        * [Embeddings](#embeddings)\n",
    "    \n",
    "#### Observations\n",
    "The [embedding space](#embeddings) produced by this training shows better separation of the basic page-types in comparison with [initial experiment](Visual-Backbone-CNN.ipynb#embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cf6f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "from pathlib import Path\n",
    "from einops import rearrange\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torchmetrics import F1Score, JaccardIndex, ConfusionMatrix\n",
    "from torchsummary import summary\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81322936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load local notebook-utils\n",
    "from scripts.backbone import *\n",
    "from scripts.dataset import *\n",
    "from scripts.trainer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b58ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print('GPU' if DEVICE == 'cuda' else 'no GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4a4e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic segmentation masks\n",
    "masks = [str(x).split('/').pop() for x in Path('./data/masks').glob('*.png')\n",
    "         if not str(x).startswith('data/masks/que-')]\n",
    "len(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e624ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIEW_SIZE = 128\n",
    "LATENT_DIM = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66c0128",
   "metadata": {},
   "source": [
    "<a name=\"data\"></a>\n",
    "\n",
    "### Create dataset and dataloader\n",
    "We are going to generate data `online` which will slow down training, but would give us lots of flexibility.\n",
    "The input is a noisy version of the random page view-port (center, rotation, zoom). The targets are:\n",
    "* segmentation: text, input-space, table-outlines\n",
    "* value: info vs. noise\n",
    "* alignment: document rotation related to the view-port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e2191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use images with masks\n",
    "samples = masks #np.random.choice(masks, 640, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581cfc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALIGNMET_WEIGHT = get_alignment_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9bece7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample = np.random.choice(samples)\n",
    "# test loader\n",
    "n = 8\n",
    "loader = DataLoader(MultitaskPretrainingDataset(sample, VIEW_SIZE, max_samples=n), batch_size=n)\n",
    "alignment = ['N/A','No','Yes']\n",
    "# show first batch\n",
    "for X, (Y1, Y2, Y3, Y4) in loader:\n",
    "    print(f'source: {sample}\\nX: {X.shape}  Y1:{Y1.shape}  Y2:{Y2.shape}  Y3:{Y3.shape}  Y4:{Y4.shape}')\n",
    "    for i in range(n):\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(8, 8))\n",
    "        ax[0].imshow(X[i,:].squeeze(), 'gray')\n",
    "        ax[0].axis('off')\n",
    "        # restore channels to avoid visual confusion\n",
    "        matrix = (np.eye(len(ORDER))[Y1[i,:]][:,:,1:] > 0) * 255\n",
    "        # til -> ilt change RGB order for better lines visibility\n",
    "        ax[1].imshow(matrix[:,:,[1,0,2]])\n",
    "        ax[1].axis('off')\n",
    "        ax[2].imshow(Y2[i,:], 'gray')\n",
    "        ax[2].axis('off')  \n",
    "        if i == 0:\n",
    "            ax[0].set_title(f'Input view', fontsize=10)\n",
    "            ax[1].set_title('Segmentation task', fontsize=10)\n",
    "            ax[2].set_title('Value task', fontsize=10)\n",
    "        else:\n",
    "            angle = Y4[i] if Y4[i] < 360 else 'N/A'\n",
    "            ax[0].set_title(f'aligned: {alignment[Y3[i]]}  rotation: {angle}', fontsize=8)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34076a8a",
   "metadata": {},
   "source": [
    "<a name=\"model\"></a>\n",
    "\n",
    "## Model\n",
    "Based on our [comparative experiment](Visual-Backbone-CNN.ipynb) the default visual-backbone `CNN` architecture we chose `64/4/residual`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfa1832",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNELS = 64\n",
    "DEPTH = 4\n",
    "#encoder = CNNEncoder(out_channels=CHANNELS, depth=DEPTH, residual=True).to(DEVICE)\n",
    "#summary(encoder, (1, VIEW_SIZE, VIEW_SIZE))\n",
    "\n",
    "PATCH_SIZE = 4\n",
    "encoder = TransformerEncoder(VIEW_SIZE, PATCH_SIZE, LATENT_DIM, DEPTH)\n",
    "#summary(encoder.to(DEVICE), (1, VIEW_SIZE, VIEW_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0a7cc0",
   "metadata": {},
   "source": [
    "<a name=\"model\"></a>\n",
    "\n",
    "In this experiment we let's the same visual encoder with several task-specific decoders: `segmentation`, `value`, `alignment`, `rotation` and train them all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b7f622",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_head, get_decoder = get_cnn_head, get_cnn_decoder\n",
    "get_head, get_decoder = get_vit_head, get_vit_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fa0ab1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class MultitaskUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    train multiple models using the same visual encoder:\n",
    "    two decoders: segmentation and value\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone: nn.Module, get_head: Callable, get_decoder: Callable, latent_dim: int = 512):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        # teask-specific decoders w/o bridges\n",
    "        self.segmentation = get_decoder(4)\n",
    "        self.value = get_decoder(2)\n",
    "        # teask-specific classifiers\n",
    "        self.alignment = get_head(3)\n",
    "        self.rotation = get_head(361)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e = self.backbone(x)\n",
    "        embedding = e[-1]\n",
    "        segmentation = self.segmentation(e[:])\n",
    "        value = self.value(e[:])\n",
    "        alignment = self.alignment(embedding)\n",
    "        rotation = self.rotation(embedding)\n",
    "        return segmentation, value, alignment, rotation\n",
    "\n",
    "#MultitaskUNet(encoder, get_head, get_decoder).to(DEVICE)(X.to(DEVICE))\n",
    "#summary(MultitaskUNet(encoder, get_head, get_decoder).to(DEVICE), (1, VIEW_SIZE, VIEW_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232e842a",
   "metadata": {},
   "source": [
    "<a name=\"run\"></a>\n",
    "\n",
    "## Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20b8189",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MultitaskPretrainingDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd38be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = np.random.choice(samples, int(len(samples) * 0.95), replace=False)\n",
    "test_samples = list(set(samples).difference(set(train_samples)))\n",
    "len(train_samples), len(test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfe38a3",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "\n",
    "#### 1. Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7bb9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultitaskUNet(encoder, get_head, get_decoder).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6096bec",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "\n",
    "#### 2. Define optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293b19aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEGMENTATION_WEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9674922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DENOISING_WEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37522ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALIGNMET_WEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c1471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = [ DiceLoss(4).to(DEVICE),\n",
    "             DiceLoss(2).to(DEVICE),\n",
    "             nn.CrossEntropyLoss(weight=torch.tensor(ALIGNMET_WEIGHT, dtype=torch.float32)).to(DEVICE),\n",
    "             nn.CrossEntropyLoss().to(DEVICE) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007ff367",
   "metadata": {},
   "source": [
    "We make tasks weights trainable parameters and learn them along the model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d6495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    class HydraLoss(nn.Module):\n",
    "        \"\"\"\n",
    "        Construct combined loss with trainable weights:\n",
    "        https://arxiv.org/abs/1705.07115\n",
    "        \"\"\"\n",
    "        def __init__(self, criteria: list):\n",
    "            super().__init__()\n",
    "            self.criteria = criteria\n",
    "            self.log_vars = nn.Parameter(torch.zeros((len(criteria))))\n",
    "\n",
    "        def forward(self, preds, targets):\n",
    "            losses = []\n",
    "            for i, criterion in enumerate(self.criteria):\n",
    "                loss = criterion(preds[i], targets[i])\n",
    "                losses.append(torch.exp(-self.log_vars[i]) * loss + self.log_vars[i])\n",
    "            return torch.sum(torch.stack(losses))\n",
    "\n",
    "    criterion = HydraLoss(criteria).to(DEVICE)\n",
    "    # optimize both: model and loss parameters\n",
    "    params = [p for p in model.parameters()] + [p for p in criterion.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a020e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6\n",
    "optimizer = AdamW(params, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e376adf4",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "\n",
    "#### 3. Define evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87733a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    'segmentation': {\n",
    "        'f1-score': F1Score(task='multiclass', num_classes=4).to(DEVICE) },\n",
    "    'value': {\n",
    "        'f1-score': F1Score(task='multiclass', num_classes=2).to(DEVICE) },\n",
    "    'alignment': {\n",
    "        'confmat': ConfusionMatrix(task='multiclass', num_classes=3).to(DEVICE),\n",
    "        'f1-score': F1Score(task='multiclass', num_classes=3).to(DEVICE) },\n",
    "    'rotation': {\n",
    "        'confmat': ConfusionMatrix(task='multiclass', num_classes=361).to(DEVICE),\n",
    "        'f1-score': F1Score(task='multiclass', num_classes=361).to(DEVICE) }}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3824d102",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "\n",
    "#### 4. Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3a5a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "num_epochs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfc6eef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(model, dataset, VIEW_SIZE, criterion, optimizer, metrics, multi_y=True)\n",
    "results = trainer.run(train_samples, test_samples, batch_size, num_epochs=num_epochs, validation_steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcde749",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "\n",
    "#### 5. Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af88af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(trainer.loss_history, trainer.metrics_history, multi_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bae78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in results:\n",
    "    for metric in results[task]:\n",
    "        if metric != 'confmat':\n",
    "            print(f'{task:>20} {metric}: {results[task][metric]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89eb4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confmat(np.array(trainer.metrics_history['alignment']['confmat']),\n",
    "             alignment, 'Alignment task confusion-matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880a1421",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confmat(np.array(trainer.metrics_history['rotation']['confmat']),\n",
    "             None, list(range(30, 350, 30)), 'Rotation task confusion-matrix', size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777003f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# let's see some examples with new variation from test-samples\n",
    "loader = DataLoader(MultitaskPretrainingDataset(np.random.choice(samples), VIEW_SIZE, max_samples=8),\n",
    "                    batch_size=8)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X, (Y1, Y2, Y3, Y4) in loader:\n",
    "        P = [np.argmax(p.cpu().numpy(), axis=1) for p in model(X.to(DEVICE))]\n",
    "        for i in range(X.shape[0]):\n",
    "            fig, ax = plt.subplots(1, 4, figsize=(11, 11))\n",
    "            # input view\n",
    "            ax[0].imshow(X[i,:].squeeze().numpy(), 'gray')\n",
    "            ax[0].axis('off')\n",
    "            \n",
    "            # segmentation target color-channels\n",
    "            matrix = (np.eye(len(ORDER))[Y1[i,:]][:,:,1:] > 0) * 255\n",
    "            ax[1].imshow(matrix[:,:,[1,0,2]])\n",
    "            ax[1].axis('off')\n",
    "            \n",
    "            # task output\n",
    "            matrix = (np.eye(len(ORDER))[P[0][i,:]][:,:,1:] > 0) * 255\n",
    "            ax[2].imshow(matrix[:,:,[1,0,2]])\n",
    "            ax[2].axis('off')\n",
    "            \n",
    "            # kinetic awareness task\n",
    "            ax[3].imshow(P[1][i,:], 'gray')\n",
    "            ax[3].axis('off')\n",
    "            \n",
    "            if i == 0:\n",
    "                ax[0].set_title('Input view', fontsize=10)\n",
    "                ax[1].set_title('Segmentation target', fontsize=10)\n",
    "                ax[2].set_title('Segmentation output', fontsize=10)\n",
    "                ax[3].set_title('Value output', fontsize=10)\n",
    "            else:\n",
    "                ax[0].set_title((f'Align: {alignment[Y3[i]]} [true]  {alignment[P[2][i]]} [detected]   '\n",
    "                                 f'Rotation: {Y4[i]} [true]  {P[3][i]} [detected]'),\n",
    "                                fontsize=10, ha='left', x=0)\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0985a602",
   "metadata": {},
   "source": [
    "<a name=\"embeddings\"></a>\n",
    "\n",
    "Now, lets check the latent space produced with this encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1571d388",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['mixed','plain-text','form-table','non-doc']\n",
    "labeled = pd.read_csv('./data/labeled-sample.csv')\n",
    "labeled.groupby('label').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c356252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TopViewDataset(VIEW_SIZE, labeled['source'], labeled['label'], contrast=0.3)\n",
    "embeddings, labels = get_embeddings(dataset, model.backbone, reduce=MeanReduce())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2676ffe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "norm = StandardScaler().fit(embeddings)\n",
    "P = pca.fit_transform(norm.transform(embeddings))\n",
    "# classes means\n",
    "centers = np.array([np.median(P[np.where(np.array(labels) == k)], axis=0) for k in range(len(classes))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdac14c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(9, 4))\n",
    "cmap = colormaps['gist_rainbow']\n",
    "labels = np.array(labels)\n",
    "n = len(classes) - 1\n",
    "for c in range(len(classes) - 1):\n",
    "    ax[0].scatter(P[labels==c,0], P[labels==c,1], s=3, color=cmap(c/n), alpha=0.3)\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "ax[0].set_title('Docs layout types')\n",
    "\n",
    "ax[1].scatter(P[:,0], P[:,1], s=3, c=labels/n, cmap=cmap, alpha=0.3)\n",
    "for c in range(len(classes)):\n",
    "    ax[1].scatter(centers[c,0], centers[c,1], color=cmap(c/n),\n",
    "                    s=75, marker='posv'[c], edgecolor='black', label=classes[c])\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_yticks([])\n",
    "ax[1].set_title('Docs vs. non-docs')\n",
    "ax[1].legend(bbox_to_anchor=(1, 1), frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b54f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), f'./models/visual-multitask-CNN-R-{CHANNELS}-{DEPTH}.pt')\n",
    "torch.save(model.state_dict(), f'./models/visual-multitask-ViT-{PATCH_SIZE}-{DEPTH}.pt')\n",
    "\n",
    "# save encoder only\n",
    "#torch.save(encoder.state_dict(), f'./models/visual-backbone-CNN-R-{CHANNELS}-{DEPTH}.pt')\n",
    "torch.save(encoder.state_dict(), f'./models/visual-backbone-ViT-{PATCH_SIZE}-{DEPTH}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84ad42f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# generate static test-batch for comparison\n",
    "loader = DataLoader(MultitaskPretrainingDataset(np.random.choice(samples), VIEW_SIZE, max_samples=8),\n",
    "                    batch_size=8)\n",
    "batch = []\n",
    "for X, (Y1, Y2, Y3, Y4) in loader:\n",
    "    batch.append((X, (Y1, Y2, Y3, Y4)))\n",
    "\n",
    "for checkpoint in sorted(os.listdir('models/checkpoint')):\n",
    "    if checkpoint.endswith('.pt'):\n",
    "        print('\\n\\n\\n')\n",
    "        model = MultitaskUNet(encoder, get_head, get_decoder).to(DEVICE)\n",
    "        model.load_state_dict(torch.load(f'models/checkpoint/{checkpoint}'))\n",
    "        # let's see some examples with new variation from test-samples\n",
    "        loader = DataLoader(MultitaskPretrainingDataset(np.random.choice(samples), VIEW_SIZE, max_samples=4),\n",
    "                            batch_size=4)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X, (Y1, Y2, Y3, Y4) in batch:\n",
    "                P = [np.argmax(p.cpu().numpy(), axis=1) for p in model(X.to(DEVICE))]\n",
    "                for i in range(X.shape[0]):\n",
    "                    fig, ax = plt.subplots(1, 4, figsize=(11, 11))\n",
    "                    # input view\n",
    "                    ax[0].imshow(X[i,:].squeeze().numpy(), 'gray')\n",
    "                    ax[0].axis('off')\n",
    "\n",
    "                    # segmentation target color-channels\n",
    "                    matrix = (np.eye(len(ORDER))[Y1[i,:]][:,:,1:] > 0) * 255\n",
    "                    ax[1].imshow(matrix[:,:,[1,0,2]])\n",
    "                    ax[1].axis('off')\n",
    "\n",
    "                    # task output\n",
    "                    matrix = (np.eye(len(ORDER))[P[0][i,:]][:,:,1:] > 0) * 255\n",
    "                    ax[2].imshow(matrix[:,:,[1,0,2]])\n",
    "                    ax[2].axis('off')\n",
    "\n",
    "                    # kinetic awareness task\n",
    "                    ax[3].imshow(P[1][i,:], 'gray')\n",
    "                    ax[3].axis('off')\n",
    "\n",
    "                    if i == 0:\n",
    "                        ax[0].set_title('Input view', fontsize=10)\n",
    "                        ax[1].set_title('Segmentation target', fontsize=10)\n",
    "                        ax[2].set_title('Segmentation output', fontsize=10)\n",
    "                        ax[3].set_title('Value output', fontsize=10)\n",
    "                    else:\n",
    "                        ax[0].set_title((f'Align: {alignment[Y3[i]]} [true]  {alignment[P[2][i]]} [detected]   '\n",
    "                                         f'Rotation: {Y4[i]} [true]  {P[3][i]} [detected]'),\n",
    "                                        fontsize=10, ha='left', x=0)\n",
    "                    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f28ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
