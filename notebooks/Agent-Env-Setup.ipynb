{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02b50bb7",
   "metadata": {},
   "source": [
    "# RL environment setup\n",
    "https://stable-baselines3.readthedocs.io/en/v1.0/guide/imitation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dda454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "from gymnasium import Env, ActionWrapper, ObservationWrapper, RewardWrapper, Wrapper\n",
    "from gymnasium.spaces import Box, Dict, Discrete, MultiDiscrete\n",
    "from stable_baselines3 import PPO, A2C\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.policies import obs_as_tensor\n",
    "from torch.nn import AdaptiveAvgPool2d\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd157e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.render import AgentView\n",
    "from scripts.backbone import *\n",
    "from scripts.dataset import Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c836b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIEW_SIZE = 128\n",
    "CHANNELS = 64\n",
    "DEPTH = 4\n",
    "LATENT_DIM = 128\n",
    "\n",
    "visual_backbone = CNNEncoder(out_channels=64, depth=4, residual=True).to(DEVICE)\n",
    "visual_backbone.load_state_dict(torch.load('./models/visual-encoder-CNN-R-64-4-S.pt'))\n",
    "visual_encoder = VisualEncoder(visual_backbone, reduce=AdaptiveAvgPool2d((1, 1)), frozen=True)\n",
    "visual_projection = VisualProjection(visual_encoder, 512, LATENT_DIM).to(DEVICE)\n",
    "visual_projection.load_state_dict(torch.load('./models/visual-projection-CNN.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d8a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images with semantic segmentation masks available\n",
    "images = [str(x).split('/').pop() for x in Path(f'./data/masks').glob('*.png')]\n",
    "\n",
    "docs = {k:[] for k in set(['-'.join(x.split('-')[:-1]) for x in images])}\n",
    "for source in sorted(images):\n",
    "    docs['-'.join(source.split('-')[:-1])].append(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04afd092",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = np.random.choice(images)\n",
    "image = 255 - np.array(ImageOps.grayscale(Image.open(f'./data/images/{source}')))\n",
    "nav = AgentView(image.astype(np.uint8), VIEW_SIZE, bias=0)\n",
    "observation = nav.top()\n",
    "plt.imshow(observation, 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dae33a8",
   "metadata": {},
   "source": [
    "## Base-Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa7061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocNav(Env):    \n",
    "    def __init__(self, pages: list, view_size: int,\n",
    "                       max_episode_steps: int = None, render_mode: str = 'rgb_array'):\n",
    "        super(DocNav, self).__init__()\n",
    "        self.pages = pages\n",
    "        self.dim = view_size\n",
    "        self.max_episode_steps = max_episode_steps or float('inf')\n",
    "        # renderer native spaces\n",
    "        self.action_space = Box(low=-1, high=1, shape=(4,), dtype=np.float32)\n",
    "        self.observation_space = Box(low=0, high=255, shape=(self.dim, self.dim, 1), dtype=np.uint8)\n",
    "        self.nav = None\n",
    "    \n",
    "    def render(self):\n",
    "        # original visual observation\n",
    "        return self.observation.astype(np.uint8)\n",
    "    \n",
    "    def close(self):\n",
    "        self.nav = None\n",
    "        \n",
    "    def info(self):\n",
    "        state = self.nav.state\n",
    "        return {'page':self.index, 'rotation':state[2], 'zoom':state[3], 'center':self.nav.loc}\n",
    "    \n",
    "    def reward(self):\n",
    "        return 0. if self.nav.isin() else -1.\n",
    "\n",
    "    def terminated(self):\n",
    "        return self.done\n",
    "\n",
    "    def truncated(self):\n",
    "        if self.max_episode_steps and self.steps >= self.max_episode_steps:\n",
    "            return True\n",
    "        return False if self.nav.isin() else True\n",
    "       \n",
    "    def reset(self, seed: int = None, options: dict = None) -> np.array:\n",
    "        super().reset(seed=seed, options=options)\n",
    "        self.steps = 0\n",
    "        self.done = False\n",
    "        # load first page\n",
    "        source = self.pages[0]\n",
    "        self.index = 0\n",
    "        # set renderer\n",
    "        image = 255 - np.array(ImageOps.grayscale(Image.open(f'./data/images/{source}')))\n",
    "        self.nav = AgentView(image.astype(np.uint8), self.dim, bias=0)\n",
    "        # set viewport\n",
    "        self.observation = self.nav.top()\n",
    "        return self.observation, self.info()\n",
    "\n",
    "    def step(self, action: np.array) -> tuple:\n",
    "        self.steps += 1\n",
    "        self.last_state = self.nav.state\n",
    "        self.observation = self.nav.transform(action)\n",
    "        self.last_action = action\n",
    "        reward = self.reward()\n",
    "        terminated, truncated = self.terminated(), self.truncated()\n",
    "        return ( self.observation,\n",
    "                 reward,\n",
    "                 terminated or truncated, # for vector-env\n",
    "                 truncated,               # which misses this\n",
    "                 self.info() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a779e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DocNav(images, VIEW_SIZE, max_episode_steps=10)\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbaa56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk(env, scale: float = 1., limit: int = 100):\n",
    "    \"\"\"\n",
    "    take random walk to test the env\n",
    "    \"\"\"\n",
    "    observation, info = env.reset()\n",
    "    for step in range(limit):\n",
    "        action = env.action_space.sample() * scale\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        center, rotation, zoom = info['center'], info['rotation'], info['zoom']\n",
    "        plt.title((f'Action: {np.round(action, 4)}\\nReward: {reward:.4f}\\n'\n",
    "                   f'Steps: {step + 1:<3}  Done: {terminated}   Lost: {truncated}\\n'\n",
    "                   f'Center: {np.round(center, 2)}\\nRotation: {rotation:.2f}\\nZoom: {zoom:.2f}'),\n",
    "                  ha='left', x=0, fontdict={'family':'monospace','size':10})\n",
    "        if terminated:\n",
    "            return\n",
    "        img = plt.imshow(env.render(), cmap='gray')\n",
    "        display(plt.gcf())\n",
    "        clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c75840",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_walk(env, scale=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d823816",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    embedding = visual_projection(Normalize(observation).unsqueeze(1).to(DEVICE)).unsqueeze(1)\n",
    "    embedding = embedding.cpu().numpy()\n",
    "    plt.scatter(range(len(embedding)), embedding, s=10, c=embedding/2.5, cmap='rainbow')\n",
    "    plt.title(f'Embedding value: [{np.min(embedding):.0f},{np.max(embedding):.0f}] shape: {embedding.shape}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098dac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodedVisual(ObservationWrapper):\n",
    "    \"\"\"\n",
    "    use embedding space instead of original visual observation\n",
    "    \"\"\"\n",
    "    def __init__(self, env, dim, encoder, device):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = Box(low=-1, high=1, shape=(dim,), dtype=np.float32)\n",
    "        self.encoder = encoder\n",
    "        self.device = device\n",
    "        \n",
    "    def observation(self, observation) -> np.array:\n",
    "        \"\"\"\n",
    "        transform the native renderer observation to\n",
    "        operational observation: embedding vector in this case\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            embedding = self.encoder(Normalize(observation).unsqueeze(1).to(self.device))\n",
    "            # normalize to fit in [-1, 1]\n",
    "            return embedding.cpu().numpy().squeeze() * 0.25\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcfa1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = EncodedVisual(DocNav(images, VIEW_SIZE, max_episode_steps=10), LATENT_DIM, visual_projection, DEVICE)\n",
    "check_env(env, warn=True)\n",
    "\n",
    "random_walk(env, scale=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096557f9",
   "metadata": {},
   "source": [
    "## Learning environment\n",
    "The renderer native action space maybe too complex for standard RL to be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66160bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteRotate(ActionWrapper):\n",
    "    \"\"\"\n",
    "    use only 3 basic actions: go CCW one degree, hold, go CW one degree\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.action_value = list(np.array([-1., 0., 1.])/180.)\n",
    "        self.action_space = Discrete(3)\n",
    "\n",
    "    def action(self, value):\n",
    "        \"\"\"\n",
    "        translate operational to the native action format\n",
    "        \"\"\"\n",
    "        return np.array([0, 0, self.action_value[int(value)], 0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1f5e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DiscreteRotate(EncodedVisual(DocNav(images, VIEW_SIZE), LATENT_DIM, visual_projection, DEVICE))\n",
    "check_env(env, warn=True)\n",
    "\n",
    "random_walk(env, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e48646",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PageAlign(DocNav):\n",
    "    \"\"\"\n",
    "    agent can only rotate, and should rotate in the nearest of (0, 90, 180, 270) direction\n",
    "    one degree at a time: means total steps should be under 45\n",
    "    \"\"\"\n",
    "    def __init__(self, pages: list, view_size: int):\n",
    "        super().__init__(pages=pages, view_size=view_size, max_episode_steps=200)\n",
    "    \n",
    "    def reward(self):\n",
    "        \"\"\"\n",
    "        This simplified scenario considers alignment only:\n",
    "        success is any of (0, 90, 180, 270) states\n",
    "        \"\"\"\n",
    "        if not self.nav.isin():\n",
    "            return -1000.\n",
    "        # evaluate current state\n",
    "        curr = self.nav.state[2] % 90\n",
    "        curr = min(curr, 90 - curr)\n",
    "        if curr == 0:\n",
    "            self.done = True\n",
    "            return 1000. / self.steps\n",
    "        # compare current and previous states\n",
    "        prev = self.last_state[2] % 90\n",
    "        prev = min(prev, 90 - prev)\n",
    "        if prev > curr: # move in the right direction\n",
    "            return -0.01\n",
    "        return -0.1 * self.steps\n",
    "       \n",
    "    def reset(self, seed: int = None, options: dict = None) -> np.array:\n",
    "        super().reset(seed=seed, options=options)\n",
    "        self.steps = 0\n",
    "        # load random page\n",
    "        self.index = np.random.choice(len(self.pages))\n",
    "        source = self.pages[self.index]\n",
    "        # set rendering\n",
    "        image = 255 - np.array(ImageOps.grayscale(Image.open(f'./data/images/{source}')))\n",
    "        self.nav = AgentView(image.astype(np.uint8), self.dim, bias=0)        \n",
    "        # set random viewport\n",
    "        std = 0 # make sure there's something to see ( some pages are half-empty )\n",
    "        while std < 10.:\n",
    "            center = (np.array(self.nav.space.center) * (0.25 + np.random.rand() * 1.5)).astype(int)\n",
    "            rotation = np.random.choice(360)\n",
    "            zoom = -1 - np.random.rand() * 2.5\n",
    "            observation = self.nav.set_state(center, rotation, zoom)\n",
    "            std = np.std(observation)\n",
    "        self.observation = observation\n",
    "        return self.observation, self.info()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cd3dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_env():\n",
    "    return DiscreteRotate(EncodedVisual(PageAlign(images, VIEW_SIZE), LATENT_DIM, visual_projection, DEVICE))\n",
    "\n",
    "env = random_env()\n",
    "check_env(env, warn=True)\n",
    "\n",
    "random_walk(env, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897313b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = cpu_count()\n",
    "# vector-env\n",
    "vec = make_vec_env(random_env, n_envs=num_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80355f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf runs/rl-align\n",
    "#agent = PPO('MlpPolicy', vec, verbose=1, learning_rate=1e-4, tensorboard_log='runs/rl-align/')\n",
    "#agent.learn(total_timesteps=1e6)\n",
    "#agent.save('./models/align-PPO')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a61908d",
   "metadata": {},
   "source": [
    "    ------------------------------------------\n",
    "    | rollout/                |              |\n",
    "    |    ep_len_mean          | 34.5         |\n",
    "    |    ep_rew_mean          | 29.1         |\n",
    "    | time/                   |              |\n",
    "    |    fps                  | 61           |\n",
    "    |    iterations           | 62           |\n",
    "    |    time_elapsed         | 16582        |\n",
    "    |    total_timesteps      | 1015808      |\n",
    "    | train/                  |              |\n",
    "    |    approx_kl            | 0.0024247682 |\n",
    "    |    clip_fraction        | 0.0209       |\n",
    "    |    clip_range           | 0.2          |\n",
    "    |    entropy_loss         | -0.33        |\n",
    "    |    explained_variance   | 0.419        |\n",
    "    |    learning_rate        | 0.0001       |\n",
    "    |    loss                 | 673          |\n",
    "    |    n_updates            | 610          |\n",
    "    |    policy_gradient_loss | -0.00194     |\n",
    "    |    value_loss           | 1.94e+03     |\n",
    "    ------------------------------------------\n",
    "    \n",
    "    \n",
    "    tensorboard --bind_all --logdir ./runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cb37d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPO.load('./models/align-PPO', env=env, print_system_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915ad1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_proba(agent, embedding):\n",
    "    with torch.no_grad():\n",
    "        x = obs_as_tensor(embedding[np.newaxis, :], agent.policy.device)\n",
    "        # get actions probabilities given observation\n",
    "        return agent.policy.get_distribution(x).distribution.probs.cpu().numpy()\n",
    "\n",
    "\n",
    "def run_episode(env, agent, limit=100):\n",
    "    \"\"\"\n",
    "    let the agent drive\n",
    "    \"\"\"\n",
    "    venv = DummyVecEnv([lambda: env])\n",
    "    venv.render_mode = 'rgb_array'\n",
    "    observation = venv.reset()\n",
    "    for step in range(limit):\n",
    "        dist = get_action_proba(agent, observation)[0]\n",
    "        proba = ', '.join([str(x) for x in np.round(dist, 2)])\n",
    "        action,__ = agent.predict(observation, deterministic=True)\n",
    "        observation, reward, terminated, info = venv.step(action)\n",
    "        center, rotation, zoom = info[0]['center'], info[0]['rotation'], info[0]['zoom']\n",
    "        plt.title((f'Action: {np.round(action[0], 4)}   Proba: [{proba}]\\nReward: {reward[0]:.4f}\\n'\n",
    "                   f'Steps: {step + 1:<3}  Done: {terminated[0]}\\n'\n",
    "                   f'Center: {np.round(center, 2)}\\nRotation: {rotation:.2f}\\nZoom: {zoom:.2f}'),\n",
    "                  ha='left', x=0, fontdict={'family':'monospace','size':10})\n",
    "        if terminated[0]:\n",
    "            return\n",
    "        img = plt.imshow(env.render(), cmap='gray')\n",
    "        display(plt.gcf())\n",
    "        clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fdfa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = random_env()\n",
    "run_episode(env, agent, limit=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce281e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestAlign(PageAlign):\n",
    "    \"\"\"\n",
    "    This reward is the same as PageAlign except it never signals `done` state\n",
    "    \"\"\"\n",
    "    def reward(self):\n",
    "        if not self.nav.isin():\n",
    "            return -1000.\n",
    "        curr = self.nav.state[2] % 90\n",
    "        curr = min(curr, 90 - curr)\n",
    "        if curr == 0:\n",
    "            ### self.done = True ###\n",
    "            return 1000. / self.steps\n",
    "        prev = self.last_state[2] % 90\n",
    "        prev = min(prev, 90 - prev)\n",
    "        if prev > curr:\n",
    "            return -0.01\n",
    "        return -0.1 * self.steps\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849b19cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DiscreteRotate(EncodedVisual(TestAlign(images, VIEW_SIZE), LATENT_DIM, visual_projection, DEVICE))\n",
    "run_episode(env, agent, limit=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb2c88b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818aca0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
