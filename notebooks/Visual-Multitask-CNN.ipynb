{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dcb5e52",
   "metadata": {},
   "source": [
    "# Visual: supervised multi-task training\n",
    "This notebook explores the `CNN`-based `visual-encoder` training for downstream tasks of segmentation and classification. The training runs with the `bridge` (cross-connections) disabled to force most information captured at the embedding level (bottleneck).\n",
    "\n",
    "* [Dataset and Dataloader](#data)\n",
    "* [Backbone model](#model)\n",
    "* [Training and Validation](#run)\n",
    "    * [Define models](#1)\n",
    "    * [Define optimization](#2)\n",
    "    * [Define validation metrics](#3)\n",
    "    * [Run training](#4)\n",
    "    * [Evaluate results](#5)\n",
    "        * [Embeddings](#embeddings)\n",
    "    \n",
    "#### Observations\n",
    "The [embedding space](#embeddings) produced by this training shows better separation of the basic page-types in comparison with [initial experiment](Visual-Backbone-CNN.ipynb#embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cf6f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "from pathlib import Path\n",
    "from einops import rearrange\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torchmetrics import F1Score, JaccardIndex, ConfusionMatrix\n",
    "from torchsummary import summary\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81322936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load local notebook-utils\n",
    "from scripts.backbone import *\n",
    "from scripts.dataset import *\n",
    "from scripts.trainer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b58ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print('GPU' if DEVICE == 'cuda' else 'no GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4a4e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic segmentation masks\n",
    "masks = [str(x).split('/').pop() for x in Path('./data/masks').glob('*.png')\n",
    "         if not str(x).startswith('data/masks/que-')]\n",
    "len(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e624ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIEW_SIZE = 128\n",
    "LATENT_DIM = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66c0128",
   "metadata": {},
   "source": [
    "<a name=\"data\"></a>\n",
    "\n",
    "### Create dataset and dataloader\n",
    "We are going to generate data `online` which will slow down training, but would give us lots of flexibility.\n",
    "The input is a noisy version of the random page view-port (center, rotation, zoom). The targets are:\n",
    "* segmentation: text, input-space, table-outlines\n",
    "* value: info vs. noise\n",
    "* alignment: document rotation related to the view-port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e2191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use images with masks\n",
    "samples = masks #np.random.choice(masks, 640, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6afd0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskPretrainingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    batch of random view-ports from a single page for a set of tasks:\n",
    "    1. semantic segmentation\n",
    "    2. value extraction and denoising\n",
    "    3. alignment\n",
    "    4. rotation\n",
    "    \"\"\"\n",
    "    def __init__(self, source: str, view_size: int, max_samples: int,\n",
    "                       alignment_threshold: float = 0,\n",
    "                       unknown_fraction: float = 0.1, aligned_fraction: float = 0.2):\n",
    "        self.view_size = view_size\n",
    "        self.max_samples = max_samples\n",
    "        self.alignment_threshold = alignment_threshold\n",
    "        self.unknown_fraction = unknown_fraction\n",
    "        self.aligned_fraction = aligned_fraction\n",
    "        # load source image\n",
    "        orig = np.array(ImageOps.grayscale(Image.open(f'{ROOT}/data/images/{source}')))\n",
    "        view = make_noisy_sample(orig)\n",
    "        # load segmentation mask\n",
    "        mask = np.array(Image.open(f'{ROOT}/data/masks/{source}'))\n",
    "        # define renderers for all\n",
    "        self.view = render.AgentView((view).astype(np.uint8), view_size, bias=np.random.randint(100))\n",
    "        self.segmentation = render.AgentView((np.eye(len(ORDER))[mask][:,:,1:] > 0) * 255, view_size)\n",
    "        self.value = render.AgentView(255 - orig, view_size)\n",
    "        # define image preprocesing\n",
    "        self.transform = NormalizeView()\n",
    "    \n",
    "    def random_viewport(self):\n",
    "        \"\"\"\n",
    "        the challenge here -- we need both -- coverage and consistency for a good representation\n",
    "        \"\"\"\n",
    "        center = (np.array(self.view.space.center) * (0.25 + np.random.rand() * 1.5)).astype(int)\n",
    "        if np.random.rand() < self.aligned_fraction:\n",
    "            rotation = np.random.choice([0, 90, 180, 270])\n",
    "        else:\n",
    "            rotation = np.random.randint(0, 360)\n",
    "        zoom = np.random.rand() * 4.0 - 3.5\n",
    "        return center, rotation, zoom\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.max_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if np.random.rand() < self.unknown_fraction: # random non-doc image for out-of-class example\n",
    "            X = self.transform(make_negative_sample(self.view_size))\n",
    "            Y = torch.Tensor(np.zeros((self.view_size, self.view_size))).long()\n",
    "            return X, (Y, Y, 0, 360)\n",
    "        # generate random viewport\n",
    "        std = 0\n",
    "        while std < 10: # make sure there's something to see\n",
    "            center, rotation, zoom = self.random_viewport()\n",
    "            view = self.view.render(center, rotation, zoom)\n",
    "            std = np.std(view)\n",
    "        # render views\n",
    "        X = self.transform(view)\n",
    "        # initialize segmentation masks channels\n",
    "        Y1 = np.zeros((self.view_size, self.view_size, len(ORDER)))\n",
    "        # render masks in the same view-port\n",
    "        view = self.segmentation.render(center, rotation, zoom)\n",
    "        # fix scattered after rotation value back to binary\n",
    "        view = (view/255. > 0.25).astype(int)\n",
    "        # set target as a class-indices matrix\n",
    "        Y1[:,:,1:] = view\n",
    "        # segmentation task target\n",
    "        Y1 = torch.Tensor(np.argmax(Y1, axis=(2))).long()\n",
    "        # value task target\n",
    "        view = self.value.render(center, rotation, zoom)\n",
    "        Y2 = torch.Tensor(view/255. >= 0.25).squeeze().long()\n",
    "        # alignment task target\n",
    "        d = rotation % 90\n",
    "        Y3 = int(min(d, 90 - d) <= self.alignment_threshold) + 1\n",
    "        Y4 = int(rotation)\n",
    "        return X, (Y1, Y2, Y3, Y4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581cfc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alignment_weight(alignment_threshold=0, unknown_fraction=0.1, aligned_fraction=0.2):\n",
    "    \"\"\"\n",
    "    estimate alignment task class-weight given chosen configuration\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    for _ in range(10000): # run 10000 tries and get stats\n",
    "        if np.random.rand() < unknown_fraction:\n",
    "            x.append(0)\n",
    "        else:\n",
    "            r = np.random.choice([0, 90, 180, 270]) if np.random.rand() < aligned_fraction else \\\n",
    "                np.random.randint(0, 360)\n",
    "            d = r % 90\n",
    "            x.append(int(min(d, 90 - d) <= alignment_threshold) + 1)\n",
    "\n",
    "    w = pd.Series(x)\n",
    "    w = w.sum() - w.groupby(w).size()\n",
    "    return list(np.round(list(w/w.sum()), 2))\n",
    "\n",
    "\n",
    "ALIGNMET_WEIGHT = get_alignment_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9bece7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample = np.random.choice(samples)\n",
    "# test loader\n",
    "n = 8\n",
    "loader = DataLoader(MultitaskPretrainingDataset(sample, VIEW_SIZE, max_samples=n), batch_size=n)\n",
    "alignment = ['N/A','No','Yes']\n",
    "# show first batch\n",
    "for X, (Y1, Y2, Y3, Y4) in loader:\n",
    "    print(f'source: {sample}\\nX: {X.shape}  Y1:{Y1.shape}  Y2:{Y2.shape}  Y3:{Y3.shape}  Y4:{Y4.shape}')\n",
    "    for i in range(n):\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(8, 8))\n",
    "        ax[0].imshow(X[i,:].squeeze(), 'gray')\n",
    "        ax[0].axis('off')\n",
    "        # restore channels to avoid visual confusion\n",
    "        matrix = (np.eye(len(ORDER))[Y1[i,:]][:,:,1:] > 0) * 255\n",
    "        # til -> ilt change RGB order for better lines visibility\n",
    "        ax[1].imshow(matrix[:,:,[1,0,2]])\n",
    "        ax[1].axis('off')\n",
    "        ax[2].imshow(Y2[i,:], 'gray')\n",
    "        ax[2].axis('off')  \n",
    "        if i == 0:\n",
    "            ax[0].set_title(f'Input view', fontsize=10)\n",
    "            ax[1].set_title('Segmentation task', fontsize=10)\n",
    "            ax[2].set_title('Value task', fontsize=10)\n",
    "        else:\n",
    "            angle = Y4[i] if Y4[i] < 360 else 'N/A'\n",
    "            ax[0].set_title(f'aligned: {alignment[Y3[i]]}  rotation: {angle}', fontsize=8)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34076a8a",
   "metadata": {},
   "source": [
    "<a name=\"model\"></a>\n",
    "\n",
    "## Model\n",
    "Based on our [comparative experiment](Visual-Backbone-CNN.ipynb) the default visual-backbone `CNN` architecture we chose `64/4/residual`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5324a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_head(output_dim: int):\n",
    "    return nn.Sequential(\n",
    "        nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        nn.Flatten(start_dim=1),\n",
    "        Head(LATENT_DIM, output_dim))\n",
    "\n",
    "def get_vit_head(output_dim: int):\n",
    "    return nn.Sequential(\n",
    "        MeanReduce(),\n",
    "        Head(LATENT_DIM, output_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8d2e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_decoder(num_classes: int, bridge: bool = False):\n",
    "    return nn.Sequential(\n",
    "        CNNDecoder(LATENT_DIM, DEPTH - 1, True, bridge, True),\n",
    "        nn.Conv2d(CHANNELS, num_classes, 1, 1),\n",
    "        nn.Softmax(dim=1))\n",
    "\n",
    "def get_vit_decoder(num_classes: int, bridge: bool = False):\n",
    "    return nn.Sequential(\n",
    "        TransformerDecoder(VIEW_SIZE, PATCH_SIZE, LATENT_DIM, DEPTH - 1, channels=num_classes, bridge=bridge),\n",
    "        nn.Softmax(dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfa1832",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = get_cnn_backbone(pretrained=True, frozen=False)\n",
    "#encoder = get_vit_backbone(pretrained=True, frozen=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0a7cc0",
   "metadata": {},
   "source": [
    "<a name=\"model\"></a>\n",
    "\n",
    "In this experiment we let's the same visual encoder with several task-specific decoders: `segmentation`, `value`, `alignment`, `rotation` and train them all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b7f622",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_head, get_decoder = get_cnn_head, get_cnn_decoder\n",
    "#get_head, get_decoder = get_vit_head, get_vit_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fa0ab1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class MultitaskUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    train multiple models using the same visual encoder:\n",
    "    two decoders: segmentation and value\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone: nn.Module, get_head: Callable, get_decoder: Callable, latent_dim: int = 512):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        # teask-specific decoders w/o bridges\n",
    "        self.segmentation = get_decoder(4)\n",
    "        self.value = get_decoder(2)\n",
    "        # teask-specific classifiers\n",
    "        self.alignment = get_head(3)\n",
    "        self.rotation = get_head(361)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e = self.backbone(x)\n",
    "        embedding = e[-1]\n",
    "        segmentation = self.segmentation(e[:])\n",
    "        value = self.value(e[:])\n",
    "        alignment = self.alignment(embedding)\n",
    "        rotation = self.rotation(embedding)\n",
    "        return segmentation, value, alignment, rotation\n",
    "\n",
    "#MultitaskUNet(encoder, get_head, get_decoder).to(DEVICE)(X.to(DEVICE))\n",
    "#summary(MultitaskUNet(encoder, get_head, get_decoder).to(DEVICE), (1, VIEW_SIZE, VIEW_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232e842a",
   "metadata": {},
   "source": [
    "<a name=\"run\"></a>\n",
    "\n",
    "## Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20b8189",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MultitaskPretrainingDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd38be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = np.random.choice(samples, int(len(samples) * 0.95), replace=False)\n",
    "test_samples = list(set(samples).difference(set(train_samples)))\n",
    "len(train_samples), len(test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfe38a3",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "\n",
    "#### 1. Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7bb9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultitaskUNet(encoder, get_head, get_decoder).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6096bec",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "\n",
    "#### 2. Define optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293b19aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEGMENTATION_WEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9674922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DENOISING_WEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37522ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALIGNMET_WEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c1471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = [ DiceLoss(4).to(DEVICE),\n",
    "             DiceLoss(2).to(DEVICE),\n",
    "             nn.CrossEntropyLoss(weight=torch.tensor(ALIGNMET_WEIGHT, dtype=torch.float32)).to(DEVICE),\n",
    "             nn.CrossEntropyLoss().to(DEVICE) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12292604",
   "metadata": {},
   "source": [
    "We can define our combined loss criterion as a weighted sum of tasks losses. However, tasks losses dynamic most probably will not be well aligned along the training making static tasks weights a suboptimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a99da5",
   "metadata": {},
   "source": [
    "    class CombinedLoss(nn.Module):\n",
    "        def __init__(self, criteria: list, weights: list):\n",
    "            assert len(criteria) == len(weights)\n",
    "            super(CombinedLoss, self).__init__()\n",
    "            self.criteria = criteria\n",
    "            self.weights = weights        \n",
    "\n",
    "        def forward(self, preds, targets):\n",
    "            losses = []\n",
    "            for i, criterion in enumerate(self.criteria):\n",
    "                losses.append(criterion(preds[i], targets[i]) * self.weights[i])\n",
    "            return torch.sum(torch.stack(losses))\n",
    "\n",
    "    criterion = CombinedLoss(criteria, [1., 1., 10., 10.]).to(device)\n",
    "    # optimized model parameters only\n",
    "    params = model.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007ff367",
   "metadata": {},
   "source": [
    "Instead we can make tasks weights trainable parameters and learn them along the model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d6495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    class HydraLoss(nn.Module):\n",
    "        \"\"\"\n",
    "        Construct combined loss with trainable weights:\n",
    "        https://arxiv.org/abs/1705.07115\n",
    "        \"\"\"\n",
    "        def __init__(self, criteria: list):\n",
    "            super().__init__()\n",
    "            self.criteria = criteria\n",
    "            self.log_vars = nn.Parameter(torch.zeros((len(criteria))))\n",
    "\n",
    "        def forward(self, preds, targets):\n",
    "            losses = []\n",
    "            for i, criterion in enumerate(self.criteria):\n",
    "                loss = criterion(preds[i], targets[i])\n",
    "                losses.append(torch.exp(-self.log_vars[i]) * loss + self.log_vars[i])\n",
    "            return torch.sum(torch.stack(losses))\n",
    "\n",
    "    criterion = HydraLoss(criteria).to(DEVICE)\n",
    "    # optimize both: model and loss parameters\n",
    "    params = [p for p in model.parameters()] + [p for p in criterion.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a020e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6\n",
    "optimizer = AdamW(params, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e376adf4",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "\n",
    "#### 3. Define evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87733a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    'segmentation': {\n",
    "        'f1-score': F1Score(task='multiclass', num_classes=4).to(DEVICE) },\n",
    "    'value': {\n",
    "        'f1-score': F1Score(task='multiclass', num_classes=2).to(DEVICE) },\n",
    "    'alignment': {\n",
    "        'confmat': ConfusionMatrix(task='multiclass', num_classes=3).to(DEVICE),\n",
    "        'f1-score': F1Score(task='multiclass', num_classes=3).to(DEVICE) },\n",
    "    'rotation': {\n",
    "        'confmat': ConfusionMatrix(task='multiclass', num_classes=361).to(DEVICE),\n",
    "        'f1-score': F1Score(task='multiclass', num_classes=361).to(DEVICE) }}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3824d102",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "\n",
    "#### 4. Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3a5a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_epochs = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfc6eef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!rm -rf ./runs/visual-pretraining-cnn\n",
    "trainer = Trainer(model, dataset, VIEW_SIZE, criterion, optimizer, metrics, multi_y=True)\n",
    "#                 tensorboard_dir='runs/visual-pretraining-cnn') # log progress to tensorboard\n",
    "results = trainer.run(train_samples, test_samples, batch_size, num_epochs=num_epochs, validation_steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcde749",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "\n",
    "#### 5. Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af88af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(trainer.loss_history, trainer.metrics_history, multi_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b757b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in results:\n",
    "    for metric in results[task]:\n",
    "        if metric != 'confmat':\n",
    "            print(f'{task:>20} {metric}: {results[task][metric]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89eb4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confmat(np.array(trainer.metrics_history['alignment']['confmat']),\n",
    "             alignment, 'Alignment task confusion-matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880a1421",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confmat(np.array(trainer.metrics_history['rotation']['confmat']),\n",
    "             None, list(range(30, 350, 30)), 'Rotation task confusion-matrix', size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777003f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# let's see some examples\n",
    "loader = DataLoader(MultitaskPretrainingDataset(np.random.choice(samples), VIEW_SIZE, max_samples=8),\n",
    "                    batch_size=8)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X, (Y1, Y2, Y3, Y4) in loader:\n",
    "        P = [np.argmax(p.cpu().numpy(), axis=1) for p in model(X.to(DEVICE))]\n",
    "        for i in range(X.shape[0]):\n",
    "            fig, ax = plt.subplots(1, 4, figsize=(11, 11))\n",
    "            # input view\n",
    "            ax[0].imshow(X[i,:].squeeze().numpy(), 'gray')\n",
    "            ax[0].axis('off')\n",
    "            \n",
    "            # segmentation target color-channels\n",
    "            matrix = (np.eye(len(ORDER))[Y1[i,:]][:,:,1:] > 0) * 255\n",
    "            ax[1].imshow(matrix[:,:,[1,0,2]])\n",
    "            ax[1].axis('off')\n",
    "            \n",
    "            # task output\n",
    "            matrix = (np.eye(len(ORDER))[P[0][i,:]][:,:,1:] > 0) * 255\n",
    "            ax[2].imshow(matrix[:,:,[1,0,2]])\n",
    "            ax[2].axis('off')\n",
    "            \n",
    "            # kinetic awareness task\n",
    "            ax[3].imshow(P[1][i,:], 'gray')\n",
    "            ax[3].axis('off')\n",
    "            \n",
    "            if i == 0:\n",
    "                ax[0].set_title('Input view', fontsize=10)\n",
    "                ax[1].set_title('Segmentation target', fontsize=10)\n",
    "                ax[2].set_title('Segmentation output', fontsize=10)\n",
    "                ax[3].set_title('Value output', fontsize=10)\n",
    "            else:\n",
    "                ax[0].set_title((f'Align: {alignment[Y3[i]]} [true]  {alignment[P[2][i]]} [detected]   '\n",
    "                                 f'Rotation: {Y4[i]} [true]  {P[3][i]} [detected]'),\n",
    "                                fontsize=10, ha='left', x=0)\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0985a602",
   "metadata": {},
   "source": [
    "<a name=\"embeddings\"></a>\n",
    "\n",
    "Let's check the latent space produced with trained encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1571d388",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['mixed','plain-text','form-table','non-doc']\n",
    "labeled = pd.read_csv('./data/labeled-sample.csv')\n",
    "labeled.groupby('label').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c356252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TopViewDataset(VIEW_SIZE, labeled['source'], labeled['label'], contrast=0.3)\n",
    "embeddings, labels = get_embeddings(dataset, model.backbone, reduce=nn.AdaptiveAvgPool2d((1, 1)))\n",
    "# add to tensorboar-projector\n",
    "#trainer.writer.add_embedding(embeddings, metadata=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2676ffe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "norm = StandardScaler().fit(embeddings)\n",
    "P = pca.fit_transform(norm.transform(embeddings))\n",
    "# classes means\n",
    "centers = np.array([np.median(P[np.where(np.array(labels) == k)], axis=0) for k in range(len(classes))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9775f21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "cmap = colormaps['gist_rainbow']\n",
    "labels = np.array(labels)\n",
    "n = len(classes) - 1\n",
    "ax.scatter(P[:,0], P[:,1], s=3, c=labels/n, cmap=cmap, alpha=0.3)\n",
    "for c in range(len(classes)):\n",
    "    ax.scatter(centers[c,0], centers[c,1], color=cmap(c/n),\n",
    "                   s=75, marker='posv'[c], edgecolor='black', label=classes[c])\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_title('Docs vs. non-docs')\n",
    "ax.legend(bbox_to_anchor=(1, 1), frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b54f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'./models/visual-multitask-CNN-R-{CHANNELS}-{DEPTH}.pt')\n",
    "#torch.save(model.state_dict(), f'./models/visual-multitask-ViT-{PATCH_SIZE}-{DEPTH}.pt')\n",
    "#trainer.writer.close()\n",
    "\n",
    "# save encoder only\n",
    "torch.save(encoder.state_dict(), f'./models/visual-backbone-CNN-R-{CHANNELS}-{DEPTH}.pt')\n",
    "#torch.save(encoder.state_dict(), f'./models/visual-backbone-ViT-{PATCH_SIZE}-{DEPTH}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5219666",
   "metadata": {},
   "source": [
    "Let's check changes between the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc3a370",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# generate static test-batch for comparison\n",
    "loader = DataLoader(MultitaskPretrainingDataset(np.random.choice(samples), VIEW_SIZE, max_samples=8),\n",
    "                    batch_size=8)\n",
    "batch = []\n",
    "for X, (Y1, Y2, Y3, Y4) in loader:\n",
    "    batch.append((X, (Y1, Y2, Y3, Y4)))\n",
    "\n",
    "for checkpoint in sorted(os.listdir('models/checkpoint')):\n",
    "    if checkpoint.endswith('.pt'):\n",
    "        print('\\n\\n\\n')\n",
    "        model = MultitaskUNet(encoder, get_head, get_decoder).to(DEVICE)\n",
    "        model.load_state_dict(torch.load(f'models/checkpoint/{checkpoint}'))\n",
    "        # let's see some examples with new variation from test-samples\n",
    "        loader = DataLoader(MultitaskPretrainingDataset(np.random.choice(samples), VIEW_SIZE, max_samples=4),\n",
    "                            batch_size=4)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X, (Y1, Y2, Y3, Y4) in batch:\n",
    "                P = [np.argmax(p.cpu().numpy(), axis=1) for p in model(X.to(DEVICE))]\n",
    "                for i in range(X.shape[0]):\n",
    "                    fig, ax = plt.subplots(1, 4, figsize=(11, 11))\n",
    "                    # input view\n",
    "                    ax[0].imshow(X[i,:].squeeze().numpy(), 'gray')\n",
    "                    ax[0].axis('off')\n",
    "\n",
    "                    # segmentation target\n",
    "                    matrix = (np.eye(len(ORDER))[Y1[i,:]][:,:,1:] > 0) * 255\n",
    "                    ax[1].imshow(matrix[:,:,[1,0,2]])\n",
    "                    ax[1].axis('off')\n",
    "\n",
    "                    # segmentation output\n",
    "                    matrix = (np.eye(len(ORDER))[P[0][i,:]][:,:,1:] > 0) * 255\n",
    "                    ax[2].imshow(matrix[:,:,[1,0,2]])\n",
    "                    ax[2].axis('off')\n",
    "\n",
    "                    # value output\n",
    "                    ax[3].imshow(P[1][i,:], 'gray')\n",
    "                    ax[3].axis('off')\n",
    "\n",
    "                    if i == 0:\n",
    "                        ax[0].set_title('Input view', fontsize=10)\n",
    "                        ax[1].set_title('Segmentation target', fontsize=10)\n",
    "                        ax[2].set_title('Segmentation output', fontsize=10)\n",
    "                        ax[3].set_title('Value output', fontsize=10)\n",
    "                    else:\n",
    "                        ax[0].set_title((f'Align: {alignment[Y3[i]]} [true]  {alignment[P[2][i]]} [detected]   '\n",
    "                                         f'Rotation: {Y4[i]} [true]  {P[3][i]} [detected]'),\n",
    "                                        fontsize=10, ha='left', x=0)\n",
    "                    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cd63aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
