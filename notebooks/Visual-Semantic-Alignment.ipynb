{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8fd6c70",
   "metadata": {},
   "source": [
    "# Semantic alignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4c2166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from time import time\n",
    "from PIL import Image, ImageOps\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "from pathlib import Path\n",
    "from einops import rearrange\n",
    "from typing import Callable\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torchmetrics import F1Score, ConfusionMatrix\n",
    "from torchsummary import summary\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0b7b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, Filter, FieldCondition, MatchValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c687f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load local notebook-utils\n",
    "from scripts.backbone import *\n",
    "from scripts.dataset import *\n",
    "from scripts.trainer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34facee",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print('GPU' if DEVICE == 'cuda' else 'no GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370285a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic segmentation masks\n",
    "samples = [str(x).split('/').pop() for x in Path('./data/masks').glob('*.png')\n",
    "           if not str(x).startswith('data/masks/que-')]\n",
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ee425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIEW_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f15ed06",
   "metadata": {},
   "source": [
    "## Define semantic space\n",
    "Text presence in the view is the main indicator, secondary is presence of straight lines, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf11fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NONDOC = [\n",
    "    'not a document',\n",
    "    'nothing like document',\n",
    "    'does not look like a document',\n",
    "    'no document in the view',\n",
    "]\n",
    "\n",
    "SCOPE = { # page corners in the view\n",
    "    (1, 1, 1, 1):'full-page view',\n",
    "    (1, 1, 1, 0):'top-right partial view of a page',\n",
    "    (1, 1, 0, 1):'top-left partial view of a page',\n",
    "    (0, 1, 1, 1):'bootom-righ partial view of a page',\n",
    "    (1, 0, 1, 1):'bottom-left partial view of a page',\n",
    "    (1, 1, 0, 0):'top part of a page',\n",
    "    (0, 0, 1, 1):'bottom part of a page',\n",
    "    (0, 1, 1, 0):'right side of a page',\n",
    "    (1, 0, 0, 1):'left side of a page',\n",
    "    (1, 0, 0, 0):'top-left corner of a page',\n",
    "    (0, 1, 0, 0):'top-right corner of a page',\n",
    "    (0, 0, 1, 0):'bottom-right corner of a page',\n",
    "    (0, 0, 0, 1):'bottom-left corner of a page',\n",
    "    (0, 0, 0, 0):'page fragment',\n",
    "}\n",
    "\n",
    "ORIENTATION = {\n",
    "    0:'straight',\n",
    "    90:'turned on the left side',\n",
    "    180:'turned upside-down',\n",
    "    270:'turned on the right side',\n",
    "}\n",
    "\n",
    "KEYS = list(SCOPE.keys()) + list(ORIENTATION.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e44d21",
   "metadata": {},
   "source": [
    "## Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645bacfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_encoder = get_cnn_encoder(pretrained=True, frozen=True)\n",
    "#visual_encoder = get_vit_encoder(pretrained=True, frozen=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3a5dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticEncoder(nn.Module):\n",
    "    def __init__(self, model_id: str, max_seq_length: int = 128):\n",
    "        super(SemanticEncoder, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id, max_seq_length=max_seq_length)\n",
    "        self.model = AutoModel.from_pretrained(model_id)\n",
    "        # freeze params\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        # use cls token hidden representation as the sentence's embedding\n",
    "        self.target_idx = 0\n",
    "        \n",
    "    def mean_pool(self, text):\n",
    "        encoded_input = self.tokenizer(text, padding=True, truncation=True, return_tensors='pt')        \n",
    "        output = self.model(**encoded_input)\n",
    "        token_embeddings = output[0] # all token embeddings\n",
    "        attention_mask = encoded_input['attention_mask']\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        norm = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return nn.functional.normalize(torch.sum(token_embeddings * input_mask_expanded, 1) / norm)\n",
    "    \n",
    "    def encode(self, text):\n",
    "        return self.mean_pool(text)\n",
    "        # or use cls-token\n",
    "        #encoded_input = self.tokenizer(text, padding=True, truncation=True, return_tensors='pt')        \n",
    "        #output = self.model(**encoded_input)\n",
    "        #last_hidden_state = output.last_hidden_state\n",
    "        #return last_hidden_state[:,self.target_idx,:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490cd595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLMID = 'sentence-transformers/distiluse-base-multilingual-cased-v1'\n",
    "LLMID = 'sentence-transformers/paraphrase-distilroberta-base-v1'\n",
    "semantic_encoder = SemanticEncoder(LLMID, max_seq_length=64) #.to(DEVICE)\n",
    "\n",
    "captions = ['view description', 'another view description']\n",
    "embeddings = semantic_encoder.encode(captions)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cc745e",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff40274",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    'doc':['non-doc','doc','?'],\n",
    "    'text':['no text','text','?'],\n",
    "    'rotation':['straight','90','180','270','rotated','n/a'],\n",
    "    'zoom':['unreadable','readable','?']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b7df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticAlignDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    use a single document for a batch of random view-ports\n",
    "    X: view-image and view-description text embedding\n",
    "    Y: semantic and visual projections covariance matrix\n",
    "       and tasks labels for both visual and semantic classifiers\n",
    "    \"\"\"\n",
    "    def __init__(self, source: str, view_size: int, max_samples: int,\n",
    "                       encode: Callable = semantic_encoder.encode, \n",
    "                       debug: bool = False):\n",
    "        self.encode = encode\n",
    "        self.view_size = view_size\n",
    "        self.max_samples = max_samples\n",
    "        self.debug = debug\n",
    "        # ensure less collisions and better contrast\n",
    "        #keys = KEYS * int(np.ceil((max_samples - 1)/len(KEYS)))\n",
    "        # better representation of target-resolution and rotation\n",
    "        keys = KEYS + [(0, 0, 0, 0)] * (max_samples - len(KEYS) - 1)\n",
    "        self.keys = keys[:max_samples - 1] + ['']\n",
    "        self.order = np.random.choice(range(max_samples), max_samples, replace=False)\n",
    "        self.angle = list(np.random.choice([a for a in range(1, 360) if a not in [90, 180, 270]],\n",
    "                                               max_samples, replace=False))\n",
    "        # load source image\n",
    "        orig = np.array(ImageOps.grayscale(Image.open(f'{ROOT}/data/images/{source}')))\n",
    "        view = make_noisy_sample(orig) if np.random.rand() > 0.5 else 255 - orig\n",
    "        # load segmentation mask\n",
    "        mask = np.array(Image.open(f'{ROOT}/data/masks/{source}'))\n",
    "        # define renderers for all\n",
    "        self.view = render.AgentView((view).astype(np.uint8), view_size, bias=np.random.randint(100))\n",
    "        self.segmentation = render.AgentView((np.eye(len(ORDER))[mask][:,:,1:] > 0) * 255, view_size)\n",
    "        # define image preprocesing\n",
    "        self.transform = Normalize\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.max_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        key = self.keys[self.order[idx]]\n",
    "        if key == '':\n",
    "            # random non-doc image for out-of-class example\n",
    "            X1  = self.transform(make_negative_sample(self.view_size).astype(np.float32)/255.)\n",
    "            caption = np.random.choice(NONDOC)\n",
    "            X2 = self.encode(caption).squeeze()\n",
    "            Y1 = caption if self.debug else idx\n",
    "            return (X1, X2), tuple([Y1] + [0, 0, 5, 2] * 2)\n",
    "        # generate random viewport\n",
    "        center, rotation, zoom = self.random_viewport(key)\n",
    "        std = 0\n",
    "        while std < 10: # make sure there's something to see\n",
    "            rotation = np.random.randint(0, 360)\n",
    "            center = (np.array(self.view.space.center) * (0.25 + np.random.rand(2) * 0.5)).astype(int)\n",
    "            zoom = -1. - np.random.rand() * 2\n",
    "            observation = self.view.render(center, rotation, zoom)\n",
    "            std = np.std(observation)\n",
    "        # render views\n",
    "        X1 = self.transform(observation.astype(np.float32)/255.)\n",
    "        # render masks in the same view-port\n",
    "        view = self.segmentation.render(center, rotation, zoom)\n",
    "        # fix scattered after rotation value back to binary\n",
    "        view = (view/255. > 0.25).astype(int)\n",
    "        scores = np.sum(view, axis=(0, 1))/164. # percent by channel\n",
    "        lines, inputs, text = scores\n",
    "        # alignment task target\n",
    "        caption, info = self.generate_description(key, center, rotation, zoom, scores)\n",
    "        X2 = self.encode(caption).squeeze()\n",
    "        Y1 = caption if self.debug else idx # f'{caption}\\n{info}'\n",
    "        Y2 = 2 if (text < 0.25 and lines < 0.25) or caption == 'hard to identify' else 1\n",
    "        Y3 = 0 if text == 0 else 1 if text > 5 else 2\n",
    "        Y4 = 5 if text < 1 else {0:0, 90:1, 180:2, 270:3}.get(rotation, 4)\n",
    "        Y5 = self.parse_zoom(text, zoom)\n",
    "        return (X1, X2), tuple([Y1] + [Y2, Y3, Y4, Y5] * 2)\n",
    "    \n",
    "    def random_viewport(self, key):\n",
    "        \"\"\"\n",
    "        generate a random viewport which fits description `key`\n",
    "        \"\"\"\n",
    "        center = np.array(self.view.space.center).astype(float)\n",
    "        if key in SCOPE.keys():\n",
    "            n = sum(key) # num corners in the view\n",
    "            zoom = [0., -2.75, -3., -3.5, -4.][n] + np.random.rand() * 0.25\n",
    "            if n == 0: # small fragment\n",
    "                shift = np.random.rand(2) - 0.5\n",
    "                rotation = self.angle.pop()\n",
    "            elif n == 4: # full-page\n",
    "                rotation = np.random.choice([0, 90, 180, 270]) + np.random.choice([-2, -1, 0, 1, 2])\n",
    "                shift = (np.random.rand(2) - 0.5) * 0.1\n",
    "            else: # corners in the view\n",
    "                f = 0.75 / n\n",
    "                d = np.array([[-1., -1.],[-1., 1.],[1., 1.],[1., -1.]])[np.array(key) > 0] * f\n",
    "                d += (np.random.rand(*d.shape) * 0.05 - 0.05)\n",
    "                shift = np.sum(d, axis=0)\n",
    "                rotation = self.angle.pop()\n",
    "            center *= (1. + shift)\n",
    "        else:\n",
    "            zoom = 0.5 - np.random.rand() * 2.5\n",
    "            center *= (0.4 + np.random.rand() * 1.2)\n",
    "            rotation = key if key in [0, 90, 180, 270] else self.angle.pop()\n",
    "        return center.astype(int), rotation, zoom\n",
    "    \n",
    "    def generate_description(self, key, center, rotation, zoom, scores):\n",
    "        \"\"\"\n",
    "        text caption for the view using heuristics based on the dataset stats\n",
    "        \"\"\"\n",
    "        lines, inputs, text = scores\n",
    "        info = f'lines: {lines:.2f}   inputs: {inputs:.2f}   text: {text:.2f}   zoom: {zoom:.2f}'\n",
    "        if lines == 0 and text == 0:\n",
    "            return 'no text and no lines in the view', info\n",
    "        orientation = self.parse_align(text, rotation, zoom)\n",
    "        scope = self.parse_scope(key, zoom)\n",
    "        content = self.parce_content(text, lines, inputs, zoom)\n",
    "        if content == '':\n",
    "            return 'hard to identify', info\n",
    "        return f'{scope} with {content} {orientation}', info\n",
    "    \n",
    "    def parse_align(self, text, rotation, zoom):\n",
    "        \"\"\"\n",
    "        rotation assessement based on text; if no text\n",
    "        \"\"\"\n",
    "        if text == 0 or (text < 1 and zoom > -1):\n",
    "            return 'straight' if rotation in [0, 90, 180, 270] else 'rotated'\n",
    "        if rotation in [0, 90, 180, 270]:\n",
    "            return ORIENTATION[rotation]\n",
    "        return f'rotated {rotation:.0f} degrees counterclockwise' if rotation < 180 else \\\n",
    "               f'rotated {360 - rotation:.0f} degrees clockwise'\n",
    "    \n",
    "    def parse_scope(self, key, zoom):\n",
    "        if key in SCOPE:\n",
    "            return SCOPE[key]\n",
    "        if zoom > 0:\n",
    "            return 'small fragment of a page'\n",
    "        return 'page fragment'\n",
    "    \n",
    "    def parce_content(self, text, lines, inputs, zoom):\n",
    "        content = []\n",
    "        if text > 0.1:\n",
    "            content.append('text')\n",
    "        if lines > 0.1:\n",
    "            content.append('lines')\n",
    "        return ' and '.join(content)\n",
    "    \n",
    "    def parse_zoom(self, text, zoom):\n",
    "        if text < 1: return 2     # unknown\n",
    "        if zoom > 0.5: return 1   # word\n",
    "        if zoom > -0.5: return 1  # text\n",
    "        if zoom > -1.5: return 1  # block (readable)\n",
    "        return 0                  # page (not-readable)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9638d7d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample = np.random.choice(samples)\n",
    "# test loader\n",
    "batch_size = 8\n",
    "loader = DataLoader(SemanticAlignDataset(sample, VIEW_SIZE, batch_size, debug=True), batch_size=batch_size)\n",
    "for X, Y in loader:\n",
    "    for i in range(batch_size):\n",
    "        fig, ax = plt.subplots(figsize=(4, 4))\n",
    "        ax.imshow(X[0][i,:].squeeze(), 'gray')\n",
    "        ax.axis('off')\n",
    "        text = '  '.join([labels[task][Y[k + 1][i]] for k, task in enumerate(labels)])\n",
    "        ax.set_title(f'{Y[0][i]}\\nlabels:  {text}', ha='left', x=0, fontsize=10)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0980c2e",
   "metadata": {},
   "source": [
    "#### Dataset class-weights estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a832421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 # intended batch-size\n",
    "\n",
    "stats = {x:[] for x in labels}\n",
    "timer = 0 # check how much time data-synth takes\n",
    "for source in np.random.choice(samples, 100, replace=False):\n",
    "    start = time()\n",
    "    loader = DataLoader(SemanticAlignDataset(source, VIEW_SIZE, batch_size, debug=False), batch_size=batch_size)\n",
    "    for X, Y in loader:\n",
    "        for i, label in enumerate(labels.keys(), 1):\n",
    "            stats[label] += Y[i].tolist()\n",
    "    timer += (time() - start)/100\n",
    "    \n",
    "print(f'Average time to make a batch: {timer:.0f} sec')\n",
    "stats = pd.DataFrame.from_dict(stats, orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f721d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check batch structure\n",
    "loader = DataLoader(SemanticAlignDataset(sample, VIEW_SIZE, batch_size), batch_size=batch_size)\n",
    "for X, Y in loader:\n",
    "    print(X[0].shape, X[1].shape)\n",
    "    print(Y[0].shape, Y[1].shape, Y[2].shape, Y[3].shape, Y[4].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b03be3",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74891c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf25c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projection(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, latent_dim: int):\n",
    "        super(Projection, self).__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, latent_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.GELU(),\n",
    "            nn.Linear(latent_dim, latent_dim))\n",
    "        self.norm = nn.LayerNorm(latent_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.projection(x)\n",
    "        x = self.mlp(h)\n",
    "        x = x + h\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0549c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_projection = Projection(768, LATENT_DIM).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e88136",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualProjection(nn.Sequential):\n",
    "    def __init__(self, encoder: nn.Module, embedding_dim: int, latent_dim: int, dropout: float = 0.):\n",
    "        super(VisualProjection, self).__init__(\n",
    "            encoder,\n",
    "            Projection(embedding_dim, latent_dim))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3292fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_projection = VisualProjection(visual_encoder, 512, LATENT_DIM).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f96c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskClassifier(nn.Module):\n",
    "    def __init__(self, latent_dim: int, tasks: list):\n",
    "        super().__init__()\n",
    "        self.tasks = nn.ModuleList([Head(latent_dim, num_clases) for num_clases in tasks])\n",
    "                                         \n",
    "    def forward(self, x):\n",
    "        return [task(x) for task in self.tasks]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2485ebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_load = [len(labels[x]) for x in labels]\n",
    "classifier = MultitaskClassifier(LATENT_DIM, task_load).to(DEVICE)\n",
    "for out in classifier(visual_projection(X[0].to(DEVICE))): print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8abc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlignMultitaskModel(nn.Module):\n",
    "    def __init__(self, visual_projection: nn.Module, semantic_projection: nn.Module,\n",
    "                       visual_classifier: nn.Module, semantic_classifier: nn.Module):\n",
    "        super(AlignMultitaskModel, self).__init__()\n",
    "        self.visual_projection = visual_projection\n",
    "        self.semantic_projection = semantic_projection\n",
    "        self.visual_classifier = visual_classifier\n",
    "        self.semantic_classifier = semantic_classifier\n",
    "\n",
    "    def forward(self, vx, sx):\n",
    "        # calculate vectors\n",
    "        vx = self.visual_projection(vx)\n",
    "        sx = self.semantic_projection(sx)\n",
    "        # calculate projections similarity\n",
    "        d = torch.softmax(sx @ vx.T, dim=-1)\n",
    "        # run detectors\n",
    "        v = self.visual_classifier(vx)\n",
    "        s = self.semantic_classifier(sx)\n",
    "        return [d] + list(v) + list(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be065593",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_classifier = MultitaskClassifier(LATENT_DIM, task_load).to(DEVICE)\n",
    "semantic_classifier = MultitaskClassifier(LATENT_DIM, task_load).to(DEVICE)\n",
    "\n",
    "model = AlignMultitaskModel(visual_projection, semantic_projection,\n",
    "                            visual_classifier, semantic_classifier).to(DEVICE)\n",
    "\n",
    "for out in model(X[0].to(DEVICE), X[1].to(DEVICE)): print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa13ba34",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fbe617",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = np.random.choice(samples, int(len(samples) * 0.95), replace=False)\n",
    "test_samples = list(set(samples).difference(set(train_samples)))\n",
    "len(train_samples), len(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa5b629",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SemanticAlignDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b236511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [1. - stats.groupby(task).size()/len(stats) for task in labels]\n",
    "weights = [list(w / sum(w)) for w in weights]\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a45f106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tasks loss\n",
    "criteria = [nn.CrossEntropyLoss().to(DEVICE)] +\\\n",
    "           [nn.CrossEntropyLoss(weight=torch.tensor(w, dtype=torch.float32)).to(DEVICE) for w in weights] * 2\n",
    "# composite parameterized loss\n",
    "criterion = HydraLoss(criteria).to(DEVICE)\n",
    "# optimize both: model and loss\n",
    "params = [p for p in model.parameters()] + [p for p in criterion.parameters()]\n",
    "\n",
    "learning_rate = 1e-6\n",
    "optimizer = AdamW(params, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7510c725",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {'align': {'confmat': ConfusionMatrix(task='multiclass', num_classes=batch_size).to(DEVICE)}}\n",
    "for prefix in ['visual','semantic']:\n",
    "    for i, task in enumerate(labels):    \n",
    "        metrics[f'{prefix}-{task}'] = {\n",
    "            'f1-score': F1Score(task='multiclass', num_classes=len(weights[i])).to(DEVICE),\n",
    "            'confmat': ConfusionMatrix(task='multiclass', num_classes=len(weights[i])).to(DEVICE)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edcce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "trainer = Trainer(model, dataset, VIEW_SIZE, criterion, optimizer, metrics, multi_x=True, multi_y=True,\n",
    "                  autocast=False)\n",
    "results = trainer.run(train_samples, test_samples, batch_size, num_epochs=num_epochs, validation_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbd4092",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(trainer.loss_history, trainer.metrics_history, multi_x=True, multi_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f42137",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da520c97",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "matrix = np.sum(np.array(results['align']['confmat']), axis=0)\n",
    "ax.imshow(matrix/np.max(matrix), cmap='coolwarm')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title('align confusion matrix')\n",
    "plt.show()\n",
    "\n",
    "for task in labels:\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(5, 5))\n",
    "    for i, prefix in enumerate(['visual','semantic']):\n",
    "        matrix = np.sum(np.array(results[f'{prefix}-{task}']['confmat']), axis=0)\n",
    "        ax[i].imshow(matrix/np.max(matrix), cmap='coolwarm')\n",
    "        ax[i].set_xticks(range(len(labels[task])))\n",
    "        ax[i].set_title(f'{prefix}-{task}', fontsize=10)\n",
    "    ax[0].set_yticks([])\n",
    "    ax[1].set_xticks(range(len(labels[task])))\n",
    "    ax[1].yaxis.tick_right()\n",
    "    ax[1].set_yticks(range(len(labels[task])))\n",
    "    ax[1].set_yticklabels([f'{k}: {v}' for k, v in enumerate(labels[task])], fontsize=10)\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a8e73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "V, S, L = [], [], {x:[] for x in labels} # pair vectors with labels\n",
    "for source in np.random.choice(samples, 100, replace=False):\n",
    "    loader = DataLoader(SemanticAlignDataset(source, VIEW_SIZE, batch_size, debug=False), batch_size=batch_size)\n",
    "    for X, Y in loader:\n",
    "        with torch.no_grad():\n",
    "            V += list(visual_projection(X[0].to(DEVICE)).cpu().numpy())\n",
    "            S += list(semantic_projection(X[1].to(DEVICE)).cpu().numpy())\n",
    "        for i, label in enumerate(labels.keys(), 1):\n",
    "            L[label] += Y[i].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f96ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_labels(pe, te, color, labels, title):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(9, 4))\n",
    "    cmap = colormaps['gist_rainbow']\n",
    "    for c in range(len(labels)):\n",
    "        ax[0].scatter(te[color==c,0], te[color==c,1], s=5, color=cmap(c/len(labels)), alpha=0.5)\n",
    "        ax[1].scatter(pe[color==c,0], pe[color==c,1], s=5, color=cmap(c/len(labels)), alpha=0.5, label=labels[c])\n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_yticks([])\n",
    "    ax[0].set_title(f'{title}: tSNE')\n",
    "    ax[1].set_xticks([])\n",
    "    ax[1].set_yticks([])\n",
    "    ax[1].set_title(f'{title}: PCA')\n",
    "    ax[1].legend(bbox_to_anchor=(1, 1), frameon=False)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5b75e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10)\n",
    "norm = StandardScaler().fit(V)\n",
    "E = norm.transform(V)\n",
    "P = pca.fit_transform(E)\n",
    "T = TSNE(n_components=2, perplexity=90).fit_transform(E)\n",
    "\n",
    "for task in labels:\n",
    "    plot_labels(P, T, np.array(L[task]), labels[task], f'Visual {task}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b390d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10)\n",
    "norm = StandardScaler().fit(S)\n",
    "E = norm.transform(S)\n",
    "P = pca.fit_transform(E)\n",
    "T = TSNE(n_components=2, perplexity=90).fit_transform(E)\n",
    "\n",
    "for task in labels:\n",
    "    plot_labels(P, T, np.array(L[task]), labels[task], f'Semantic {task}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccb888f",
   "metadata": {},
   "source": [
    "## Simple search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523dd192",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX = 'visual-align'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ae8239",
   "metadata": {},
   "outputs": [],
   "source": [
    "qclient = QdrantClient(':memory:')\n",
    "qclient.delete_collection(collection_name=INDEX)\n",
    "qclient.create_collection(\n",
    "    collection_name=INDEX, \n",
    "    vectors_config=VectorParams(size=LATENT_DIM, distance=Distance.COSINE),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c054ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = []\n",
    "\n",
    "for observation in NONDOC:\n",
    "    payload.append({ 'observation':observation, 'action':None })\n",
    "\n",
    "for observation in SCOPE.values():\n",
    "    payload.append({ 'observation':observation, 'action':'fix rotation' })\n",
    "    \n",
    "for angle in range(1, 360):\n",
    "    angle, d1, d2 = (360 - angle, '', 'counter') if angle > 180 else (angle, 'counter', '')\n",
    "    payload.append({'observation':f'rotated {angle} degrees {d1}clockwise',\n",
    "                    'action':f'rotate {angle} degrees {d2}clockwise'})\n",
    "\n",
    "payload += [\n",
    "    { 'observation':'straight', 'action':'fix zoom' },\n",
    "    { 'observation':'turned on the left side', 'action':'rotate 90 degrees clockwise' },\n",
    "    { 'observation':'turned upside-down', 'action':'rotate 180 degrees' },\n",
    "    { 'observation':'turned on the right side', 'action':'rotate 90 degrees counterclockwise' },\n",
    "    \n",
    "    { 'observation':'no text and no lines', 'action':'zoom out' },\n",
    "    { 'observation':'text', 'action':'fix rotation' },\n",
    "    { 'observation':'lines', 'action':'zoom out' },\n",
    "    { 'observation':'text and lines', 'action':'fix rotation' },\n",
    "]\n",
    "\n",
    "for scope in SCOPE.values():\n",
    "    for content in ['text and lines','text','lines']:\n",
    "        for rotation in list(ORIENTATION.values()) + ['rotated counterclockwise','rotated clockwise']:\n",
    "            payload.append({'observation':f'{scope} with {content} {rotation}',\n",
    "                            'action':''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2b1338",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = semantic_encoder.encode([x['observation'] for x in payload])\n",
    "with torch.no_grad():\n",
    "    embeddings = semantic_projection(torch.Tensor(embeddings).to(DEVICE)).cpu().numpy()\n",
    "    \n",
    "embeddings.shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf177bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5cd32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qclient.upload_collection(\n",
    "    collection_name=INDEX,\n",
    "    vectors=embeddings,\n",
    "    payload=payload,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eea4d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar(observation_vector: list, limit: int = 5):\n",
    "    # search for closest vectors with type `observation`\n",
    "    results = qclient.search(\n",
    "        collection_name=INDEX,\n",
    "        query_vector=observation_vector,\n",
    "        #query_filter=Filter(must=[FieldCondition(key='type', match=MatchValue(value='observation'))]),\n",
    "        limit=limit,\n",
    "    )\n",
    "    # return top matches with scores\n",
    "    return [{**x.payload, **{'score':x.score}} for x in results if x.score >= 0.25]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23dff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar(embeddings[0,:], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0120f337",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'monospace'\n",
    "sample = np.random.choice(samples)\n",
    "loader = DataLoader(SemanticAlignDataset(sample, VIEW_SIZE, batch_size, debug=True), batch_size=batch_size)\n",
    "for X, Y in loader:\n",
    "    with torch.no_grad():\n",
    "        # extract visual features\n",
    "        P = visual_projection(X[0].to(DEVICE)).cpu().numpy()\n",
    "        # get predictions\n",
    "        preds = model(X[0].to(DEVICE), X[1].to(DEVICE))\n",
    "        matrix = preds[0].cpu().numpy()\n",
    "        tasks = [torch.argmax(p, dim=1).cpu().numpy() for p in preds[1:]]\n",
    "        fig, ax = plt.subplots(figsize=(4, 4))\n",
    "        ax.imshow(matrix, 'coolwarm')\n",
    "        ax.yaxis.tick_right()\n",
    "        ax.set_yticks(range(len(Y[0])))\n",
    "        ax.set_yticklabels(['{}. {}'.format(i, y) for i, y in enumerate(Y[0])], fontsize=8)\n",
    "        ax.set_xticks(range(len(Y[0])))\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "        \n",
    "    for i in range(batch_size):\n",
    "        similar = '\\n'.join([f\"{x['observation']} [score {x['score']:.2f}]\" for x in find_similar(P[i,:], 3)])\n",
    "        fig, ax = plt.subplots(figsize=(4, 4))\n",
    "        ax.imshow(X[0][i,:].squeeze(), 'gray')\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'True:\\n{Y[0][i]}\\n\\nSearch:\\n{similar}', ha='left', x=0, fontsize=10)\n",
    "\n",
    "        text = [('    Task        Visual        Semantic      True\\n'\n",
    "                 '   ------------------------------------------------------')] +\\\n",
    "               [(f'    {task:<12}{labels[task][tasks[k][i]]:<12}  {labels[task][tasks[k + 4][i]]:<12}'\n",
    "                 f'  {labels[task][Y[k + 1][i]]:<12}') for k, task in enumerate(labels)]\n",
    "\n",
    "        ax.annotate('\\n'.join(text), xy=(1, 0.5), xytext=(0, 10), xycoords=('axes fraction','figure fraction'),\n",
    "                    textcoords='offset points', size=10, ha='left', va='center')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d91e2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(visual_projection.state_dict(), f'./models/visual-projection-CNN.pt')\n",
    "torch.save(visual_classifier.state_dict(), f'./models/visual-classifier-CNN.pt')\n",
    "torch.save(semantic_projection.state_dict(), f'./models/semantic-projection-CNN.pt')\n",
    "torch.save(semantic_classifier.state_dict(), f'./models/semantic-classifier-CNN.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c960552c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
