{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49c166cd",
   "metadata": {},
   "source": [
    "# Data extraction from PDF forms\n",
    "For this project we need\n",
    "* `layout`: hierarchy of blocks defined by bounding box coordinates and `type`\n",
    "* `text`: words and `phrases` (linked sequences)\n",
    "* `inputs`: fields where certain data should be entered defined by text `label` and data type spec.\n",
    "* `images`: some are `logos` we want to recognize; some contain text we want to be aware of for our vision model\n",
    "\n",
    "There are several options, we use `PyMuPDF` package: `scripts/parse.py` is initial bulk extraction for exploration.\n",
    "\n",
    "For our doc-indexing pipeline we need a refined version based on the representation model our exploration outputs. We also need to chose embedding models (text and image) for similarity queries.\n",
    "\n",
    "* For the text embeddings we are good to go with a pretrained model, maybe with a minimal tune up.\n",
    "* For the image embedding we are going to train our own model based on either `ResNet` or `ViT` architecture adapted to grayscale.\n",
    "\n",
    "The single-source-batch data-loaders we use could make learning very sensitive to data quality: we need a way to classify each source for fitness to be a learning sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a492c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781ca126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run initial parsing for exploration\n",
    "#!python scripts/parse.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e6336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc-level lookup table\n",
    "docs = pd.read_csv('./data/forms.csv.gz')\n",
    "docs = docs.loc[docs['lang'].isin(['en','fr','sp'])].fillna('')\n",
    "docs['taxonomy'] = docs.apply(lambda r:f\"{r['type']}{r['sub']}\".strip().upper(), axis=1)\n",
    "\n",
    "# page-level reference (multipage docs)\n",
    "pages = pd.read_csv('./data/page-summary.csv.gz')\n",
    "pages['file'] = pages['source'].apply(lambda x:'-'.join(x.split('-')[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e34059",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOX = ['top','left','bottom','right']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcae5fc",
   "metadata": {},
   "source": [
    "### Explore data quality evaluation strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a460ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed = []\n",
    "for i,source in enumerate(pages[(pages['num-pages']==1)&(pages['text-input'].isna())]['source'].to_list()):\n",
    "    try:\n",
    "        D = pd.read_csv(f'data/info/{source}.csv.gz')\n",
    "    except FileNotFoundError:\n",
    "        print(source)\n",
    "        continue        \n",
    "    D = D.loc[D['block-type']=='word']\n",
    "    D.loc[:,'text'] = D.loc[:,'text'].fillna('').astype(str).str.strip()\n",
    "    D = D.loc[D['text']!='']\n",
    "    text = ' '.join(D['text'].to_list())\n",
    "    if text.find('The document you are trying to load requires Adobe Reader') != -1 \\\n",
    "       or text.find('Please wait...') != -1:\n",
    "        failed.append(source[:-2])\n",
    "len(failed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d3d30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index, stats = [],[]\n",
    "# gather page word stats\n",
    "for i,source in enumerate(pages['source'].to_list()):\n",
    "    try:\n",
    "        D = pd.read_csv(f'data/info/{source}.csv.gz')\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    # filter out duplicate blocks (lines)\n",
    "    boxes = D.sort_values('block-type', ascending=False)\n",
    "    boxes = boxes.loc[boxes['block-type'].isin(['block','line']), BOX]\n",
    "    boxes.loc[:,BOX] = np.round(boxes.loc[:,BOX] * 1000).astype(int)\n",
    "    n = len(boxes)\n",
    "    boxes = boxes.drop_duplicates(keep='first')\n",
    "    drop = D[(D['block-type']=='block')&(~D.index.isin(boxes.index))].index\n",
    "    #if len(drop) < n - len(boxes):\n",
    "    #print(f'duplicate blocks: dropped {len(drop)} of {n - len(boxes)} ...')\n",
    "    D = D.loc[~D.index.isin(drop)]    \n",
    "    D.to_csv(f'data/info/{source}.csv.gz', index=False, compression='gzip')\n",
    "    \n",
    "    if len(D.loc[(D['sin'] > 0)&(D['sin'] < 1)]) > 0:\n",
    "        print(source)\n",
    "    \n",
    "    words = D.loc[D['block-type']=='word']\n",
    "    words.loc[:,'text'] = words.loc[:,'text'].fillna('').astype(str).str.strip()\n",
    "    #words.loc[:,BOX] = words.loc[:,BOX].astype(float)\n",
    "    words = words.loc[words['text']!='']\n",
    "    if len(words) > 0:\n",
    "        words['height'] = words['bottom'] - words['top']\n",
    "        words['width'] = words['right'] - words['left']\n",
    "        # estimate space between the words\n",
    "        space = words[['top','left']].merge(words[['top','right']], on='top')\n",
    "        space = space.loc[space['left'] > space['right']]\n",
    "        space = (space['left'] - space['right']).min()        \n",
    "        words = words.median(numeric_only=True).to_dict()\n",
    "        words['space'] = space\n",
    "        stats.append(words)\n",
    "        index.append(source)\n",
    "    print(f'done: {(i + 1)/len(pages):.2%}', end='\\r')\n",
    "\n",
    "stats = pd.DataFrame.from_dict(stats)\n",
    "mean = stats.mean(numeric_only=True)\n",
    "pages = pages.set_index('source')\n",
    "pages.loc[index,stats.columns[:-2]] = stats.values[:,:-2]\n",
    "pages['word-width'] = None\n",
    "pages['word-height'] = None\n",
    "pages.loc[index,['word-width','word-height','space']] = stats[['width','height','space']].values\n",
    "\n",
    "# detect and mark outlier-pages\n",
    "div = np.log(((stats - mean) ** 2).sum(axis=1))\n",
    "div /= div.max()\n",
    "pages['div'] = 1\n",
    "pages.loc[index,'div'] = div.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8023967",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "div.plot(kind='density', ax=ax[0])\n",
    "ax[0].axvline(x=0.32, linestyle=':')\n",
    "ax[0].axvline(x=0.55, linestyle=':')\n",
    "ax[1].plot(sorted(div.to_list())[::-1])\n",
    "ax[1].axhline(y=0.32, linestyle=':')\n",
    "ax[1].axhline(y=0.55, linestyle=':')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c5b082",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outliers = pages[pages['div'] > 0.8].index.to_list()\n",
    "#Image.open(f'./data/images/{np.random.choice(outliers)}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4763727",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = set(pages[(pages['div'] < 0.5)|(pages['div'] >= 0.8)].index)\n",
    "len(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf311e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages[(pages['div'] >= 0.5)&(pages['div'] < 0.8)].to_csv('./data/pages.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d6720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [str(x).split('/').pop()[:-4] for x in Path('./data/images').glob('*.png')]\n",
    "info = [str(x).split('/').pop()[:-7] for x in Path('./data/info').glob('*.csv.gz')]\n",
    "len(set(images).intersection(set(outliers))), len(set(info).intersection(set(outliers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a5416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for source in outliers:\n",
    "    os.remove(f'data/images/{source}.png')\n",
    "    os.remove(f'data/info/{source}.csv.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f66fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [str(x).split('/').pop()[:-4] for x in Path('./data/images').glob('*.png')]\n",
    "info = [str(x).split('/').pop()[:-7] for x in Path('./data/info').glob('*.csv.gz')]\n",
    "len(set(images).intersection(set(outliers))), len(set(info).intersection(set(outliers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b867b7b1",
   "metadata": {},
   "source": [
    "### Explore images on the pages\n",
    "Some logos contain the text which can mess up our learning; but they can help identify the page origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaf7301",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = pd.read_csv('./data/pages.csv.gz')\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bf05a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_text(data):\n",
    "    images = data.loc[data['block-type']=='image']\n",
    "    if len(images) == 0:\n",
    "        return data\n",
    "    source, page = data.iloc[0][['source','page']]\n",
    "    image = np.array(ImageOps.grayscale(Image.open(f'./data/images/{source}-{page}.png')))\n",
    "    scale = min(image.shape)\n",
    "    text = []\n",
    "    for t, l, b, r in images[['top','left','bottom','right']].values:\n",
    "        if b - t > 0.5 or r - l > 0.5:\n",
    "            text.append('IMAGE: ')\n",
    "            continue\n",
    "        t, l, b, r = int(t * scale), int(l * scale), int(b * scale), int(r * scale)\n",
    "        try:\n",
    "            clip = image[max(t - 5, 0):min(b + 5, image.shape[0]), max(l - 5, 0):min(r + 5, image.shape[1])]\n",
    "            t = ts.image_to_string(clip).strip()\n",
    "            t = ' '.join(re.split(r'\\W+', t)).strip()\n",
    "            text.append(f'IMAGE: {t}')\n",
    "        except:\n",
    "            text.append('IMAGE: ')\n",
    "    data.loc[data['block-type']=='image','text'] = text\n",
    "    return data\n",
    "\n",
    "\n",
    "source = np.random.choice(info)\n",
    "data = pd.read_csv(f'./data/info/{source}.csv.gz')\n",
    "data = image_text(data)\n",
    "data[data['block-type']=='image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3050dc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f05be8b",
   "metadata": {},
   "source": [
    "    image_to_text = []\n",
    "    for i, source in enumerate(info):\n",
    "        data = pd.read_csv(f'./data/info/{source}.csv.gz')\n",
    "        images = data.loc[data['block-type']=='image']\n",
    "        if len(images) == 0:\n",
    "            continue\n",
    "        image = np.array(ImageOps.grayscale(Image.open(f'./data/images/{source}.png')))\n",
    "        scale = min(image.shape)\n",
    "        for d in images.to_dict('records'):\n",
    "            t, b = int(d['top'] * scale), int(d['bottom'] * scale)\n",
    "            l, r = int(d['left'] * scale), int(d['right'] * scale)\n",
    "            try:\n",
    "                clip = image[min(t - 5, 0):min(b + 5, image.shape[0]),min(l - 5, 0):min(r + 5, image.shape[1])]\n",
    "                d['text'] = ts.image_to_string(clip).strip()\n",
    "            except:\n",
    "                print('error...')\n",
    "                d['text'] = 'ERROR'\n",
    "            image_to_text.append(d)\n",
    "        print(f'done: {(i + 1)/len(info):.2%}', end='\\r')\n",
    "\n",
    "    data.to_csv('./data/image-text.csv.gz', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b88648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/image-text.csv.gz')\n",
    "print(f\"errors: {len(data[data['text']=='ERROR'])/len(data):.2%}\")\n",
    "print(f\"text: {len(data[~data['text'].isna()])/len(data):.2%}\")\n",
    "print(f\"?: {len(data[data['text'].isna()])/len(data):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7010fe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(11, 5))\n",
    "data['aspect-ratio'].plot(kind='density', ax=ax[0])\n",
    "for x in [0.42, 0.60, 0.77, 1.29, 1.65, 2.35]:\n",
    "    ax[0].axvline(x=x, linestyle=':')\n",
    "ax[0].set_yticks([])\n",
    "ax[0].set_title('Pages aspect ratio dist.')\n",
    "data['scale'].plot(kind='density', ax=ax[1])\n",
    "for x in [700, 1700]:\n",
    "    ax[1].axvline(x=x, linestyle=':')\n",
    "ax[1].set_yticks([])\n",
    "ax[1].set_title('Pages scale dist.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d112bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source, page = data[data['scale'] < 1000].sample().iloc[0][['source','page']]\n",
    "#Image.open(f'data/images/{source}-{page}.png')\n",
    "len(set(data[data['scale'] < 1000]['source']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0ecfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data['scale'] > 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563b6f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['width'] = data['right'] - data['left']\n",
    "data['height'] = data['bottom'] - data['top']\n",
    "data['area'] = data['width'] * data['height']\n",
    "print(f\"cover: {len(data[(data['width'] > 0.9)|(data['width'] > 0.6)])/len(data):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dcbc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:,BOX + ['width','height']] = np.round(data.loc[:,BOX + ['width','height']] * 100)\n",
    "data['area'] = data['width'] * data['height']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec5d9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data['area'].sort_values(ascending=False).values)\n",
    "plt.title('Rank by covered area')\n",
    "plt.axvline(x=700, linestyle=':')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f85c943",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values('area', ascending=False)\n",
    "test = data.iloc[:700]\n",
    "data = data.iloc[700:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8686f268",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(11, 11))\n",
    "w, h = int(data['left'].max()), int(data['top'].max())\n",
    "matrix = np.zeros((h, w))\n",
    "for t, l, b, r in data[~data['text'].isna()][BOX].values.astype(int):\n",
    "    matrix[t:b,l:r] += 1\n",
    "ax[0].imshow(matrix/np.max(matrix), 'Reds')\n",
    "ax[0].set_title('Small images')\n",
    "    \n",
    "w, h = int(test['left'].max()), int(test['top'].max())\n",
    "matrix = np.zeros((h, w))\n",
    "for t, l, b, r in test[~test['text'].isna()][BOX].values.astype(int):\n",
    "    matrix[t:b,l:r] += 1\n",
    "ax[1].imshow(matrix/np.max(matrix), 'Blues')\n",
    "ax[1].set_title('Big images')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a92b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[data['text'].isna()])/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f74133",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test[test['text'].isna()])/len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1986a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(11, 5))\n",
    "data['area'].plot(kind='density', ax=ax[0])\n",
    "ax[0].set_yticks([])\n",
    "ax[0].set_title('Image covered area')\n",
    "ax[1].scatter(data['width'], data['height'], s=3, alpha=0.3)\n",
    "ax[1].set_title('Image shape [width, height]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165b30d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfe8a71",
   "metadata": {},
   "source": [
    "### Interactive correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230c456e",
   "metadata": {},
   "source": [
    "    image_to_text = []\n",
    "    for source in [str(x).split('/').pop()[:-7] for x in Path('./data/info').glob('*.csv.gz')]:\n",
    "        data = pd.read_csv(f'./data/info/{source}.csv.gz')\n",
    "        images = data.loc[data['block-type']=='image']\n",
    "        if len(images) == 0:\n",
    "            continue\n",
    "        if data.iloc[0]['scale'] < 1000:\n",
    "            # not a form\n",
    "            continue\n",
    "\n",
    "        image = np.array(ImageOps.grayscale(Image.open(f'./data/images/{source}.png')))\n",
    "        scale = min(image.shape)\n",
    "        for d in images.to_dict('records'):\n",
    "            t, b = int(d['top'] * scale), int(d['bottom'] * scale)\n",
    "            l, r = int(d['left'] * scale), int(d['right'] * scale)\n",
    "            clip = image[t - 10:b + 10,l - 10:r + 10]\n",
    "            img = plt.imshow(clip, 'gray')\n",
    "            plt.title(f\"{source}   [{d['top']:.4f}, {d['bottom']:.4f}, {d['left']:.4f}, {d['right']:.4f}]\")\n",
    "            #img.set_data(clip)\n",
    "            display(plt.gcf())\n",
    "            clear_output(wait=True)        \n",
    "\n",
    "            text = ts.image_to_string(clip).strip()\n",
    "            correction = input(' '.join(text.split())+'\\n')\n",
    "            d['text'] = correction.strip()\n",
    "            image_to_text.append(d)\n",
    "\n",
    "    pd.DataFrame.from_dict(image_to_text).to_csv('./data/image-text.csv.gz', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6b563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "            # block-num is unreliable: got to scan through instead of simple merge on block-num + word\n",
    "            #df = lines.loc[(lines['block-type']!='image')&(~lines['text'].isna()),['text'] + INFO]\n",
    "            #df['text'] = df['text'].apply(str.split)\n",
    "            #df = df.explode('text')\n",
    "            #D, W = df['text'].values, words['text'].values\n",
    "            #i, j = 0, 0\n",
    "            #while i < len(W) and j < len(D):\n",
    "            #    while i < len(W) and len(np.where(D == W[i])[0]) == 0:\n",
    "            #        i += 1\n",
    "            #    if i >= len(W):\n",
    "            #        break\n",
    "            #    j = np.where(D == W[i])[0][0]\n",
    "            #    while i < len(W) and j < len(D) and W[i] == D[j]:\n",
    "            #        words.iloc[i,-5:] = df.iloc[j,-5:]\n",
    "            #        i += 1; j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55bb6b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "550b8b8e",
   "metadata": {},
   "source": [
    "### Visual encoder: embeddings model\n",
    "Let's start from [ResNet](https://pytorch.org/vision/0.8/_modules/torchvision/models/resnet.html) and see if we can get away with 512 embedding size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a8fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch._dynamo.config.verbose = True\n",
    "torch.cuda.empty_cache()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('GPU' if device == 'cuda' else 'no GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7979293",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIEW_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22027ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL='RN18'\n",
    "LATENT_DIM = models.resnet.ResNet(models.resnet.BasicBlock, [2, 2, 2, 2]).fc.in_features\n",
    "print(LATENT_DIM) # embedding size\n",
    "\n",
    "class GrayResNetEncoder(models.resnet.ResNet):\n",
    "    def __init__(self, block, layers):\n",
    "        self.inplanes = 64\n",
    "        super(GrayResNetEncoder, self).__init__(block, layers)\n",
    "        # the first layer grayscale adaptation\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.fc = nn.Identity()\n",
    "\n",
    "# ResNet18        \n",
    "encoder = GrayResNetEncoder(models.resnet.BasicBlock, [2, 2, 2, 2])\n",
    "# load state dicts trained in baseline-exploration\n",
    "encoder.load_state_dict(torch.load('./models/visual-encoder-RN18.pt', map_location='cpu'))\n",
    "#encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0b053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir data/clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40131a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf data/clips/*.png\n",
    "\n",
    "index, embeddings = [],[]\n",
    "for i, source in enumerate(info):\n",
    "    data = pd.read_csv(f'./data/info/{source}.csv.gz')\n",
    "    images = data.loc[data['block-type']=='image']\n",
    "    if len(images) == 0:\n",
    "        continue\n",
    "    image = np.array(ImageOps.grayscale(Image.open(f'./data/images/{source}.png')))\n",
    "    scale = min(image.shape)\n",
    "    k = -1\n",
    "    for t, l, b, r in (images[BOX]  * scale).astype(int).values:\n",
    "        k += 1\n",
    "        clip = image[max(t - 5, 0):min(b + 5, image.shape[0]),max(l - 5, 0):min(r + 5, image.shape[1])]\n",
    "        if min(clip.shape) > scale//2:\n",
    "            # cover image\n",
    "            continue\n",
    "        if min(clip.shape) == 0:\n",
    "            text = 'ZERO'\n",
    "            continue\n",
    "        else:\n",
    "            #try:\n",
    "            #    text = ts.image_to_string(clip).strip()\n",
    "            #except:\n",
    "            #    text = 'ERROR'\n",
    "            \n",
    "            size = tuple((np.array(clip.shape) * VIEW_SIZE/max(clip.shape)).astype(int))[::-1]\n",
    "            try:\n",
    "                clip = Image.fromarray(clip).resize(size)\n",
    "                path = f'./data/clips/{source}-C{k}.png'\n",
    "            except:\n",
    "                continue\n",
    "            else:\n",
    "                clip.save(path)                \n",
    "                clip = 255. - np.array(clip).astype(float)\n",
    "                mn, mx = np.min(clip), np.max(clip)\n",
    "                if mn == mx:\n",
    "                    text = 'EMPTY'\n",
    "                    continue\n",
    "                else:                \n",
    "                    img = np.zeros((VIEW_SIZE, VIEW_SIZE))\n",
    "                    h, w = (VIEW_SIZE - clip.shape[0])//2, (VIEW_SIZE - clip.shape[1])//2\n",
    "                    img[h:h + clip.shape[0],w:w + clip.shape[1]] = (clip - mn)/(mx - mn)\n",
    "                    with torch.no_grad():\n",
    "                        vec = encoder(torch.Tensor(img.reshape((1, 1, VIEW_SIZE, VIEW_SIZE))))\n",
    "                \n",
    "        embeddings.append(vec.numpy().squeeze())\n",
    "        index.append({'source':source, 'path':path })\n",
    "        \n",
    "    print(f'done: {(i + 1)/len(info):.2%}', end='\\r')\n",
    "\n",
    "print(f'processed: {len(index)} images')\n",
    "pd.DataFrame.from_dict(index).to_csv('./data/clips.csv.gz', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8359a680",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(embeddings).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15da926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.array(embeddings).squeeze()\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "norm = StandardScaler().fit(embeddings)\n",
    "pca.fit(norm.transform(embeddings))\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d400105",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pca.transform(norm.transform(embeddings))\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10, 3))\n",
    "# top two components colored by feature value\n",
    "for i,j in [[0,1],[1,2],[2,0]]:\n",
    "    ax[i].scatter(Y[:,i], Y[:,j], s=3, alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5daae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "model = MiniBatchKMeans(n_clusters=3, n_init=100)\n",
    "C3 = model.partial_fit(Y).predict(Y)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10, 3))\n",
    "# top two components colored by feature value\n",
    "for i,j in [[0,1],[1,2],[2,0]]:\n",
    "    ax[i].scatter(Y[:,i], Y[:,j], s=3, c=C3, cmap='rainbow', alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159f0101",
   "metadata": {},
   "source": [
    "#### Exploring high-dimensional data with [t-SNE](https://distill.pub/2016/misread-tsne/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b388f24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = pd.DataFrame(TSNE(n_components=2, perplexity=90).fit_transform(np.array(embeddings)), columns=['x','y'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.scatter(T['x'], T['y'], c=C3, cmap='rainbow', alpha=0.5, s=5)\n",
    "ax.set_title('Documents embeddings by component')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180c1aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "for c in range(3):\n",
    "    t = T.loc[C3==c]\n",
    "    S = SpectralClustering(n_clusters=7, assign_labels='discretize').fit_predict(t.values)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.scatter(t['x'], t['y'], c=S, cmap='rainbow', alpha=0.5, s=5)\n",
    "    ax.set_title('Documents embeddings by component')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bcb5da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "for c in range(3):\n",
    "    t = T.loc[C3==c]\n",
    "    S = DBSCAN(eps=2., min_samples=5).fit_predict(t.values)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.scatter(t['x'], t['y'], c=S, cmap='rainbow', alpha=0.5, s=5)\n",
    "    ax.set_title(f'Cluster C{c} subclusters')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b48c59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5439745",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
