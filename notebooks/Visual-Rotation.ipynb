{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dcb5e52",
   "metadata": {},
   "source": [
    "# Visual: rotation\n",
    "This notebook explores `rotation` part of the context. We use our [prospective pretrained visual encoders](#encoders) and explore how much of the rotation information we've got there:\n",
    "let's check both [classification and regression](#model) scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cf6f1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "rcParams['xtick.labelsize'] = 7\n",
    "rcParams['ytick.labelsize'] = 7\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from torch import nn, Tensor\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torchmetrics import F1Score, ConfusionMatrix, R2Score\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81322936",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# local lib\n",
    "from scripts import simulate as sim\n",
    "from scripts import parse, render\n",
    "from scripts.backbone import *\n",
    "from scripts.trainer import *\n",
    "from scripts.dataset import NormalizeView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b58ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#torch._dynamo.config.verbose = True\n",
    "torch.cuda.empty_cache()\n",
    "print('GPU' if DEVICE == 'cuda' else 'no GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4a4e23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# semantic segmentation masks\n",
    "images = [str(x).split('/').pop() for x in Path('./data/masks').glob('*.png')\n",
    "           if not str(x).startswith('data/masks/que-')]\n",
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e624ac6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "VIEW_SIZE = 128\n",
    "LATENT_DIM = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e2191a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples = images #np.random.choice(images, 160, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6afd0b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class RotationDataset(Dataset):\n",
    "    def __init__(self, source: str, view_size: int, max_samples: int):\n",
    "        self.num_steps = max_samples\n",
    "        view = 255 - np.array(ImageOps.grayscale(Image.open(f'data/images/{source}')))\n",
    "        self.nav = render.AgentView(view, view_size)\n",
    "        self.transform = NormalizeView()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_steps\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        std = 0\n",
    "        while std < 10: # make sure there's something to see\n",
    "            rotation = np.random.randint(0, 360)\n",
    "            center = (np.array(self.nav.space.center) * (0.25 + np.random.rand(2) * 0.5)).astype(int)\n",
    "            zoom = -1. - np.random.rand() * 2\n",
    "            observation = self.nav.render(center, rotation, zoom)\n",
    "            std = np.std(observation)\n",
    "        X = self.transform(observation)\n",
    "        # classification target: integer angle\n",
    "        Y1 = rotation\n",
    "        # regression target: float [-1., 1.]\n",
    "        Y2 = torch.Tensor([-(360. - rotation)/180. if rotation > 180 else float(rotation)/180.]).float()\n",
    "        return X, (Y1, Y2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9bece7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample = np.random.choice(images)\n",
    "print(sample)\n",
    "# test loader\n",
    "batch_size = 4\n",
    "loader = DataLoader(RotationDataset(sample, VIEW_SIZE, batch_size), batch_size)\n",
    "# show first batch\n",
    "for X, (Y1, Y2) in loader:\n",
    "    for i in range(batch_size):\n",
    "        fig, ax = plt.subplots(figsize=(3, 3))\n",
    "        ax.imshow(X[i,:].squeeze(), 'gray')\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'rotation: [ {Y1[i].squeeze()} ] [ {Y2[i].squeeze():.2f} ]', fontsize=10)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34076a8a",
   "metadata": {},
   "source": [
    "<a name=\"encoders\"></a>\n",
    "\n",
    "### Backbones to compare\n",
    "For this experiment we use pretrained [CNN](Visual-Backbone-CNN.ipynb) and [ViT](Visual-Backbone-ViT.ipynb) backbones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa4c740",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_encoder = get_cnn_backbone(pretrained=True, frozen=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd7991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_encoder = get_vit_backbone(pretrained=True, frozen=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3d9182",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoders = { 'CNN':cnn_encoder, 'ViT':vit_encoder }\n",
    "tags = list(encoders.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab873921",
   "metadata": {},
   "source": [
    "<a name=\"model\"></a>\n",
    "\n",
    "### Model\n",
    "The model takes pretrained encoder and attaches two MPL-heads: 360 degrees classification and `[-1, 1]` regression. If classification works -- that's all we need. However, 360 classes is a lot. Regression may be even more challenging due to a fixed interval with singular edges, and it is less useful for us anyway. However, letting the encoders to learn further (do not freeze weights) both scenarios at once may improve the quality of the embeddings down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b81dabf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_cnn_head(output_dim: int):\n",
    "    return nn.Sequential(\n",
    "        nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        nn.Flatten(start_dim=1),\n",
    "        Head(LATENT_DIM, output_dim))\n",
    "\n",
    "def get_vit_head(output_dim: int):\n",
    "    return nn.Sequential(\n",
    "        MeanReduce(),\n",
    "        Head(LATENT_DIM, output_dim))\n",
    "            \n",
    "heads = { 'CNN':get_cnn_head, 'ViT':get_vit_head }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fa0ab1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class RotationEstimator(nn.Module):\n",
    "    def __init__(self, backbone: nn.Module, frozen: bool, get_head: Callable):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        if frozen: # freeze weights\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.classifier = get_head(360) # no softmax as we going to use crossentropy\n",
    "        self.regressor = get_head(1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedding = self.backbone(x)[-1]\n",
    "        cls = self.classifier(embedding)\n",
    "        reg = self.regressor(embedding)\n",
    "        return cls, reg\n",
    "\n",
    "# see how much we've got there\n",
    "#model = RotationEstimator(cnn_encoder, True, get_cnn_head)\n",
    "\n",
    "# see how much we can get there\n",
    "model = RotationEstimator(cnn_encoder, False, get_cnn_head)\n",
    "\n",
    "summary(model.to(DEVICE), (1, VIEW_SIZE, VIEW_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22db5fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for output in model(X.to(DEVICE)):\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232e842a",
   "metadata": {},
   "source": [
    "<a name=\"training\"></a>\n",
    "\n",
    "### Comparative training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd38be2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_samples = np.random.choice(samples, int(len(samples) * 0.95), replace=False)\n",
    "test_samples = list(set(samples).difference(set(train_samples)))\n",
    "len(train_samples), len(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3a5a99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ce891b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = RotationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce64e35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = [RotationEstimator(encoders[tag], False, heads[tag]).to(DEVICE) for tag in tags]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97544295",
   "metadata": {},
   "source": [
    "<a name=\"hydra\"></a>\n",
    "We can define our combined loss criterion as a weighted sum of tasks losses. However, tasks losses dynamics may not be well aligned along the training making static tasks weights a suboptimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a52d3ee",
   "metadata": {},
   "source": [
    "    class CombinedLoss(nn.Module):\n",
    "        def __init__(self, criteria: list, weights: list):\n",
    "            assert len(criteria) == len(weights)\n",
    "            super(CombinedLoss, self).__init__()\n",
    "            self.criteria = criteria\n",
    "            self.weights = weights        \n",
    "\n",
    "        def forward(self, preds, targets):\n",
    "            losses = []\n",
    "            for i, criterion in enumerate(self.criteria):\n",
    "                losses.append(criterion(preds[i], targets[i]) * self.weights[i])\n",
    "            return torch.sum(torch.stack(losses))\n",
    "\n",
    "    criterion = CombinedLoss(criteria, [1., 1., 10., 10.]).to(device)\n",
    "    # optimized model parameters only\n",
    "    params = model.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb55de01",
   "metadata": {},
   "source": [
    "Instead, we make tasks weights trainable parameters and learn them along the model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5ab02b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    class HydraLoss(nn.Module):\n",
    "        \"\"\"\n",
    "        Construct combined loss with trainable weights:\n",
    "        https://arxiv.org/abs/1705.07115\n",
    "        \"\"\"\n",
    "        def __init__(self, criteria: list):\n",
    "            super().__init__()\n",
    "            self.criteria = criteria\n",
    "            self.log_vars = nn.Parameter(torch.zeros((len(criteria))))\n",
    "\n",
    "        def forward(self, preds, targets):\n",
    "            losses = []\n",
    "            for i, criterion in enumerate(self.criteria):\n",
    "                loss = criterion(preds[i], targets[i])\n",
    "                losses.append(torch.exp(-self.log_vars[i]) * loss + self.log_vars[i])\n",
    "            return torch.sum(torch.stack(losses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0b53b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 5e-6\n",
    "\n",
    "criteria = [HydraLoss([nn.CrossEntropyLoss(), nn.MSELoss()]).to(DEVICE) for _ in range(len(models))]\n",
    "optimizers = [AdamW([p for p in model.parameters()] + [p for p in criterion.parameters()], lr=learning_rate)\n",
    "              for model, criterion in zip(models, criteria)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87733a6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics = [{ 'classification': {'f1-score': F1Score(task='multiclass', num_classes=360).to(DEVICE),\n",
    "                                'confmat': ConfusionMatrix(task='multiclass', num_classes=360).to(DEVICE) },\n",
    "             'regression': { 'r2-score': R2Score().to(DEVICE) }}\n",
    "           for _ in range(len(models))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9fbf81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = MultiTrainer(dataset, models, VIEW_SIZE, criteria, optimizers, metrics, tags=tags, multi_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea8639c",
   "metadata": {},
   "source": [
    "Let's run a few epochs with a full visual in between to see how it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12cf798",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 6\n",
    "validation_steps = 2\n",
    "k, offset = num_epochs//validation_steps, 0\n",
    "for _ in range(k):\n",
    "    # run training\n",
    "    results = trainer.run(train_samples, test_samples, batch_size, k, 1, offset=offset)\n",
    "    offset += k\n",
    "    # show loss and validation history\n",
    "    trainer.plot_compare()\n",
    "\n",
    "    # get predictions by both models for the same data\n",
    "    preds, targets = [[] for _ in range(len(models))], []\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "    for source in test_samples:\n",
    "        loader = DataLoader(RotationDataset(source, VIEW_SIZE, batch_size//2), batch_size//2)\n",
    "        for X, (Y1, Y2) in loader:\n",
    "            targets.append(Y2.squeeze().numpy())\n",
    "            with torch.no_grad():\n",
    "                for i in range(len(models)):\n",
    "                    torch.cuda.empty_cache()\n",
    "                    preds[i].append(models[i](X.to(DEVICE))[1].squeeze().cpu().numpy())\n",
    "\n",
    "    ticks = list(range(30, 361, 30))\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(8, 8))\n",
    "    for i, tag in enumerate(tags):        \n",
    "        # show classifier confusion\n",
    "        matrix = trainer.metrics_history[i]['classification']['confmat']\n",
    "        ax[0][i].imshow(matrix/np.max(matrix), cmap='coolwarm')\n",
    "        total = np.sum(matrix)\n",
    "        ax[0][i].set_xticks(ticks)\n",
    "        ax[0][i].set_yticks(ticks)\n",
    "        ax[0][i].set_title(f'{tags[i].upper()} confusion matrix', fontsize=10)\n",
    "        ax[0][i].set_xlabel('Predicted')\n",
    "        ax[0][i].set_ylabel('Actual')\n",
    "\n",
    "        # show regressor residuals\n",
    "        ax[1][i].scatter(targets, np.array(targets) - np.array(preds[i]), s=3, alpha=0.2)\n",
    "        ax[1][i].axhline(y=0, color='C3')\n",
    "        ax[1][i].set_title(f'{tag.upper()} residual plot')\n",
    "        ax[1][i].set_xlabel('Target')\n",
    "        ax[1][i].set_ylabel('Error')\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0453ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.plot_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3dd98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_compare()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d378b7cb",
   "metadata": {},
   "source": [
    "    for tag, model in zip(tags, models):\n",
    "        torch.save(model.state_dict(), f'./models/visual-rotation-{tag}.pt')\n",
    "        torch.save(encoders[tag].state_dict(), f'./models/visual-backbone-{tag}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a75d7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
