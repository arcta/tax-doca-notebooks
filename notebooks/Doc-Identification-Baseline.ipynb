{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dcb5e52",
   "metadata": {},
   "source": [
    "# Doc identification based on taxonomy markers\n",
    "Tax documents are a highly populated yet very tight semantic cluster: identification by the textual content might be inefficient or insufficient due to significant semantic overlap and the noise presented by the user inputs. Identification by the form title would not help much, some forms share the main/part of the title, and we would have to locate it first which might be challenge if we deal with uploaded scans where not only pages orientation but also pages order could be mixed. \n",
    "\n",
    "The official forms usually have `taxonomy markers` (type identifiers) displayed on the first page (sometimes on all the pages), most often they appear in the header and/or the footer areas of the page. If we could read those markers in a reliable way and have a lookup table -- the document identification would be straight forward no matter how populated the domain is. For the multi-page documents we only need to classify one page with confidence. Often, we don't even need to scan a whole page: only the header and footer areas.\n",
    "\n",
    "Our doc-identification model is a combination of taxonomy pattern matching and semantic search. (Classification models would be neither efficient nor stable with this data scenario.)\n",
    "\n",
    "With our [Indexing-Pipeline](./Indexing-Pipeline.ipynb) we leverage the PDF form-blanks to build semantic-index from the doc-titles and a pattern lookup table from the taxonomy-markers extracted from the header/footer areas. This notebook focused on the OCR pipeline: it should be scalable and resistant to OCR misses. First, we have to make sure that our strategy would work.\n",
    "\n",
    "* [Strategy](#desc)\n",
    "    * [Taxonomy match](#tax)\n",
    "    * [Semantic match](#sem)\n",
    "* [Expectation](#base)\n",
    "* [Pipeline:](#pipe)\n",
    "    * [Skew detection](#skew)\n",
    "    * [Orientation detection](#orient)\n",
    "    * [Content classification](#form)\n",
    "    * [Pattern match](#match)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cf6f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytesseract as pts\n",
    "import matplotlib as mpl\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches\n",
    "from pathlib import Path\n",
    "from IPython.display import display, clear_output\n",
    "from fuzzysearch import find_near_matches\n",
    "from fitz import fitz\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81322936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local lib\n",
    "from scripts import prep, parse\n",
    "from scripts import simulate as sim\n",
    "from scripts.baselines import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b133ff",
   "metadata": {},
   "source": [
    "    #torch._dynamo.config.verbose = True\n",
    "    torch.cuda.empty_cache()\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc5361",
   "metadata": {},
   "source": [
    "<a name=\"desc\"></a>\n",
    "\n",
    "## Strategy\n",
    "We've got our taxonomy-lookup table parsed from the form-blanks pdf files: we can guess that the file name reflects the taxonomy pattern we will be looking for the document identification. Let's check if that's the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44b7c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc-level lookup table\n",
    "docs = pd.read_csv('./data/forms.csv.gz')\n",
    "docs = docs.loc[docs['lang'].isin(['en','fr','sp'])].fillna('')\n",
    "docs['taxonomy'] = docs.apply(lambda r:f\"{r['type']}{r['sub']}\".strip().upper(), axis=1)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f9d757",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Documents: {len(docs.dropna())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794bccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# page-level reference (multipage docs)\n",
    "pages = pd.read_csv('./data/page-summary.csv.gz')\n",
    "pages['file'] = pages['source'].apply(lambda x:'-'.join(x.split('-')[:-1]))\n",
    "pages = pages.merge(docs[['file','taxonomy','ext','lang']], on='file')\n",
    "pages.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50014f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Pages: {len(pages)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3183922b",
   "metadata": {},
   "source": [
    "<a name=\"tax\"></a>\n",
    "\n",
    "### Taxonomy match\n",
    "Usually taxonomy is represented by short alphanumeric pattern with hyphens and dots; sometimes a single letter or a single number (in this case our search may end up with a lot of false positives).\n",
    "\n",
    "The challenge is the might be either hyphen or whitespace used as separator on random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb94817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(x):\n",
    "    return re.sub(r'[\\.\\- ]', '', x).upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e110063d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first letter `f` in IRS filenames stands for `form` followerd by number (false positive)\n",
    "def search_pattern(d):\n",
    "    if d['file'].startswith('irs-'):\n",
    "        if d['lang'] == 'sp':\n",
    "            return stem(f\"FORMULARIO {d['taxonomy']}\")\n",
    "        return stem(f\"FORM {d['taxonomy']}\")\n",
    "    return stem(f\"{d['taxonomy']}{d['ext'].replace('-','')}\")\n",
    "\n",
    "pages['pattern'] = pages.apply(search_pattern, axis=1)\n",
    "pages[['file','taxonomy','pattern']].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412ff871",
   "metadata": {},
   "source": [
    "#### Sample doc\n",
    "Let's check our hypothesis with some random document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1143755",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = 'cnd-t100b.fr' #np.random.choice(list(set(pages['doc'])))\n",
    "# get all pages data\n",
    "files = pages[pages['file']==doc]['source'].to_list()\n",
    "desc = docs.loc[docs['file']==doc,'desc'].iloc[0]\n",
    "pattern = pages.loc[pages['file']==doc,'pattern'].iloc[0]\n",
    "print(f'Doc: {doc}\\n{desc}\\n#pages: {len(files)}\\nTaxonomy marker: {pattern}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4b5f38",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "order = [int(source.split('-').pop()) for source in files]\n",
    "\n",
    "def find_marker(pattern):\n",
    "    def has_marker(x):\n",
    "        return stem(str(x)).find(pattern) != -1\n",
    "    return has_marker\n",
    "    \n",
    "# show pages with layout-blocks marked by type and highlight if found the taxonomy marker\n",
    "for i,o in enumerate(np.argsort(order)):\n",
    "    source = files[o]\n",
    "    \n",
    "    image = Image.open(f'data/images/{source}.png')\n",
    "    content = pd.read_csv(f'data/info/{source}.csv.gz')\n",
    "    try: # check if the for-inputs data is available\n",
    "        inputs = pd.read_csv(f'data/inputs/{source}.csv.gz')\n",
    "        inputs = inputs.loc[inputs['field_type_string']!='Button']\n",
    "    except: # pdf has no widgets\n",
    "        inputs = None\n",
    "        \n",
    "    # check if marker present in the text\n",
    "    content['marker'] = content['text'].apply(find_marker(pattern))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    \n",
    "    # make a point for the legend\n",
    "    ax.scatter([-100], [-100], color='C0', marker='s', s=100, alpha=0.4, label='word')\n",
    "    ax.plot([-100], [-100], color='C1', label='image')\n",
    "    ax.plot([-100], [-100], color='C2', label='block')\n",
    "    ax.plot([-100], [-100], color='C3', label='target block: contains marker')\n",
    "    ax.scatter([-100], [-100], color='yellow', marker='s', s=100, alpha=0.6, label='form-input')\n",
    "    \n",
    "    # use page-image as a background\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    # highlight layout blocks by type\n",
    "    for box in content[['left','top','right','bottom','scale','orig','block-type','marker']].values:\n",
    "        x1, y1, x2, y2, s, o, t, m = box\n",
    "        w, h = (x2 - x1) * s, (y2 - y1) * s\n",
    "        x, y = x1 * s, y1 * s\n",
    "        color = 'C3' if m == 1 else ('C1' if t == 1 else ('C0' if o == 'word' else 'C2'))\n",
    "        if color == 'C0':\n",
    "            patch = patches.Rectangle((x, y), w, h,\n",
    "                                      linewidth=0, edgecolor='none', facecolor='C0', alpha=0.25)\n",
    "        else:\n",
    "            patch = patches.Rectangle((x, y), w, h,\n",
    "                                      linewidth=1, edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(patch)\n",
    "        \n",
    "    # whenever the form-inputs data is available show the inputs\n",
    "    if inputs is not None:    \n",
    "        for box in inputs[['left','top','right','bottom','field_type_string']].values:\n",
    "            x1, y1, x2, y2, t = box\n",
    "            try:\n",
    "                w, h = (x2 - x1) * s, (y2 - y1) * s\n",
    "                x, y = x1 * s, y1 * s\n",
    "                ax.add_patch(patches.Rectangle((x, y), w, h,\n",
    "                                               linewidth=0, edgecolor='none', facecolor='yellow', alpha=0.6))\n",
    "            except:\n",
    "                pass # parsing error\n",
    "            \n",
    "    ax.set_title(f'Page {i + 1}')\n",
    "    ax.legend(bbox_to_anchor=(1, 1), frameon=False)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8449cbfc",
   "metadata": {},
   "source": [
    "<a name=\"base\"></a>\n",
    "\n",
    "##  Expectation\n",
    "Let's estimate expectation that finding a taxonomy-marker leads to a successful document identification. For this experiment we will use the text extracted from the PDF-docs, so we would know the hard `ceil` -- we wouldn't be able to do any better than that with OCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4c3f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = []\n",
    "# go through all data\n",
    "for doc in set(pages['file']):    \n",
    "    files = pages[pages['file']==doc]['source'].to_list()\n",
    "    data = [pd.read_csv(f'./data/info/{source}.csv.gz') for source in files]\n",
    "    data = pd.concat(data)\n",
    "    data = data.loc[~data['text'].isna()]\n",
    "    pattern = pages.loc[pages['file']==doc,'pattern'].values[0]\n",
    "    # find if the pattern from the lookup table is present on the page somewhere\n",
    "    data['marker'] = data['text'].apply(find_marker(pattern))    \n",
    "    # if found save location reference\n",
    "    success = data.loc[data['marker']==1,['left','top','right','bottom','page','text']]\n",
    "    success[['left','right']] /= data['right'].max()\n",
    "    success[['top','bottom']] /= data['bottom'].max()\n",
    "    success['pattern'] = pattern\n",
    "    success['doc'] = doc\n",
    "    search.append(success)\n",
    "    \n",
    "search = pd.concat(search)\n",
    "search.to_csv('./data/search-test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21085698",
   "metadata": {},
   "outputs": [],
   "source": [
    "success = set(search.groupby('doc').size().index)\n",
    "failure = set(pages['file']).difference(success)\n",
    "# doc-level identification stats\n",
    "print('Docs. success: {} ({:.2%})  failure: {}'.format(len(success),\n",
    "                                                       len(success)/(len(success) + len(failure)),\n",
    "                                                       len(failure)))\n",
    "# page-level search success\n",
    "print(f\"Pages success: {len(pages.loc[pages['file'].isin(success)])/len(pages):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73f5979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot left-top coordinates of the bounding-box containing taxonomy-marker\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "for i,prefix in enumerate(['irs','cnd','que']):\n",
    "    D = search[search['doc'].str.startswith(prefix)]\n",
    "    ax.scatter(D['left'], 1 - D['top'], color=f'C{i}', s=3, alpha=0.2, label=prefix)\n",
    "ax.axhline(y=0.8, linestyle=':', color='C3')\n",
    "ax.text(1, 0.81, 'header', color='C3', ha='right')\n",
    "ax.axhline(y=0.2, linestyle=':', color='C3')\n",
    "ax.text(1, 0.175, 'footer', color='C3', ha='right')\n",
    "lg = plt.legend(bbox_to_anchor=(1, 0.6), frameon=False)\n",
    "for lh in lg.legend_handles: lh.set_alpha(1)\n",
    "plt.title('Heatmap: taxonomy-marker position on the page')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f110546",
   "metadata": {},
   "source": [
    "    # see some failed examples\n",
    "    pages.loc[pages['file'].isin(failure)]\n",
    "    for doc in [np.random.choice(list(failure))]:\n",
    "        files = pages[pages['file']==doc]['source'].to_list()\n",
    "        data = [pd.read_csv(f'./data/info/{source}.csv.gz') for source in files]\n",
    "        data = pd.concat(data)\n",
    "        data = data.loc[~data['text'].isna()]\n",
    "        pattern = pages.loc[pages['file']==doc,'pattern'].values[0]\n",
    "        print(doc, pattern)\n",
    "        print('\\n'.join(data[data['orig']!='word']['text'].to_list()))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e092bfe9",
   "metadata": {},
   "source": [
    "Observation: our strategy should work.\n",
    "\n",
    "<a name=\"pipe\"></a>\n",
    "\n",
    "## Doc-identification pipeline\n",
    "1. Detect and fix skew and orientation to improve OCR outcome and to locate the header/footer areas\n",
    "2. Check `hot-spots` for the presence of taxonomy markers; if failed to find or match -- run extended search\n",
    "3. Determine if the page contains form-inputs (needs full-size OCR extraction)\n",
    "\n",
    "We've seen that variance-based skew correction works quite well with a low-resolution view. However, to correct orientation (90º, 180º, 270º) we need a better model. Our hypothesis here: we can detect orientation based on low-resolution view as well.\n",
    "\n",
    "We also need to make sure that our processing works in the presence of noise and distortion: for the baselines we are going to use [simulated noisy data](Synthetic-Data.ipynb) to minimize the necessity for the real data (which we might not have to begin with, and would be high risk when available).\n",
    "\n",
    "<a name=\"skew\"></a>\n",
    "\n",
    "### Skew detection baseline\n",
    "Skew correction should fix rotation up to 45º, in the real data we would expect some small (less than 10º) angles. \n",
    "Let's evaluate performance of our ([variance-based skew correction](./OCR-Prep.ipynb#skew)) -- we might go with it as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4a4e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# page-images extracted from pdf\n",
    "images = [f'data/images/{x}.png' for x in pages['source']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a23313a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather skew-detection stats\n",
    "n = 1000\n",
    "# size to test (224 is ViT resolution)\n",
    "test = [64, 128, 224, 500]\n",
    "result = pd.DataFrame(columns=test)\n",
    "error = pd.DataFrame(columns=test)\n",
    "for k,size in enumerate(test):\n",
    "    stats = []\n",
    "    for i in range(n):\n",
    "        source = np.random.choice(images)\n",
    "        # run augmentation\n",
    "        orig, info = prep.random_transform(prep.img_load(source), max_skew=45, noise=0.5, perspective=True)\n",
    "        # test skew detection with downscale to dpi\n",
    "        angle = prep.detect_skew(orig, max_angle=45, base_size=size)\n",
    "        info['corrected'] = angle\n",
    "        stats.append(info.copy())\n",
    "        print(f'{(n * k + i)/len(test)/n:.2%}', end='\\r')\n",
    "\n",
    "    stats = pd.DataFrame.from_dict(stats)\n",
    "    error[size] = np.abs(stats['skew'] - stats['corrected'])\n",
    "    result[size] = stats['orient'] + error[size]\n",
    "\n",
    "# visualize result\n",
    "fig, ax = plt.subplots(1, 2, figsize=(9, 4))\n",
    "for i,size in enumerate(result.columns):\n",
    "    ax[0].scatter(range(n), result[size], s=5, marker=f'{i+1}', alpha=0.5, label=size)\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([0, 90, 180, 270])\n",
    "ax[0].set_xlabel('Sample order')\n",
    "for i,size in enumerate(result.columns):\n",
    "    ax[1].scatter(error[size], result[size], s=5, marker=f'{i+1}', alpha=0.5, label=size)\n",
    "ax[1].set_yticks([0, 90, 180, 270])\n",
    "ax[1].set_xlabel('Sample error')\n",
    "ax[1].set_ylabel('Sample rotation outcome')\n",
    "lg = ax[1].legend(bbox_to_anchor=(1, 1), frameon=False)\n",
    "for lh in lg.legend_handles: lh.set_alpha(1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef3e1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fraction of the correct detection\n",
    "np.sum((error < 2))/len(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934e9408",
   "metadata": {},
   "source": [
    "Results with a set of downscale-levels show the best performance consistently at 224 size, however, there are no significant difference in the tested range; over 80% of outcomes have the error less then 2º. Good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5105695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick view size\n",
    "VIEW_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d7f78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take random page view\n",
    "source = np.random.choice(images).split('/').pop()[:-4]\n",
    "print(source)\n",
    "# generate a noisy view of a filled-in form along with data for the labels\n",
    "orig, info, inputs = sim.generate_sample(source, dpi=200, light=0.3, noise=0.3)\n",
    "# correct skew and distortion\n",
    "output = orig.copy()\n",
    "angle = prep.detect_skew(output, max_angle=45)\n",
    "output = prep.img_rotate(output, angle, fill=prep.get_bg_value(output))\n",
    "output = prep.fit_straight(output)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 10))\n",
    "ax[0].imshow(orig, 'gray')\n",
    "ax[0].set_title('Original (augmented image)')\n",
    "ax[1].imshow(output, 'gray')\n",
    "ax[1].set_title(f'Processed (detected skew {angle}º)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9329e0",
   "metadata": {},
   "source": [
    "<a name=\"orient\"></a>\n",
    "\n",
    "### [Orientation detection baseline](Classification-Baseline.ipynb#orient)\n",
    "At this point we consider that a small rotation angle (skew) and distortion are corrected. Orientation correction should fix 90º, 180º, and 270º. The hypothesis here: there are enough of visual cues for the human eye to hint the page orientation on the low-resolution view and the outlines if present. We cannot read the text from the image, but we know the page orientation, and if some text is present, if a table is present, if some inputs are present. Let's estimate how well we could do that with a neural network.\n",
    "\n",
    "Bird's eye view estimation: for this experiment we generate a noisy dataset of images which we first straighten-up, means the outcome may not be perfectly aligned (20% will have 1º+ residual skew), then, using this data, we train [orientation detector model](Classification-Baseline.ipynb#orient). The model will take a low-resolution view as an input. To avoid our model picking up on logos and local style we used `center-crop`: model only sees the middle section of the page and the side edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85206847",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = parse.extract_lines(output, units=20)\n",
    "\n",
    "# scale down to 224 on max size\n",
    "size = tuple((np.array(orig.shape) * 224/max(orig.shape)).astype(int))[::-1]\n",
    "layout = cv2.resize(output, size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10,10))\n",
    "ax[0].imshow(layout, 'gray')\n",
    "ax[0].set_title('Low resolution layout features')\n",
    "ax[1].imshow(lines, 'gray')\n",
    "ax[1].set_title('Extracted lines')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af92b411",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for _ in range(1000):\n",
    "    # take random page view\n",
    "    source = np.random.choice(images).split('/').pop()[:-4]\n",
    "    # generate a noisy view of a filled-in form along with data for the labels\n",
    "    orig, info, _ = sim.generate_sample(source, dpi=200, light=0.3, noise=0.3)\n",
    "    output, orient = test_correction_pipeline(orig)\n",
    "    results.append([orient, info['orient']])\n",
    "\n",
    "# show resulting confusion matrix\n",
    "results = pd.DataFrame(np.array(results).reshape((1000,2)), columns=['detected','actual'])\n",
    "heatmap = results.groupby(['detected','actual']).size()\n",
    "heatmap /= heatmap.sum()\n",
    "if len(heatmap) < 16:\n",
    "    heatmap = heatmap.reindex([(a,b) for a in [0,90,180,270] for b in [0,90,180,270]], fill_value=0)\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "ax.imshow(heatmap.values.reshape((4, 4)))\n",
    "plt.axis('off')\n",
    "plt.title(f\"Accuracy: {np.sum(results['detected']==results['actual'])/len(results):.2%}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb47ba66",
   "metadata": {},
   "source": [
    "<a name=\"form\"></a>\n",
    "\n",
    "### [Form detection baseline](Classification-Baseline.ipynb#form)\n",
    "In our dataset we have forms with inputs arranged in the tabular layout (`IRS` forms) and forms with inline inputs stylized in some peculiar way which make training of a generic classifier without over-fitting to the dataset quite a challenge. For this experiment we use the same data we generated for the orientation detector. (We have more positive samples in the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e29461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for _ in range(1000):\n",
    "    # take random page view with widgets info available\n",
    "    source = np.random.choice([x for x in images if not x.startswith('data/images/que-')]).split('/').pop()[:-4]\n",
    "    # generate a noisy view of a filled-in form along with data for the labels\n",
    "    orig, _, inputs = sim.generate_sample(source, dpi=200, light=0.3, noise=0.3)\n",
    "    cls = test_classification_pipeline(orig)\n",
    "    results.append([cls, len(inputs) > 0])\n",
    "\n",
    "# show resulting confusion matrix\n",
    "results = pd.DataFrame(np.array(results).reshape((1000,2)), columns=['detected','actual'])\n",
    "heatmap = results.groupby(['detected','actual']).size()\n",
    "heatmap /= heatmap.sum()\n",
    "if len(heatmap) < 4:\n",
    "    heatmap = heatmap.reindex([(a,b) for a in [0,1] for b in [0,1]], fill_value=0)\n",
    "fig, ax = plt.subplots(figsize=(2, 2))\n",
    "ax.imshow(heatmap.values.reshape((2, 2)))\n",
    "plt.axis('off')\n",
    "plt.title(f\"Accuracy: {np.sum(results['detected']==results['actual'])/len(results):.2%}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd1e097",
   "metadata": {},
   "source": [
    "<a name=\"match\"></a>\n",
    "\n",
    "### Check hot-spots for the presence of taxonomy markers\n",
    "At this point we consider the document is straighten up and in the right position. Our strategy is to look at the small fragment of the image in the order of priority which would help with gradient noise and execution time, however, it is still important how the actual OCR is performed. First, we just run a simple brute-force concept prove: go through all the data we know to have the taxonomy-markers and read the known \"hot spots\" with `Tesseract.image_to_string` method following with pattern search. [The reading strategy we address in a separate notebook.]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994b3627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_header_footer(image):\n",
    "    \"\"\"\n",
    "    visualize flow\n",
    "    \"\"\"\n",
    "    w, h = image.shape\n",
    "    d = int(h * 0.2)\n",
    "    print('------------------------ reading header ---------------------------')\n",
    "    header = normalize(image[:d,:])            \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.imshow(header, 'gray')\n",
    "    plt.axis('off')\n",
    "    plt.title('Original')\n",
    "    plt.show()\n",
    "    header = pts.image_to_string(header, lang='eng+fre')\n",
    "    print(header)\n",
    "    print('------------------------ reading footer ---------------------------')    \n",
    "    footer = normalize(image[-d:,:])\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.imshow(footer, 'gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    footer = pts.image_to_string(footer, lang='eng+fre')\n",
    "    print(footer)\n",
    "    return header, footer\n",
    "\n",
    "# grab a page from the random doc\n",
    "doc = np.random.choice(list(success))\n",
    "# get search target\n",
    "pattern = get_marker(doc)\n",
    "for source in pages[pages['doc']==doc]['source'].to_list():\n",
    "    # generate a noisy view of a filled-in form along with labels\n",
    "    orig, info, inputs = sim.generate_sample(source, dpi=300, light=0.3, noise=0.3)\n",
    "    # run correction\n",
    "    output, orient = test_correction_pipeline(orig)\n",
    "    # show original and straighten-up\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10,10))\n",
    "    ax[0].imshow(orig, 'gray')\n",
    "    ax[0].set_title('Original')\n",
    "    ax[1].imshow(output, 'gray')\n",
    "    ax[1].set_title('Prepared')\n",
    "    plt.show()\n",
    "    if info['orient'] != orient:\n",
    "        print('orientation detection failure...')\n",
    "        continue\n",
    "    check_header_footer(output)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2875ebb",
   "metadata": {},
   "source": [
    "If we allow some minimal `levelshtein` distance the outcome improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00011dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_match(pattern, max_dist=1):\n",
    "    def best_match(x):\n",
    "        if type(x) != str: return 0\n",
    "        matches = find_near_matches(pattern, stem(str(x)), max_l_dist=max_dist, max_deletions=0)\n",
    "        if len(matches) > 0:\n",
    "            return min(matches, key=lambda x:x.dist)\n",
    "    return best_match\n",
    "\n",
    "\n",
    "def search_doc(image, find):\n",
    "    \"\"\"\n",
    "    search priority order: header and footer first\n",
    "    followed by the middle part top-down\n",
    "    \"\"\"    \n",
    "    w, h = image.shape\n",
    "    d = int(h * 0.2)\n",
    "    print(w, h, d)\n",
    "    header = pts.image_to_string(normalize(image[:d,:]), lang='eng+fre')\n",
    "    b = find(header)\n",
    "    if b == 0: # if exact return, if not continue\n",
    "        return 1, 0\n",
    "    best = b\n",
    "    footer = pts.image_to_string(normalize(image[-d:,:]), lang='eng+fre')\n",
    "    b = find(footer)\n",
    "    if b == 0:\n",
    "        return 2, 0\n",
    "    best = b or best\n",
    "    return 0, best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae5b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_doc(output, find_match(pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0501385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_match(pattern, max_dist=1):\n",
    "    def best_match(x):\n",
    "        if type(x) != str: return 0\n",
    "        matches = find_near_matches(pattern, stem(str(x)), max_l_dist=max_dist, max_deletions=0)\n",
    "        if len(matches) > 0:\n",
    "            return min(matches, key=lambda x:x.dist)\n",
    "    return best_match\n",
    "\n",
    "\n",
    "def search_doc(image, find):\n",
    "    \"\"\"\n",
    "    search priority order: header and footer first\n",
    "    followed by the middle part top-down\n",
    "    \"\"\"    \n",
    "    w, h = image.shape\n",
    "    d = int(h * 0.2)          \n",
    "    header = pts.image_to_string(normalize(image[:d,:]), lang='eng+fre')\n",
    "    d = find(header)\n",
    "    if d == 0:\n",
    "        return 1, 0\n",
    "    best = d\n",
    "    footer = pts.image_to_string(normalize(image[-d:,:]), lang='eng+fre')\n",
    "    d = find(footer)\n",
    "    if d == 0:\n",
    "        return 2, 0\n",
    "    best = min(best, d)\n",
    "    for i in range(1, 4):\n",
    "        content = pts.image_to_string(normalize(image[i * d:(i + 1) * d,:]), lang='eng+fre')\n",
    "        d = find(content)\n",
    "        if d == 0:\n",
    "            return 2 + i, 0\n",
    "        best = min(best, d)\n",
    "    return 0, best or None\n",
    "\n",
    "\n",
    "# reset and add `dist` for fuzzy match\n",
    "test = search.groupby(['doc','pattern']).size().reset_index().set_index('doc')\n",
    "test['success'] = 0\n",
    "test['dist'] = None\n",
    "test['exception'] = 0\n",
    "test['proc'] = None\n",
    "# brute-force search\n",
    "for n, doc in enumerate(test.index):\n",
    "    # pattern to look for\n",
    "    pattern = test.loc[doc,'pattern']\n",
    "    result, dist = 0, 10\n",
    "    files = pages[pages['doc']==doc]['source'].to_list()\n",
    "    for source in files:\n",
    "        # generate a noisy view of a filled-in form along with data for the labels\n",
    "        orig, info, inputs = sim.generate_sample(source, dpi=300, light=0.3, noise=0.3)\n",
    "        # run correction\n",
    "        output, orient = test_correction_pipeline(orig)\n",
    "        test.loc[doc,'proc'] = 1\n",
    "        if info['orient'] != orient:\n",
    "            test.loc[doc,'exception'] += 1\n",
    "            continue\n",
    "        try:\n",
    "            i, d = search_doc(output, find_match(pattern, max_dist=1))\n",
    "        except:\n",
    "            test.loc[doc,'exception'] += 1\n",
    "            continue\n",
    "        else:\n",
    "            if d is not None and dist >= d:\n",
    "                result += 1\n",
    "                dist = min(dist, d)\n",
    "    test.loc[doc,'success'] = result\n",
    "    test.loc[doc,'dist'] = dist\n",
    "    test.to_csv('./data/test-match.csv')\n",
    "    print(f'done: {n/len(test):.2%}', end='\\r')\n",
    "\n",
    "result = test.loc[test['proc'] == 1]\n",
    "print(f\"Success: {len(result[result['success'] > 0])/len(result):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602653bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.groupby(['success','exception']).size()\n",
    "(result / result.sum() * 100).reset_index().style.background_gradient('Reds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1654e5e8",
   "metadata": {},
   "source": [
    "The method above might be ok for the concept prove, but to make it useful in real practice we have to figure out some serious optimization (this is slow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e40cd6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b2300c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
